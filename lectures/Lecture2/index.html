
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Complete Data Science Course Materials">
      
      
        <meta name="author" content="Lida Aristakesyan">
      
      
        <link rel="canonical" href="https://lidaristakesyan.github.io/data-science-course/lectures/Lecture2/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.0">
    
    
      
        <title>Lecture2 - Data Science Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.618322db.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#create-virtual-environment-with-system-site-packages-access" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Data Science Course" class="md-header__button md-logo" aria-label="Data Science Course" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Data Science Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Lecture2
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/lidaristakesyan/data-science-course" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    lidaristakesyan/data-science-course
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link">
        
  
  
    
  
  Lectures

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Data Science Course" class="md-nav__button md-logo" aria-label="Data Science Course" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Data Science Course
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/lidaristakesyan/data-science-course" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    lidaristakesyan/data-science-course
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lectures
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<p>Data Science Tools and Environments</p>
<p>Advanced Course for Data Science Students</p>
<p><em>What is Data and How to Use It Optimally in Analysis, ML/DL Pipelines</em></p>
<p><strong>Duration: 7 Lectures</strong></p>
<p>Part I: Core Tools &amp; Data Processing (3 Lectures) | Part II: Big Data Ecosystem (4 Lectures)</p>
<p>Course Overview</p>
<p>This advanced course assumes prior familiarity with Python programming and basic data science concepts. The curriculum bridges foundational knowledge with expert-level application, focusing on production-ready techniques, performance optimization, and industry best practices. Students gain hands-on experience with professional workflows that scale from exploratory analysis to enterprise-grade data pipelines.</p>
<p>Prerequisites</p>
<ul>
<li>Proficiency in Python programming (functions, classes, decorators, context managers)</li>
<li>Basic understanding of NumPy arrays and Pandas DataFrames</li>
<li>Familiarity with command-line interfaces and Git version control</li>
<li>Foundational statistics knowledge (distributions, hypothesis testing, correlation)</li>
</ul>
<p>PART I: Core Tools, Environments &amp; Data Processing</p>
<p>Lecture 1: Professional Development Environments &amp; Advanced Tool Configuration</p>
<p>Learning Objectives</p>
<ul>
<li>Design and implement reproducible virtual environments using venv, conda, and containerization</li>
<li>Configure JupyterLab for production-grade data science workflows with custom kernels</li>
<li>Implement environment management best practices for ML/DL pipeline reproducibility</li>
<li>Master dependency management, version pinning, and conflict resolution strategies</li>
</ul>
<p>1.1 Virtual Environment Architecture</p>
<p>Virtual environments provide isolated Python installations that prevent dependency conflicts between projects. Understanding the underlying architecture is essential for troubleshooting and optimization. A virtual environment creates a self-contained directory structure containing a Python interpreter symlink, a site-packages directory for installed libraries, and activation scripts that modify PATH and PYTHONPATH environment variables.</p>
<p><strong>The venv Module: Native Python Isolation</strong></p>
<p>Python's built-in venv module creates lightweight virtual environments. The key insight is that venv does not copy the Python interpreter; instead, it creates symlinks (on Unix) or copies (on Windows) to the system Python, significantly reducing disk usage while maintaining isolation.</p>
<h1 id="create-virtual-environment-with-system-site-packages-access">Create virtual environment with system site-packages access<a class="headerlink" href="#create-virtual-environment-with-system-site-packages-access" title="Permanent link">&para;</a></h1>
<p>python -m venv --system-site-packages ./venv_with_system</p>
<h1 id="create-completely-isolated-environment">Create completely isolated environment<a class="headerlink" href="#create-completely-isolated-environment" title="Permanent link">&para;</a></h1>
<p>python -m venv --clear ./venv_isolated</p>
<h1 id="upgrade-pip-immediately-after-creation">Upgrade pip immediately after creation<a class="headerlink" href="#upgrade-pip-immediately-after-creation" title="Permanent link">&para;</a></h1>
<p>source ./venv_isolated/bin/activate</p>
<p>python -m pip install --upgrade pip setuptools wheel</p>
<p><strong>Conda Environments: Beyond Python</strong></p>
<p>Conda environments differ fundamentally from venv in that they can manage non-Python dependencies (C libraries, compilers, CUDA toolkits). This is crucial for data science where packages like NumPy, SciPy, and TensorFlow depend on optimized BLAS/LAPACK implementations.</p>
<h1 id="create-environment-with-specific-python-and-dependencies">Create environment with specific Python and dependencies<a class="headerlink" href="#create-environment-with-specific-python-and-dependencies" title="Permanent link">&para;</a></h1>
<p>conda create -n ds_env python=3.11 numpy scipy pandas scikit-learn -c conda-forge</p>
<h1 id="export-environment-for-reproducibility">Export environment for reproducibility<a class="headerlink" href="#export-environment-for-reproducibility" title="Permanent link">&para;</a></h1>
<p>conda env export --from-history &gt; environment.yml</p>
<h1 id="create-exact-reproduction-from-lock-file">Create exact reproduction from lock file<a class="headerlink" href="#create-exact-reproduction-from-lock-file" title="Permanent link">&para;</a></h1>
<p>conda-lock install -n ds_env_locked conda-lock.yml</p>
<p>1.2 Advanced JupyterLab Configuration</p>
<p>JupyterLab serves as the primary interactive development environment for data scientists. Advanced configuration transforms it from a notebook viewer into a comprehensive IDE with debugging, profiling, and collaboration capabilities.</p>
<p><strong>Custom Kernel Management</strong></p>
<h1 id="register-virtual-environment-as-jupyter-kernel">Register virtual environment as Jupyter kernel<a class="headerlink" href="#register-virtual-environment-as-jupyter-kernel" title="Permanent link">&para;</a></h1>
<p>python -m ipykernel install --user --name=ds_project --display-name="DS Project (Python 3.11)"</p>
<h1 id="list-available-kernels">List available kernels<a class="headerlink" href="#list-available-kernels" title="Permanent link">&para;</a></h1>
<p>jupyter kernelspec list</p>
<h1 id="remove-obsolete-kernel">Remove obsolete kernel<a class="headerlink" href="#remove-obsolete-kernel" title="Permanent link">&para;</a></h1>
<p>jupyter kernelspec uninstall ds_old_project</p>
<p>1.3 Data Types: Memory Representation and Optimization</p>
<p>Understanding how Python and NumPy represent data in memory is fundamental to writing efficient data science code. Memory layout directly impacts cache utilization, vectorization possibilities, and overall computational performance.</p>
<p><strong>NumPy dtype System</strong></p>
<p>import numpy as np</p>
<h1 id="examine-dtype-properties">Examine dtype properties<a class="headerlink" href="#examine-dtype-properties" title="Permanent link">&para;</a></h1>
<p>dt = np.dtype('float32')</p>
<p>print(f"Itemsize: {dt.itemsize} bytes")</p>
<p>print(f"Byte order: {dt.byteorder}")</p>
<h1 id="memory-efficient-integer-selection">Memory-efficient integer selection<a class="headerlink" href="#memory-efficient-integer-selection" title="Permanent link">&para;</a></h1>
<p>data = np.array([1, 2, 3, 100, 200], dtype=np.uint8)  # 1 byte per element</p>
<p>data_large = np.array([1, 2, 3, 100, 200], dtype=np.int64)  # 8 bytes</p>
<p>print(f"uint8 memory: {data.nbytes} bytes")</p>
<p>print(f"int64 memory: {data_large.nbytes} bytes")</p>
<p><strong>Pandas Memory Optimization</strong></p>
<p>import pandas as pd</p>
<p>def optimize_dataframe(df):</p>
<p>"""Reduce DataFrame memory footprint."""</p>
<p>for col in df.columns:</p>
<p>col_type = df[col].dtype</p>
<p>if col_type == 'object':</p>
<p>num_unique = df[col].nunique()</p>
<p>if num_unique / len(df) &lt; 0.5:  # Cardinality &lt; 50%</p>
<p>df[col] = df[col].astype('category')</p>
<p>elif col_type == 'float64':</p>
<p>df[col] = pd.to_numeric(df[col], downcast='float')</p>
<p>elif col_type == 'int64':</p>
<p>df[col] = pd.to_numeric(df[col], downcast='integer')</p>
<p>return df</p>
<p>Recommended Resources for Lecture 1</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Resource</strong></td>
<td><strong>Type</strong></td>
<td><strong>Coverage</strong></td>
</tr>
<tr>
<td>Python Data Science Handbook (VanderPlas)</td>
<td>Book, Ch. 1-2</td>
<td>IPython, Jupyter, NumPy fundamentals</td>
</tr>
<tr>
<td>Effective Python, 3rd Ed. (Slatkin)</td>
<td>Book, Items 73-77</td>
<td>Virtual environments, dependencies</td>
</tr>
<tr>
<td>conda.io Documentation</td>
<td>Official Docs</td>
<td>Environment management, conda-forge</td>
</tr>
<tr>
<td>JupyterLab Documentation</td>
<td>Official Docs</td>
<td>Extension system, configuration</td>
</tr>
<tr>
<td>NumPy User Guide: Data Types</td>
<td>Official Docs</td>
<td>dtype system, structured arrays</td>
</tr>
</tbody>
</table>
<p>Lecture 2: Advanced NumPy, Pandas, and SciPy for High-Performance Computing</p>
<p>Learning Objectives</p>
<ul>
<li>Master NumPy broadcasting, advanced indexing, and memory-efficient operations</li>
<li>Implement high-performance Pandas operations using vectorization and method chaining</li>
<li>Apply SciPy's sparse matrices and optimization routines for scientific computing</li>
<li>Profile and optimize code bottlenecks in data processing pipelines</li>
</ul>
<p>2.1 NumPy: Beyond the Basics</p>
<p><strong>Broadcasting: The Key to Vectorized Operations</strong></p>
<p>Broadcasting is NumPy's mechanism for performing operations on arrays of different shapes. Understanding broadcasting rules eliminates the need for explicit loops and enables highly optimized SIMD operations. The rules are: (1) dimensions are compared from right to left, (2) dimensions are compatible if equal or one is 1, (3) missing dimensions are treated as 1.</p>
<p>import numpy as np</p>
<h1 id="broadcasting-example-normalize-features">Broadcasting example: normalize features<a class="headerlink" href="#broadcasting-example-normalize-features" title="Permanent link">&para;</a></h1>
<p>data = np.random.randn(1000, 50)  # 1000 samples, 50 features</p>
<p>mean = data.mean(axis=0)  # Shape: (50,)</p>
<p>std = data.std(axis=0)    # Shape: (50,)</p>
<h1 id="broadcasting-1000-50-50-1000-50">Broadcasting (1000, 50) - (50,) -&gt; (1000, 50)<a class="headerlink" href="#broadcasting-1000-50-50-1000-50" title="Permanent link">&para;</a></h1>
<p>normalized = (data - mean) / std</p>
<h1 id="outer-product-via-broadcasting">Outer product via broadcasting<a class="headerlink" href="#outer-product-via-broadcasting" title="Permanent link">&para;</a></h1>
<p>a = np.arange(5)[:, np.newaxis]  # Shape: (5, 1)</p>
<p>b = np.arange(3)                 # Shape: (3,)</p>
<p>outer = a * b                    # Shape: (5, 3)</p>
<p><strong>Advanced Indexing Patterns</strong></p>
<h1 id="boolean-indexing-for-conditional-selection">Boolean indexing for conditional selection<a class="headerlink" href="#boolean-indexing-for-conditional-selection" title="Permanent link">&para;</a></h1>
<p>data = np.random.randn(1000, 10)</p>
<p>mask = (data[:, 0] &gt; 0) &amp; (data[:, 1] &lt; 0)</p>
<p>filtered = data[mask]  # Returns copy</p>
<h1 id="fancy-indexing-for-reordering">Fancy indexing for reordering<a class="headerlink" href="#fancy-indexing-for-reordering" title="Permanent link">&para;</a></h1>
<p>indices = np.argsort(data[:, 0])[::-1]  # Descending order</p>
<p>sorted_data = data[indices]  # Returns copy</p>
<h1 id="npwhere-for-conditional-element-selection">np.where for conditional element selection<a class="headerlink" href="#npwhere-for-conditional-element-selection" title="Permanent link">&para;</a></h1>
<p>result = np.where(data &gt; 0, data, 0)  # ReLU operation</p>
<p>2.2 Pandas: Production-Grade Data Manipulation</p>
<p><strong>Method Chaining for Readable Pipelines</strong></p>
<p>import pandas as pd</p>
<p>import numpy as np</p>
<p>def calculate_metrics(df):</p>
<p>return df.assign(</p>
<p>total=lambda x: x['quantity'] * x['price'],</p>
<p>log_total=lambda x: np.log1p(x['total'])</p>
<p>)</p>
<p>result = (</p>
<p>pd.read_csv('sales.csv')</p>
<p>.query('date &gt;= "2024-01-01"')</p>
<p>.pipe(calculate_metrics)</p>
<p>.groupby('category', as_index=False)</p>
<p>.agg({'total': ['sum', 'mean'], 'quantity': 'count'})</p>
<p>.sort_values('sum', ascending=False)</p>
<p>)</p>
<p><strong>GroupBy: Split-Apply-Combine Pattern</strong></p>
<h1 id="transform-broadcast-aggregation-back-to-original-index">transform: broadcast aggregation back to original index<a class="headerlink" href="#transform-broadcast-aggregation-back-to-original-index" title="Permanent link">&para;</a></h1>
<p>df['pct_of_category'] = (</p>
<p>df.groupby('category')['sales']</p>
<p>.transform(lambda x: x / x.sum() * 100)</p>
<p>)</p>
<h1 id="multiple-aggregations-with-named-columns">Multiple aggregations with named columns<a class="headerlink" href="#multiple-aggregations-with-named-columns" title="Permanent link">&para;</a></h1>
<p>agg_result = df.groupby('category').agg(</p>
<p>total_sales=('sales', 'sum'),</p>
<p>avg_sales=('sales', 'mean'),</p>
<p>num_products=('product_id', 'nunique')</p>
<p>)</p>
<p>2.3 SciPy: Scientific Computing Toolkit</p>
<p><strong>Sparse Matrices for Memory Efficiency</strong></p>
<p>from scipy import sparse</p>
<p>import numpy as np</p>
<h1 id="construct-sparse-matrix-incrementally">Construct sparse matrix incrementally<a class="headerlink" href="#construct-sparse-matrix-incrementally" title="Permanent link">&para;</a></h1>
<p>row = np.array([0, 0, 1, 2, 2])</p>
<p>col = np.array([0, 2, 1, 0, 2])</p>
<p>data = np.array([1, 2, 3, 4, 5])</p>
<p>coo = sparse.coo_matrix((data, (row, col)), shape=(3, 3))</p>
<h1 id="convert-to-csr-for-efficient-arithmetic">Convert to CSR for efficient arithmetic<a class="headerlink" href="#convert-to-csr-for-efficient-arithmetic" title="Permanent link">&para;</a></h1>
<p>csr = coo.tocsr()</p>
<p>print(f"Sparse memory: {csr.data.nbytes + csr.indices.nbytes + csr.indptr.nbytes} bytes")</p>
<p>Recommended Resources for Lecture 2</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Resource</strong></td>
<td><strong>Type</strong></td>
<td><strong>Coverage</strong></td>
</tr>
<tr>
<td>Python Data Science Handbook (VanderPlas)</td>
<td>Book, Ch. 2-3</td>
<td>NumPy, Pandas comprehensive</td>
</tr>
<tr>
<td>NumPy Documentation: Broadcasting</td>
<td>Official Guide</td>
<td>Broadcasting rules, examples</td>
</tr>
<tr>
<td>Pandas User Guide: GroupBy</td>
<td>Official Docs</td>
<td>Split-apply-combine patterns</td>
</tr>
<tr>
<td>"High Performance Python" (Gorelick)</td>
<td>Book, Ch. 4-6</td>
<td>NumPy internals, profiling</td>
</tr>
<tr>
<td>Wes McKinney's Python for Data Analysis</td>
<td>Book, 3rd Ed.</td>
<td>Pandas creator's authoritative guide</td>
</tr>
</tbody>
</table>
<p>Lecture 3: Data Processing, Visualization, and Feature Engineering</p>
<p>Learning Objectives</p>
<ul>
<li>Implement robust data import/export pipelines supporting multiple formats</li>
<li>Apply advanced missing data imputation techniques appropriate to data characteristics</li>
<li>Design feature engineering pipelines with proper train/test separation</li>
<li>Create publication-quality visualizations using Matplotlib and Seaborn</li>
</ul>
<p>3.1 Multi-Format Data Import/Export</p>
<p><strong>Efficient CSV Processing</strong></p>
<p>import pandas as pd</p>
<h1 id="memory-efficient-csv-reading">Memory-efficient CSV reading<a class="headerlink" href="#memory-efficient-csv-reading" title="Permanent link">&para;</a></h1>
<p>chunk_iter = pd.read_csv(</p>
<p>'large_file.csv',</p>
<p>chunksize=100_000,</p>
<p>dtype={'id': 'int32', 'category': 'category', 'value': 'float32'},</p>
<p>usecols=['id', 'category', 'value', 'date'],</p>
<p>parse_dates=['date'],</p>
<p>na_values=['', 'NA', 'NULL', '-999']</p>
<p>)</p>
<h1 id="process-chunks-and-aggregate">Process chunks and aggregate<a class="headerlink" href="#process-chunks-and-aggregate" title="Permanent link">&para;</a></h1>
<p>results = []</p>
<p>for chunk in chunk_iter:</p>
<p>processed = chunk.groupby('category')['value'].sum()</p>
<p>results.append(processed)</p>
<p>final = pd.concat(results).groupby(level=0).sum()</p>
<p><strong>Parquet: The Columnar Format for Analytics</strong></p>
<p>import pandas as pd</p>
<h1 id="write-with-compression-and-partitioning">Write with compression and partitioning<a class="headerlink" href="#write-with-compression-and-partitioning" title="Permanent link">&para;</a></h1>
<p>df.to_parquet(</p>
<p>'data/',</p>
<p>engine='pyarrow',</p>
<p>compression='snappy',</p>
<p>partition_cols=['year', 'month'],</p>
<p>index=False</p>
<p>)</p>
<h1 id="read-with-column-selection-and-filtering">Read with column selection and filtering<a class="headerlink" href="#read-with-column-selection-and-filtering" title="Permanent link">&para;</a></h1>
<p>df = pd.read_parquet(</p>
<p>'data/',</p>
<p>columns=['id', 'value', 'category'],</p>
<p>filters=[('year', '&gt;=', 2023), ('category', 'in', ['A', 'B'])]</p>
<p>)</p>
<p>3.2 Missing Data: Theory and Practice</p>
<p><strong>Advanced Imputation Strategies</strong></p>
<p>from sklearn.impute import KNNImputer, IterativeImputer</p>
<p>from sklearn.ensemble import RandomForestRegressor</p>
<h1 id="knn-imputation-preserves-multivariate-relationships">KNN Imputation - preserves multivariate relationships<a class="headerlink" href="#knn-imputation-preserves-multivariate-relationships" title="Permanent link">&para;</a></h1>
<p>knn_imputer = KNNImputer(n_neighbors=5, weights='distance')</p>
<p>df_imputed = pd.DataFrame(</p>
<p>knn_imputer.fit_transform(df),</p>
<p>columns=df.columns,</p>
<p>index=df.index</p>
<p>)</p>
<h1 id="iterative-imputation-with-random-forest">Iterative imputation with Random Forest<a class="headerlink" href="#iterative-imputation-with-random-forest" title="Permanent link">&para;</a></h1>
<p>iter_imputer = IterativeImputer(</p>
<p>estimator=RandomForestRegressor(n_estimators=100, random_state=42),</p>
<p>max_iter=10,</p>
<p>random_state=42</p>
<p>)</p>
<p>3.3 Data Transformation Pipeline</p>
<p><strong>Scaling and Normalization</strong></p>
<p>from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer</p>
<p>from sklearn.compose import ColumnTransformer</p>
<h1 id="column-specific-transformations">Column-specific transformations<a class="headerlink" href="#column-specific-transformations" title="Permanent link">&para;</a></h1>
<p>preprocessor = ColumnTransformer([</p>
<p>('num_standard', StandardScaler(), ['age', 'income']),</p>
<p>('num_robust', RobustScaler(), ['transaction_amount']),</p>
<p>('num_power', PowerTransformer(method='yeo-johnson'), ['skewed_feature']),</p>
<p>], remainder='passthrough')</p>
<h1 id="critical-fit-on-training-data-only-transform-both">CRITICAL: fit on training data only, transform both<a class="headerlink" href="#critical-fit-on-training-data-only-transform-both" title="Permanent link">&para;</a></h1>
<p>X_train_scaled = preprocessor.fit_transform(X_train)</p>
<p>X_test_scaled = preprocessor.transform(X_test)  # No fit!</p>
<p>3.4 Outlier Detection and Handling</p>
<p>from sklearn.ensemble import IsolationForest</p>
<p>from sklearn.neighbors import LocalOutlierFactor</p>
<h1 id="isolation-forest-efficient-for-high-dimensions">Isolation Forest - efficient for high dimensions<a class="headerlink" href="#isolation-forest-efficient-for-high-dimensions" title="Permanent link">&para;</a></h1>
<p>iso_forest = IsolationForest(</p>
<p>contamination=0.05,</p>
<p>random_state=42,</p>
<p>n_jobs=-1</p>
<p>)</p>
<p>outlier_labels = iso_forest.fit_predict(X)  # -1 = outlier</p>
<h1 id="local-outlier-factor-density-based">Local Outlier Factor - density-based<a class="headerlink" href="#local-outlier-factor-density-based" title="Permanent link">&para;</a></h1>
<p>lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)</p>
<p>lof_labels = lof.fit_predict(X)</p>
<p>3.5 Visualization for Analysis</p>
<p><strong>Seaborn for Statistical Visualization</strong></p>
<p>import seaborn as sns</p>
<p>import matplotlib.pyplot as plt</p>
<p>sns.set_theme(style='whitegrid', palette='deep', font_scale=1.1)</p>
<h1 id="heatmap-for-correlation-matrix">Heatmap for correlation matrix<a class="headerlink" href="#heatmap-for-correlation-matrix" title="Permanent link">&para;</a></h1>
<p>fig, ax = plt.subplots(figsize=(10, 8))</p>
<p>corr = df.corr()</p>
<p>mask = np.triu(np.ones_like(corr, dtype=bool))</p>
<p>sns.heatmap(</p>
<p>corr, mask=mask, annot=True, fmt='.2f',</p>
<p>cmap='RdBu_r', center=0, square=True, ax=ax</p>
<p>)</p>
<p>3.6 Feature Engineering</p>
<p>from sklearn.feature_selection import SelectKBest, mutual_info_classif, SelectFromModel</p>
<p>from sklearn.ensemble import RandomForestClassifier</p>
<h1 id="mutual-information-captures-non-linear-relationships">Mutual information (captures non-linear relationships)<a class="headerlink" href="#mutual-information-captures-non-linear-relationships" title="Permanent link">&para;</a></h1>
<p>mi_selector = SelectKBest(score_func=mutual_info_classif, k=20)</p>
<p>X_mi = mi_selector.fit_transform(X, y)</p>
<h1 id="tree-based-importance-with-threshold">Tree-based importance with threshold<a class="headerlink" href="#tree-based-importance-with-threshold" title="Permanent link">&para;</a></h1>
<p>rf = RandomForestClassifier(n_estimators=100, random_state=42)</p>
<p>model_selector = SelectFromModel(rf, threshold='median')</p>
<p>X_model = model_selector.fit_transform(X, y)</p>
<p>Recommended Resources for Lecture 3</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Resource</strong></td>
<td><strong>Type</strong></td>
<td><strong>Coverage</strong></td>
</tr>
<tr>
<td>"Feature Engineering for ML" (Zheng)</td>
<td>Book</td>
<td>Comprehensive feature engineering</td>
</tr>
<tr>
<td>Pandas I/O Documentation</td>
<td>Official Docs</td>
<td>All file format support</td>
</tr>
<tr>
<td>Scikit-learn User Guide: Preprocessing</td>
<td>Official Docs</td>
<td>Transformers, pipelines</td>
</tr>
<tr>
<td>"Fundamentals of Data Visualization" (Wilke)</td>
<td>Book</td>
<td>Principles of visualization</td>
</tr>
<tr>
<td>Seaborn Tutorial Gallery</td>
<td>Official Docs</td>
<td>Statistical visualization patterns</td>
</tr>
</tbody>
</table>
<p>PART II: Big Data Ecosystem &amp; Distributed Computing</p>
<p>Lecture 4: Big Data Foundations - Hadoop Ecosystem and Distributed Computing</p>
<p>Learning Objectives</p>
<ul>
<li>Understand theoretical foundations of distributed computing and MapReduce</li>
<li>Explain HDFS architecture, data locality, and fault tolerance mechanisms</li>
<li>Compare batch vs. stream processing paradigms and their use cases</li>
<li>Evaluate when big data tools are necessary vs. single-machine optimization</li>
</ul>
<p>4.1 The Big Data Problem</p>
<p>Big data is characterized by the "Three Vs": Volume (terabytes to petabytes), Velocity (real-time streaming), and Variety (structured, semi-structured, unstructured). Distributed computing addresses this by partitioning data and computation across clusters of commodity machines.</p>
<p><strong>When Do You Actually Need Big Data Tools?</strong></p>
<p>Modern single machines with 64-128GB RAM and NVMe storage can process datasets of 50-100GB efficiently using optimized libraries (Polars, DuckDB). The overhead of distributed systems only pays off beyond certain thresholds.</p>
<h1 id="before-reaching-for-spark-try">Before reaching for Spark, try:<a class="headerlink" href="#before-reaching-for-spark-try" title="Permanent link">&para;</a></h1>
<p>import polars as pl</p>
<p>df = pl.scan_csv('large_file.csv')  # Lazy - doesn't load yet</p>
<p>result = (</p>
<p>df.filter(pl.col('date') &gt; '2024-01-01')</p>
<p>.group_by('category')</p>
<p>.agg(pl.col('value').sum())</p>
<p>.collect()  # Execute optimized query plan</p>
<p>)</p>
<h1 id="or-duckdb-in-process-analytical-database">Or DuckDB - In-process analytical database<a class="headerlink" href="#or-duckdb-in-process-analytical-database" title="Permanent link">&para;</a></h1>
<p>import duckdb</p>
<p>result = duckdb.execute("""</p>
<p>SELECT category, SUM(value) as total</p>
<p>FROM read_csv_auto('large_file.csv')</p>
<p>WHERE date &gt; '2024-01-01'</p>
<p>GROUP BY category</p>
<p>""").df()</p>
<p>4.2 Hadoop Distributed File System (HDFS)</p>
<p>HDFS is designed for storing very large files with streaming access patterns on commodity hardware. Files are split into large blocks (default 128MB) distributed across DataNodes. The NameNode maintains the filesystem namespace and block locations. This architecture optimizes for high throughput rather than low latency.</p>
<p>4.3 MapReduce: The Foundation</p>
<h1 id="conceptual-mapreduce-for-word-count">Conceptual MapReduce for word count<a class="headerlink" href="#conceptual-mapreduce-for-word-count" title="Permanent link">&para;</a></h1>
<h1 id="map-phase-line-word-1-word-1">Map phase: (line) -&gt; [(word, 1), (word, 1), ...]<a class="headerlink" href="#map-phase-line-word-1-word-1" title="Permanent link">&para;</a></h1>
<p>def mapper(line):</p>
<p>for word in line.split():</p>
<p>yield (word.lower(), 1)</p>
<h1 id="reduce-phase-word-counts-word-total">Reduce phase: (word, [counts]) -&gt; (word, total)<a class="headerlink" href="#reduce-phase-word-counts-word-total" title="Permanent link">&para;</a></h1>
<p>def reducer(word, counts):</p>
<p>return (word, sum(counts))</p>
<p>Recommended Resources for Lecture 4</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Resource</strong></td>
<td><strong>Type</strong></td>
<td><strong>Coverage</strong></td>
</tr>
<tr>
<td>"Hadoop: The Definitive Guide" (White)</td>
<td>Book, Ch. 1-4</td>
<td>HDFS, MapReduce fundamentals</td>
</tr>
<tr>
<td>"Designing Data-Intensive Apps" (Kleppmann)</td>
<td>Book, Ch. 10</td>
<td>Batch processing theory</td>
</tr>
<tr>
<td>Google MapReduce Paper (2004)</td>
<td>Research Paper</td>
<td>Original MapReduce design</td>
</tr>
<tr>
<td>Google GFS Paper (2003)</td>
<td>Research Paper</td>
<td>Distributed filesystem design</td>
</tr>
</tbody>
</table>
<p>Lecture 5: Apache Spark and PySpark for Data Processing</p>
<p>Learning Objectives</p>
<ul>
<li>Understand Spark's architecture: driver, executors, and the DAG execution model</li>
<li>Implement data transformations using DataFrame and SQL APIs</li>
<li>Optimize Spark jobs through partitioning, caching, and broadcast variables</li>
<li>Debug and profile Spark applications using the Spark UI</li>
</ul>
<p>5.1 Spark Architecture</p>
<p><strong>The Driver-Executor Model</strong></p>
<p>Spark applications consist of a driver program that coordinates execution and executors that perform computation. The driver maintains the SparkContext, converts user code into a DAG of stages, and schedules tasks on executors. This architecture enables in-memory computation across multiple operations.</p>
<p>from pyspark.sql import SparkSession</p>
<h1 id="create-sparksession">Create SparkSession<a class="headerlink" href="#create-sparksession" title="Permanent link">&para;</a></h1>
<p>spark = SparkSession.builder \</p>
<p>.appName('DataProcessingApp') \</p>
<p>.config('spark.executor.memory', '4g') \</p>
<p>.config('spark.executor.cores', '2') \</p>
<p>.config('spark.sql.shuffle.partitions', '200') \</p>
<p>.getOrCreate()</p>
<p><strong>Lazy Evaluation and the DAG</strong></p>
<h1 id="transformations-lazy-build-dag">Transformations (lazy - build DAG)<a class="headerlink" href="#transformations-lazy-build-dag" title="Permanent link">&para;</a></h1>
<p>df = spark.read.parquet('s3://bucket/data/')</p>
<p>filtered = df.filter(df['date'] &gt; '2024-01-01')  # Not executed</p>
<p>aggregated = filtered.groupBy('category').sum('value')  # Not executed</p>
<h1 id="action-triggers-execution">Action (triggers execution)<a class="headerlink" href="#action-triggers-execution" title="Permanent link">&para;</a></h1>
<p>result = aggregated.collect()  # NOW the entire pipeline executes</p>
<h1 id="explain-the-execution-plan">Explain the execution plan<a class="headerlink" href="#explain-the-execution-plan" title="Permanent link">&para;</a></h1>
<p>aggregated.explain(extended=True)</p>
<p>5.2 DataFrame API</p>
<p><strong>Window Functions for Advanced Analytics</strong></p>
<p>from pyspark.sql.window import Window</p>
<p>from pyspark.sql import functions as F</p>
<p>window_category = Window.partitionBy('category').orderBy(F.desc('sales'))</p>
<p>window_rolling = Window.partitionBy('store').orderBy('date').rowsBetween(-6, 0)</p>
<p>df_windowed = df.withColumn(</p>
<p>'rank_in_category', F.rank().over(window_category)</p>
<p>).withColumn(</p>
<p>'rolling_7day_avg', F.avg('sales').over(window_rolling)</p>
<p>).withColumn(</p>
<p>'prev_day_sales', F.lag('sales', 1).over(</p>
<p>Window.partitionBy('store').orderBy('date'))</p>
<p>)</p>
<p>5.3 Performance Optimization</p>
<p><strong>Partitioning Strategies</strong></p>
<h1 id="check-current-partitioning">Check current partitioning<a class="headerlink" href="#check-current-partitioning" title="Permanent link">&para;</a></h1>
<p>print(f"Number of partitions: {df.rdd.getNumPartitions()}")</p>
<h1 id="repartition-by-column-hash-partitioning">Repartition by column (hash partitioning)<a class="headerlink" href="#repartition-by-column-hash-partitioning" title="Permanent link">&para;</a></h1>
<p>df_repartitioned = df.repartition(100, 'customer_id')</p>
<h1 id="coalesce-to-reduce-partitions-no-shuffle">Coalesce to reduce partitions (no shuffle)<a class="headerlink" href="#coalesce-to-reduce-partitions-no-shuffle" title="Permanent link">&para;</a></h1>
<p>df_coalesced = df.coalesce(10)</p>
<h1 id="check-for-data-skew">Check for data skew<a class="headerlink" href="#check-for-data-skew" title="Permanent link">&para;</a></h1>
<p>df.groupBy(F.spark_partition_id()).count().show()</p>
<p><strong>Broadcast Joins</strong></p>
<p>from pyspark.sql.functions import broadcast</p>
<h1 id="small-dimension-table">Small dimension table<a class="headerlink" href="#small-dimension-table" title="Permanent link">&para;</a></h1>
<p>dim_table = spark.read.parquet('dimensions.parquet')  # 10MB</p>
<p>fact_table = spark.read.parquet('facts.parquet')  # 100GB</p>
<h1 id="broadcast-the-small-table">Broadcast the small table<a class="headerlink" href="#broadcast-the-small-table" title="Permanent link">&para;</a></h1>
<p>result = fact_table.join(</p>
<p>broadcast(dim_table),</p>
<p>on='dimension_id',</p>
<p>how='left'</p>
<p>)</p>
<p>Recommended Resources for Lecture 5</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Resource</strong></td>
<td><strong>Type</strong></td>
<td><strong>Coverage</strong></td>
</tr>
<tr>
<td>"Learning Spark" (Damji et al., 2nd Ed.)</td>
<td>Book</td>
<td>Comprehensive PySpark guide</td>
</tr>
<tr>
<td>"Spark: The Definitive Guide" (Chambers)</td>
<td>Book</td>
<td>Deep Spark internals</td>
</tr>
<tr>
<td>Apache Spark Documentation</td>
<td>Official Docs</td>
<td>API reference, tuning guide</td>
</tr>
<tr>
<td>"High Performance Spark" (Karau)</td>
<td>Book</td>
<td>Performance optimization</td>
</tr>
</tbody>
</table>
<p>Lecture 6: SQL Mastery and NoSQL Databases</p>
<p>Learning Objectives</p>
<ul>
<li>Write advanced SQL queries including CTEs, window functions, and recursive queries</li>
<li>Understand query execution plans and optimize SQL performance</li>
<li>Design and query document databases (MongoDB) and wide-column stores (Cassandra)</li>
<li>Select appropriate database technologies based on access patterns</li>
</ul>
<p>6.1 Advanced SQL Techniques</p>
<p><strong>Common Table Expressions (CTEs)</strong></p>
<p>-- Multi-level CTE for funnel analysis</p>
<p>WITH user_sessions AS (</p>
<p>SELECT user_id, session_id, MIN(event_time) as session_start</p>
<p>FROM events</p>
<p>WHERE event_date &gt;= CURRENT_DATE - INTERVAL '30 days'</p>
<p>GROUP BY user_id, session_id</p>
<p>),</p>
<p>funnel_events AS (</p>
<p>SELECT </p>
<p>e.user_id, e.session_id,</p>
<p>MAX(CASE WHEN event_type = 'page_view' THEN 1 ELSE 0 END) as viewed,</p>
<p>MAX(CASE WHEN event_type = 'add_to_cart' THEN 1 ELSE 0 END) as added,</p>
<p>MAX(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchased</p>
<p>FROM events e JOIN user_sessions s ON e.user_id = s.user_id</p>
<p>GROUP BY e.user_id, e.session_id</p>
<p>)</p>
<p>SELECT SUM(viewed) as views, SUM(added) as adds, SUM(purchased) as purchases</p>
<p>FROM funnel_events;</p>
<p>6.2 MongoDB: Document Database</p>
<p><strong>Aggregation Pipeline</strong></p>
<p>pipeline = [</p>
<p>{'<span class="arithmatex">\(match': {'created_at': {'\)</span>gte': datetime(2024, 1, 1)}}},</p>
<p>{'<span class="arithmatex">\(unwind': '\)</span>items'},</p>
<p>{'$group': {</p>
<p>'_id': '$items.product_id',</p>
<p>'total_quantity': {'<span class="arithmatex">\(sum': '\)</span>items.quantity'},</p>
<p>'total_revenue': {'<span class="arithmatex">\(sum': {'\)</span>multiply': ['<span class="arithmatex">\(items.price', '\)</span>items.quantity']}}</p>
<p>}},</p>
<p>{'$sort': {'total_revenue': -1}},</p>
<p>{'$limit': 10}</p>
<p>]</p>
<p>results = list(db.orders.aggregate(pipeline))</p>
<p>6.3 Apache Cassandra</p>
<p><strong>Data Model and Partition Strategy</strong></p>
<p>-- Partition by device and day for time-bounded queries</p>
<p>CREATE TABLE sensor_readings (</p>
<p>device_id UUID,</p>
<p>date DATE,</p>
<p>timestamp TIMESTAMP,</p>
<p>temperature DOUBLE,</p>
<p>humidity DOUBLE,</p>
<p>PRIMARY KEY ((device_id, date), timestamp)</p>
<p>) WITH CLUSTERING ORDER BY (timestamp DESC);</p>
<p>-- Efficient query - hits single partition</p>
<p>SELECT * FROM sensor_readings</p>
<p>WHERE device_id = ? AND date = '2024-06-15'</p>
<p>LIMIT 1000;</p>
<p>Recommended Resources for Lecture 6</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Resource</strong></td>
<td><strong>Type</strong></td>
<td><strong>Coverage</strong></td>
</tr>
<tr>
<td>"SQL Performance Explained" (Winand)</td>
<td>Book</td>
<td>Indexes, execution plans</td>
</tr>
<tr>
<td>"MongoDB: The Definitive Guide" (Bradshaw)</td>
<td>Book, 3rd Ed.</td>
<td>Document modeling, aggregation</td>
</tr>
<tr>
<td>"Cassandra: The Definitive Guide" (Carpenter)</td>
<td>Book, 3rd Ed.</td>
<td>Data modeling, operations</td>
</tr>
<tr>
<td>Use The Index, Luke (website)</td>
<td>Online Tutorial</td>
<td>SQL indexing deep dive</td>
</tr>
</tbody>
</table>
<p>Lecture 7: Building Production ETL Pipelines</p>
<p>Learning Objectives</p>
<ul>
<li>Design robust ETL/ELT pipelines following software engineering best practices</li>
<li>Implement workflow orchestration using Apache Airflow</li>
<li>Build data quality checks and monitoring into pipelines</li>
<li>Apply incremental processing patterns for efficiency</li>
</ul>
<p>7.1 ETL vs. ELT Architectures</p>
<p>Extract-Transform-Load (ETL) processes data before loading. Extract-Load-Transform (ELT) leverages the processing power of modern data warehouses (Snowflake, BigQuery, Redshift) where raw data is loaded first, then transformed using SQL within the warehouse. The transformation layer is typically managed by tools like dbt.</p>
<p>-- dbt model example (SQL-based transformation)</p>
<p>{{ config(materialized='incremental', unique_key='order_id') }}</p>
<p>WITH source_orders AS (</p>
<p>SELECT * FROM {{ ref('stg_orders') }}</p>
<p>{% if is_incremental() %}</p>
<p>WHERE updated_at &gt; (SELECT MAX(updated_at) FROM {{ this }})</p>
<p>{% endif %}</p>
<p>)</p>
<p>SELECT o.order_id, o.customer_id, c.customer_segment</p>
<p>FROM source_orders o</p>
<p>LEFT JOIN {{ ref('dim_customers') }} c ON o.customer_id = c.customer_id</p>
<p>7.2 Apache Airflow: Workflow Orchestration</p>
<p><strong>DAG Definition</strong></p>
<p>from airflow import DAG</p>
<p>from airflow.operators.python import PythonOperator</p>
<p>from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator</p>
<p>from datetime import timedelta</p>
<p>default_args = {</p>
<p>'owner': 'data_engineering',</p>
<p>'retries': 3,</p>
<p>'retry_delay': timedelta(minutes=5),</p>
<p>}</p>
<p>with DAG(</p>
<p>dag_id='daily_sales_etl',</p>
<p>default_args=default_args,</p>
<p>schedule_interval='0 6 * * *',</p>
<p>catchup=False,</p>
<p>) as dag:</p>
<p>validate = PythonOperator(task_id='validate', python_callable=validate_data)</p>
<p>transform = SparkSubmitOperator(task_id='transform', application='/spark/transform.py')</p>
<p>load = PythonOperator(task_id='load', python_callable=load_to_warehouse)</p>
<p>validate &gt;&gt; transform &gt;&gt; load</p>
<p>7.3 Data Quality and Validation</p>
<p><strong>Great Expectations Framework</strong></p>
<p>import great_expectations as gx</p>
<p>from great_expectations.dataset import PandasDataset</p>
<p>def create_sales_expectations(df):</p>
<p>ge_df = PandasDataset(df)</p>
<p>ge_df.expect_column_values_to_not_be_null('order_id')</p>
<p>ge_df.expect_column_values_to_be_unique('order_id')</p>
<p>ge_df.expect_column_values_to_be_between('quantity', min_value=1, max_value=1000)</p>
<p>ge_df.expect_column_values_to_be_between('price', min_value=0.01, max_value=100000)</p>
<p>return ge_df.validate()</p>
<p>7.4 Incremental Processing Patterns</p>
<p><strong>Change Data Capture (CDC)</strong></p>
<h1 id="timestamp-based-incremental-extraction">Timestamp-based incremental extraction<a class="headerlink" href="#timestamp-based-incremental-extraction" title="Permanent link">&para;</a></h1>
<p>def extract_incremental(conn, table, last_run_time):</p>
<p>query = f"""</p>
<p>SELECT * FROM {table}</p>
<p>WHERE updated_at &gt; %(last_run)s</p>
<p>AND updated_at &lt;= %(current_run)s</p>
<p>"""</p>
<p>current_run = datetime.utcnow()</p>
<p>df = pd.read_sql(query, conn, params={'last_run': last_run_time, 'current_run': current_run})</p>
<p>save_watermark(table, current_run)</p>
<p>return df</p>
<p>Recommended Resources for Lecture 7</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Resource</strong></td>
<td><strong>Type</strong></td>
<td><strong>Coverage</strong></td>
</tr>
<tr>
<td>"Fundamentals of Data Engineering" (Reis)</td>
<td>Book</td>
<td>Modern data engineering patterns</td>
</tr>
<tr>
<td>"Data Pipelines Pocket Reference" (Densmore)</td>
<td>Book</td>
<td>Practical pipeline patterns</td>
</tr>
<tr>
<td>Apache Airflow Documentation</td>
<td>Official Docs</td>
<td>DAG authoring, operators</td>
</tr>
<tr>
<td>dbt Documentation &amp; Courses</td>
<td>Official Resources</td>
<td>Modern ELT transformation</td>
</tr>
<tr>
<td>Great Expectations Documentation</td>
<td>Official Docs</td>
<td>Data quality testing</td>
</tr>
</tbody>
</table>
<p>Comprehensive Reading List</p>
<p>Core Textbooks</p>
<ul>
<li>VanderPlas, J. (2016). Python Data Science Handbook. O'Reilly Media. [Chapters 1-4]</li>
<li>McKinney, W. (2022). Python for Data Analysis, 3rd Edition. O'Reilly Media.</li>
<li>Kleppmann, M. (2017). Designing Data-Intensive Applications. O'Reilly Media. [Ch. 3, 10-12]</li>
<li>Damji, J. et al. (2020). Learning Spark, 2nd Edition. O'Reilly Media.</li>
<li>Reis, J. &amp; Housley, M. (2022). Fundamentals of Data Engineering. O'Reilly Media.</li>
</ul>
<p>Supplementary Books</p>
<ul>
<li>Gorelick, M. &amp; Ozsvald, I. (2020). High Performance Python, 2nd Ed. O'Reilly.</li>
<li>Zheng, A. &amp; Casari, A. (2018). Feature Engineering for Machine Learning. O'Reilly.</li>
<li>White, T. (2015). Hadoop: The Definitive Guide, 4th Ed. O'Reilly.</li>
<li>Chambers, B. &amp; Zaharia, M. (2018). Spark: The Definitive Guide. O'Reilly.</li>
<li>Winand, M. (2012). SQL Performance Explained. Self-published.</li>
<li>Bradshaw, S. et al. (2019). MongoDB: The Definitive Guide, 3rd Ed. O'Reilly.</li>
</ul>
<p>Research Papers</p>
<ul>
<li>Dean, J. &amp; Ghemawat, S. (2004). MapReduce: Simplified Data Processing on Large Clusters. OSDI.</li>
<li>Ghemawat, S. et al. (2003). The Google File System. SOSP.</li>
<li>Zaharia, M. et al. (2012). Resilient Distributed Datasets. NSDI.</li>
</ul>
<p>Online Resources</p>
<ul>
<li>Official Documentation: NumPy, Pandas, Scikit-learn, PySpark, Airflow</li>
<li>MongoDB University (university.mongodb.com) - Free certification courses</li>
<li>DataStax Academy - Cassandra training</li>
<li>dbt Learn (courses.getdbt.com) - Modern data transformation</li>
<li>Use The Index, Luke (use-the-index-luke.com) - SQL indexing</li>
</ul>
<p>Appendix: Environment Setup Guide</p>
<h1 id="create-conda-environment-for-course">Create conda environment for course<a class="headerlink" href="#create-conda-environment-for-course" title="Permanent link">&para;</a></h1>
<p>conda create -n ds_tools python=3.11 -y</p>
<p>conda activate ds_tools</p>
<h1 id="core-data-science-libraries">Core data science libraries<a class="headerlink" href="#core-data-science-libraries" title="Permanent link">&para;</a></h1>
<p>pip install numpy pandas scipy scikit-learn matplotlib seaborn</p>
<p>pip install jupyterlab ipykernel ipywidgets</p>
<h1 id="additional-libraries">Additional libraries<a class="headerlink" href="#additional-libraries" title="Permanent link">&para;</a></h1>
<p>pip install polars duckdb pyarrow fastparquet</p>
<p>pip install sqlalchemy psycopg2-binary pymongo cassandra-driver</p>
<p>pip install great-expectations missingno category_encoders</p>
<h1 id="pyspark-requires-java-811">PySpark (requires Java 8/11)<a class="headerlink" href="#pyspark-requires-java-811" title="Permanent link">&para;</a></h1>
<p>pip install pyspark==3.5.0</p>
<h1 id="register-jupyter-kernel">Register Jupyter kernel<a class="headerlink" href="#register-jupyter-kernel" title="Permanent link">&para;</a></h1>
<p>python -m ipykernel install --user --name=ds_tools</p>
<p><em>— End of Course Document —</em></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.top", "navigation.tracking", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate"], "search": "../../assets/javascripts/workers/search.7a47a382.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.e71a0d61.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js"></script>
      
    
  </body>
</html>