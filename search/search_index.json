{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Data Science Course","text":"<p>Welcome to the Data Science Course \ud83c\udf31</p>"},{"location":"#structure","title":"Structure","text":"<ul> <li>\ud83d\udcda Lecture notes &amp; Slides</li> <li>\ud83d\udcbb Code &amp; notebooks </li> <li>\ud83d\udcdd Homework assignments &amp; solutions  </li> </ul>"},{"location":"lectures/","title":"Course structure","text":"<p>Complete notes for the PA Academy Data Science course.</p>"},{"location":"lectures/#course-outline","title":"Course Outline","text":"<ul> <li>Course Outline</li> </ul>"},{"location":"lectures/#lecture-notes","title":"Lecture Notes","text":"<ul> <li>L01: Data as a Building Block</li> </ul>"},{"location":"lectures/#slides","title":"Slides","text":"<ul> <li>LS01: Data as a Building Block</li> </ul> <p>More lectures will be added throughout the course period.</p>"},{"location":"lectures/Lecture01_Foundations/","title":"Lecture 01: Data as a Building Block","text":""},{"location":"lectures/Lecture01_Foundations/#part-i-foundations-of-data-engineering","title":"PART I: FOUNDATIONS OF DATA ENGINEERING","text":""},{"location":"lectures/Lecture01_Foundations/#chapter-1-the-data-lifecycle-understanding-the-complete-journey","title":"Chapter 1: The Data Lifecycle \u2014 Understanding the Complete Journey","text":"<p>\"Data is a precious thing and will last longer than the systems themselves.\" \u2014 Tim Berners-Lee, inventor of the World Wide Web</p>"},{"location":"lectures/Lecture01_Foundations/#11-introduction-why-data-engineering-matters","title":"1.1 Introduction: Why Data Engineering Matters","text":"<p>In the modern era of artificial intelligence and machine learning, there is a common misconception that the magic happens in the models. Data scientists spend years mastering sophisticated algorithms, neural network architectures, and statistical methods. Yet, when they enter the workforce, they discover an uncomfortable truth: approximately 80% of their time is spent not on modeling, but on finding, cleaning, and preparing data.</p> <p>This reality has given rise to a fundamental shift in how we think about building intelligent systems. As Andrew Ng, co-founder of Google Brain and former Chief Scientist at Baidu, has emphasized through his advocacy for \"data-centric AI\": the quality of your data often matters more than the sophistication of your model. A simple logistic regression trained on excellent, well-curated data will frequently outperform a complex deep learning model trained on messy, poorly understood data.</p> <p>The emerging field of data-centric AI \"emphasizes the systematic engineering of data to build AI systems, shifting our focus from model to data. It is important to note that 'data-centric' differs fundamentally from 'data-driven', as the latter only emphasizes the use of data to guide AI development, which typically still centers on models\" (Zha et al., 2023).</p> <p>This chapter introduces you to the complete data lifecycle\u2014the journey that data takes from its point of origin to its ultimate use in analysis, machine learning, and decision-making. Understanding this lifecycle transforms you from a passive consumer of datasets into an informed practitioner who can:</p> <ul> <li>Trace the provenance of any dataset and understand its limitations</li> <li>Identify where data quality issues originate</li> <li>Design better features because you understand the source systems</li> <li>Communicate effectively with data engineers and infrastructure teams</li> <li>Build your own data pipelines when necessary</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#12-the-data-lifecycle-a-conceptual-framework","title":"1.2 The Data Lifecycle: A Conceptual Framework","text":"<p>Data does not simply appear in a clean CSV file ready for analysis. It flows through a series of stages, each with its own challenges, tools, and best practices. Understanding this flow\u2014the data lifecycle\u2014provides the mental framework necessary for working effectively with data at any scale.</p>"},{"location":"lectures/Lecture01_Foundations/#121-the-eight-stages-of-the-data-lifecycle","title":"1.2.1 The Eight Stages of the Data Lifecycle","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                                  \u2502\n\u2502                         THE DATA LIFECYCLE                                       \u2502\n\u2502                                                                                  \u2502\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n\u2502     \u2502 GENERATE \u2502\u2500\u2500\u2500\u25b6\u2502 COLLECT  \u2502\u2500\u2500\u2500\u25b6\u2502  STORE   \u2502\u2500\u2500\u2500\u25b6\u2502TRANSFORM \u2502               \u2502\n\u2502     \u2502          \u2502    \u2502 (Ingest) \u2502    \u2502 (Persist)\u2502    \u2502 (Process)\u2502               \u2502\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n\u2502                                                            \u2502                    \u2502\n\u2502                                                            \u25bc                    \u2502\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n\u2502     \u2502 ARCHIVE  \u2502\u25c0\u2500\u2500\u2500\u2502 MAINTAIN \u2502\u25c0\u2500\u2500\u2500\u2502  SERVE   \u2502\u25c0\u2500\u2500\u2500\u2502 ANALYZE  \u2502               \u2502\n\u2502     \u2502 (Retire) \u2502    \u2502 (Operate)\u2502    \u2502 (Deliver)\u2502    \u2502 (Consume)\u2502               \u2502\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n\u2502                                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Let us examine each stage in detail.</p>"},{"location":"lectures/Lecture01_Foundations/#122-stage-1-generation-where-data-originates","title":"1.2.2 Stage 1: Generation \u2014 Where Data Originates","text":"<p>Every piece of data begins somewhere. Understanding the origin of data\u2014its provenance\u2014is essential for assessing its reliability, understanding its structure, and anticipating its limitations.</p>"},{"location":"lectures/Lecture01_Foundations/#sources-of-data-generation","title":"Sources of Data Generation","text":"Source Category Examples Characteristics Transactional Systems Point-of-sale systems, banking transactions, e-commerce orders Highly structured, real-time, mission-critical accuracy User Interactions Website clicks, mobile app events, search queries Semi-structured, extremely high volume, behavioral Operational Systems CRM records, ERP data, inventory management Structured, business-process driven IoT and Sensors Temperature readings, GPS coordinates, machine telemetry Time-series, continuous streams, potentially noisy Third-Party Sources APIs (weather, financial markets, social media), purchased datasets Various formats, rate-limited, external dependencies User-Generated Content Reviews, comments, uploaded files, form submissions Unstructured or semi-structured, requires validation Machine-Generated Logs Application logs, server metrics, error reports Semi-structured, high volume, essential for debugging"},{"location":"lectures/Lecture01_Foundations/#the-importance-of-understanding-data-generation","title":"The Importance of Understanding Data Generation","text":"<p>When you receive a dataset, asking \"where did this data come from?\" is not merely an academic exercise. The generation context determines:</p> <ul> <li>Data quality: Manual entry systems have different error profiles than automated sensors</li> <li>Timeliness: Real-time systems provide current data; batch extracts may be hours or days old</li> <li>Completeness: Some systems capture everything; others sample or aggregate</li> <li>Bias: The mechanism of data collection often introduces systematic biases</li> </ul> <p>Key Insight: The most sophisticated analysis cannot overcome fundamental flaws introduced at the data generation stage. Always trace your data back to its source.</p>"},{"location":"lectures/Lecture01_Foundations/#123-stage-2-collection-bringing-data-into-your-systems","title":"1.2.3 Stage 2: Collection \u2014 Bringing Data Into Your Systems","text":"<p>Once data is generated, it must be captured and brought into your data infrastructure. This process\u2014data ingestion\u2014presents its own set of challenges and design decisions.</p>"},{"location":"lectures/Lecture01_Foundations/#batch-vs-streaming-ingestion","title":"Batch vs. Streaming Ingestion","text":"<p>The two fundamental paradigms for data collection are batch processing and stream processing:</p> <p>Batch Ingestion <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Source    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Scheduled Job     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Storage   \u2502\n\u2502   System    \u2502     \u2502 (hourly/daily/weekly)\u2502    \u2502   System    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Batch ingestion collects data at scheduled intervals. A nightly job might extract all new records from a production database, transform them, and load them into a data warehouse. This approach is:</p> <ul> <li>Simpler to implement and debug</li> <li>More efficient for large volumes (economies of scale)</li> <li>Acceptable when real-time data is not required</li> <li>Easier to ensure data consistency</li> </ul> <p>Stream Ingestion <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Source    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Continuous        \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Storage   \u2502\n\u2502   System    \u2502     \u2502   Stream Processor  \u2502     \u2502   System    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Stream ingestion processes data continuously as it arrives. User click events might flow through Apache Kafka into a real-time analytics system within milliseconds. This approach is:</p> <ul> <li>Essential when freshness matters (fraud detection, real-time recommendations)</li> <li>More complex to implement correctly</li> <li>Requires careful handling of late-arriving data</li> <li>Demands robust error handling for continuous operation</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#common-ingestion-patterns","title":"Common Ingestion Patterns","text":"Pattern Description Use Case Full Load Extract entire dataset each time Small reference tables, initial loads Incremental Load Extract only new or changed records Large transactional tables Change Data Capture (CDC) Capture individual insert/update/delete operations Real-time replication, audit trails Event Streaming Continuous flow of discrete events User behavior tracking, IoT data"},{"location":"lectures/Lecture01_Foundations/#124-stage-3-storage-persisting-data-for-future-use","title":"1.2.4 Stage 3: Storage \u2014 Persisting Data for Future Use","text":"<p>Once collected, data must be stored in systems appropriate for its structure, volume, and intended use. The choice of storage system has profound implications for cost, query performance, and analytical capabilities.</p>"},{"location":"lectures/Lecture01_Foundations/#the-storage-landscape","title":"The Storage Landscape","text":"<p>Modern data infrastructure typically includes multiple storage systems, each optimized for different purposes:</p> <p>Databases are optimized for transactional operations\u2014reading and writing individual records quickly and reliably. They store the current state of operational data.</p> <p>Data Warehouses are optimized for analytical queries\u2014aggregating millions of records to answer business questions. They store historical data in structured, pre-defined schemas.</p> <p>Data Lakes store raw data in its native format, without requiring a pre-defined schema. They accommodate structured, semi-structured, and unstructured data at massive scale.</p> <p>As the MongoDB documentation explains: \"A database stores the current data required to power an application. A data warehouse stores current and historical data for one or more systems in a predefined and fixed schema for the purpose of analyzing the data. Data lakes store data in their raw form\" (MongoDB, 2024).</p> <p>We will explore these storage systems in depth in Chapter 3.</p>"},{"location":"lectures/Lecture01_Foundations/#125-stage-4-transformation-making-data-useful","title":"1.2.5 Stage 4: Transformation \u2014 Making Data Useful","text":"<p>Raw data is rarely suitable for analysis. The transformation stage encompasses all the processing required to convert raw data into analysis-ready datasets.</p>"},{"location":"lectures/Lecture01_Foundations/#the-transformation-spectrum","title":"The Transformation Spectrum","text":"<p>Transformations range from simple to complex:</p> Transformation Type Examples Complexity Cleaning Removing duplicates, handling null values, correcting typos Basic Standardization Consistent date formats, normalized text case, unified units Basic Validation Enforcing data types, range checks, referential integrity Moderate Enrichment Adding derived fields, joining with reference data Moderate Aggregation Summarizing transactions by day/week/month Moderate Feature Engineering Creating ML features from raw signals Advanced Complex Business Logic Multi-step calculations, conditional transformations Advanced"},{"location":"lectures/Lecture01_Foundations/#the-etl-vs-elt-paradigm","title":"The ETL vs. ELT Paradigm","text":"<p>Historically, transformations occurred during the data movement process itself\u2014Extract, Transform, Load (ETL). Data was extracted from sources, transformed in a dedicated processing system, and then loaded into the destination.</p> <p>Modern cloud data warehouses have enabled a different approach\u2014Extract, Load, Transform (ELT). Raw data is loaded directly into the destination, and transformations occur within the powerful processing engines of cloud warehouses like Snowflake, BigQuery, or Redshift.</p> <p>We will examine these paradigms thoroughly in Chapter 6.</p>"},{"location":"lectures/Lecture01_Foundations/#126-stage-5-analysis-extracting-insights","title":"1.2.6 Stage 5: Analysis \u2014 Extracting Insights","text":"<p>The analysis stage is where data scientists typically enter the picture. This encompasses:</p> <ul> <li>Exploratory Data Analysis (EDA): Understanding distributions, relationships, and anomalies</li> <li>Statistical Analysis: Hypothesis testing, significance testing, causal inference</li> <li>Machine Learning: Building predictive and prescriptive models</li> <li>Business Intelligence: Creating reports, dashboards, and visualizations</li> </ul> <p>While this stage is the traditional focus of data science education, its success depends entirely on the quality of the preceding stages.</p>"},{"location":"lectures/Lecture01_Foundations/#127-stage-6-serving-delivering-value","title":"1.2.7 Stage 6: Serving \u2014 Delivering Value","text":"<p>Analysis has no value unless its results reach decision-makers. The serving stage delivers insights through:</p> <ul> <li>Dashboards and Reports: Self-service access to metrics and KPIs</li> <li>APIs: Programmatic access for applications</li> <li>Embedded Analytics: Insights integrated into operational systems</li> <li>ML Model Predictions: Real-time or batch scoring</li> <li>Alerts and Notifications: Proactive communication of important changes</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#128-stage-7-maintenance-ensuring-reliability","title":"1.2.8 Stage 7: Maintenance \u2014 Ensuring Reliability","text":"<p>Data systems require ongoing care to remain reliable and trustworthy:</p> <ul> <li>Monitoring: Tracking data freshness, quality metrics, and system health</li> <li>Alerting: Automated notification of anomalies or failures</li> <li>Documentation: Maintaining accurate metadata and lineage information</li> <li>Evolution: Updating systems as requirements change</li> <li>Incident Response: Diagnosing and resolving data issues</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#129-stage-8-archival-managing-the-end-of-life","title":"1.2.9 Stage 8: Archival \u2014 Managing the End of Life","text":"<p>Data does not live forever. The archival stage addresses:</p> <ul> <li>Retention Policies: How long different data types must be kept</li> <li>Cold Storage: Moving infrequently accessed data to cheaper storage tiers</li> <li>Deletion: Permanently removing data per policy or regulation</li> <li>Compliance: Meeting legal requirements like GDPR's \"right to erasure\"</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#13-roles-in-the-data-ecosystem","title":"1.3 Roles in the Data Ecosystem","text":"<p>The data lifecycle involves multiple specialized roles. Understanding these roles\u2014and how they interact\u2014improves collaboration and helps you identify when to seek expertise.</p>"},{"location":"lectures/Lecture01_Foundations/#131-the-core-data-roles","title":"1.3.1 The Core Data Roles","text":"Role Primary Responsibility Key Skills Tools Data Engineer Build and maintain data infrastructure Python, SQL, distributed systems Spark, Airflow, Kafka, Cloud platforms Data Scientist Extract insights and build models Statistics, ML, programming Python, R, SQL, ML frameworks Data Analyst Answer business questions with data SQL, visualization, business acumen SQL, Tableau, Excel, Looker Analytics Engineer Transform data for analysis SQL, data modeling, software engineering dbt, SQL, Git ML Engineer Deploy and scale ML systems Software engineering, MLOps Docker, Kubernetes, MLflow Data Architect Design overall data strategy Systems design, governance Architecture tools, cloud platforms"},{"location":"lectures/Lecture01_Foundations/#132-the-evolving-boundaries","title":"1.3.2 The Evolving Boundaries","text":"<p>These roles are not rigid silos. In practice:</p> <ul> <li>Data scientists increasingly need data engineering skills to be self-sufficient</li> <li>Data engineers benefit from understanding analytical use cases</li> <li>Analytics engineers emerged to bridge the gap between engineering and analysis</li> <li>Small teams often combine multiple roles into \"full-stack\" data practitioners</li> </ul> <p>For Data Scientists: Developing data engineering competency makes you dramatically more effective. You can prototype pipelines, debug data issues, and work independently on smaller projects\u2014while collaborating more effectively with specialists on larger ones.</p>"},{"location":"lectures/Lecture01_Foundations/#14-event-tracking-and-event-driven-architecture","title":"1.4 Event Tracking and Event-Driven Architecture","text":"<p>In the modern data landscape, events are the fundamental unit of behavioral data. Understanding event tracking is essential for any data scientist working with user behavior, product analytics, or real-time systems.</p>"},{"location":"lectures/Lecture01_Foundations/#141-what-is-an-event","title":"1.4.1 What is an Event?","text":"<p>An event is a record of something that happened at a specific point in time. Unlike a database record that represents the current state of an entity, an event captures a discrete occurrence.</p> <p>The anatomy of an event:</p> <pre><code>EVENT = WHAT happened + WHEN it happened + WHO did it + WHERE + CONTEXT\n</code></pre> <p>Consider this example of a well-structured event:</p> <pre><code>{\n  \"event_id\": \"evt_8f14e45f-ceea-367a-a27d-f0c12e45bc\",\n  \"event_name\": \"product_added_to_cart\",\n  \"timestamp\": \"2025-11-29T14:32:15.123Z\",\n\n  \"user_id\": \"usr_abc123\",\n  \"anonymous_id\": \"anon_device_fingerprint_xyz\",\n  \"session_id\": \"sess_789xyz\",\n\n  \"properties\": {\n    \"product_id\": \"prod_456\",\n    \"product_name\": \"Wireless Noise-Canceling Headphones\",\n    \"price\": 249.99,\n    \"currency\": \"USD\",\n    \"quantity\": 1,\n    \"category\": \"Electronics/Audio\",\n    \"brand\": \"SoundMax\"\n  },\n\n  \"context\": {\n    \"page_url\": \"https://shop.example.com/products/wireless-headphones\",\n    \"page_title\": \"Wireless Headphones - SoundMax\",\n    \"referrer\": \"https://www.google.com/search?q=best+wireless+headphones\",\n    \"device\": {\n      \"type\": \"mobile\",\n      \"os\": \"iOS\",\n      \"os_version\": \"17.1\",\n      \"browser\": \"Safari\",\n      \"screen_width\": 390,\n      \"screen_height\": 844\n    },\n    \"location\": {\n      \"country\": \"United States\",\n      \"region\": \"California\",\n      \"city\": \"San Francisco\",\n      \"timezone\": \"America/Los_Angeles\"\n    },\n    \"campaign\": {\n      \"source\": \"google\",\n      \"medium\": \"cpc\",\n      \"campaign\": \"holiday_sale_2025\",\n      \"term\": \"wireless headphones\"\n    }\n  },\n\n  \"metadata\": {\n    \"sent_at\": \"2025-11-29T14:32:15.456Z\",\n    \"received_at\": \"2025-11-29T14:32:15.789Z\",\n    \"sdk_version\": \"2.5.1\",\n    \"library\": \"analytics.js\"\n  }\n}\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#142-the-event-driven-architecture","title":"1.4.2 The Event-Driven Architecture","text":"<p>Events flow through a system architecture designed to capture, transport, and process them reliably at scale.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         EVENT-DRIVEN ARCHITECTURE                                \u2502\n\u2502                                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                          EVENT PRODUCERS                                    \u2502 \u2502\n\u2502  \u2502                                                                             \u2502 \u2502\n\u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502 \u2502\n\u2502  \u2502   \u2502   Web   \u2502   \u2502 Mobile  \u2502   \u2502 Backend \u2502   \u2502   IoT   \u2502   \u2502  Third  \u2502     \u2502 \u2502\n\u2502  \u2502   \u2502  Apps   \u2502   \u2502  Apps   \u2502   \u2502Services \u2502   \u2502 Devices \u2502   \u2502  Party  \u2502     \u2502 \u2502\n\u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502           \u2502             \u2502             \u2502             \u2502             \u2502            \u2502\n\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502                                \u2502                                                \u2502\n\u2502                                \u25bc                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                       EVENT COLLECTION LAYER                                \u2502 \u2502\n\u2502  \u2502                                                                             \u2502 \u2502\n\u2502  \u2502   \u2022 SDK/Tracking Libraries (Segment, Amplitude, Mixpanel, custom)          \u2502 \u2502\n\u2502  \u2502   \u2022 API Gateways and Webhook Receivers                                      \u2502 \u2502\n\u2502  \u2502   \u2022 Validation, Enrichment, and Routing                                     \u2502 \u2502\n\u2502  \u2502                                                                             \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                       \u2502                                         \u2502\n\u2502                                       \u25bc                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                       EVENT STREAMING LAYER                                 \u2502 \u2502\n\u2502  \u2502                                                                             \u2502 \u2502\n\u2502  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502\n\u2502  \u2502   \u2502                    MESSAGE QUEUE / EVENT BUS                          \u2502 \u2502 \u2502\n\u2502  \u2502   \u2502                                                                       \u2502 \u2502 \u2502\n\u2502  \u2502   \u2502    Apache Kafka  \u2022  Amazon Kinesis  \u2022  Google Pub/Sub  \u2022  RabbitMQ   \u2502 \u2502 \u2502\n\u2502  \u2502   \u2502                                                                       \u2502 \u2502 \u2502\n\u2502  \u2502   \u2502    \u2022 Durability: Events persisted to disk                            \u2502 \u2502 \u2502\n\u2502  \u2502   \u2502    \u2022 Scalability: Partitioned for parallel processing                \u2502 \u2502 \u2502\n\u2502  \u2502   \u2502    \u2022 Decoupling: Producers and consumers operate independently       \u2502 \u2502 \u2502\n\u2502  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                       \u2502                                         \u2502\n\u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502           \u2502                           \u2502                           \u2502             \u2502\n\u2502           \u25bc                           \u25bc                           \u25bc             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502   REAL-TIME     \u2502       \u2502     BATCH       \u2502       \u2502    DERIVED      \u2502       \u2502\n\u2502  \u2502   PROCESSING    \u2502       \u2502    STORAGE      \u2502       \u2502     DATA        \u2502       \u2502\n\u2502  \u2502                 \u2502       \u2502                 \u2502       \u2502                 \u2502       \u2502\n\u2502  \u2502 \u2022 Live dashboards\u2502      \u2502 \u2022 Data Lake     \u2502       \u2502 \u2022 Data Warehouse\u2502       \u2502\n\u2502  \u2502 \u2022 Alerting      \u2502       \u2502 \u2022 Raw event     \u2502       \u2502 \u2022 Aggregated    \u2502       \u2502\n\u2502  \u2502 \u2022 Real-time ML  \u2502       \u2502   archive       \u2502       \u2502   tables        \u2502       \u2502\n\u2502  \u2502 \u2022 Fraud detection\u2502      \u2502 \u2022 Parquet files \u2502       \u2502 \u2022 ML features   \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                                                                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#143-designing-an-event-taxonomy","title":"1.4.3 Designing an Event Taxonomy","text":"<p>A well-designed event taxonomy\u2014the organized catalog of all events your system tracks\u2014is crucial for maintainability and analytical utility.</p>"},{"location":"lectures/Lecture01_Foundations/#event-naming-conventions","title":"Event Naming Conventions","text":"<p>Consistency in naming enables self-service analytics and reduces confusion:</p> <p>Recommended Pattern: <code>object_action</code></p> <pre><code>\u2713 product_viewed\n\u2713 product_added_to_cart\n\u2713 cart_viewed\n\u2713 checkout_started\n\u2713 checkout_step_completed\n\u2713 order_completed\n\u2713 order_refunded\n\u2713 user_signed_up\n\u2713 user_logged_in\n\u2713 subscription_started\n\u2713 subscription_cancelled\n</code></pre> <p>Patterns to Avoid:</p> <pre><code>\u2717 click                      (too generic)\n\u2717 event_1, event_2           (meaningless)\n\u2717 productAddedToCart         (inconsistent casing)\n\u2717 add_to_cart_button_clicked_on_product_detail_page  (too specific)\n\u2717 tracking_event             (redundant)\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#event-categories","title":"Event Categories","text":"<p>Organizing events into categories improves discoverability:</p> Category Purpose Example Events Acquisition How users arrive <code>campaign_clicked</code>, <code>referral_received</code> Activation First value moments <code>user_signed_up</code>, <code>onboarding_completed</code> Engagement Core product usage <code>feature_used</code>, <code>content_viewed</code> Conversion Revenue events <code>subscription_started</code>, <code>purchase_completed</code> Retention Return behavior <code>user_returned</code>, <code>streak_maintained</code> System Technical events <code>error_occurred</code>, <code>page_loaded</code>"},{"location":"lectures/Lecture01_Foundations/#144-event-properties-and-context","title":"1.4.4 Event Properties and Context","text":"<p>The value of an event lies not just in knowing it occurred, but in the rich context captured alongside it.</p>"},{"location":"lectures/Lecture01_Foundations/#properties-vs-context","title":"Properties vs. Context","text":"<p>Properties are specific to the event type:</p> <pre><code>// For \"product_added_to_cart\"\n\"properties\": {\n  \"product_id\": \"prod_456\",\n  \"product_name\": \"Wireless Headphones\",\n  \"price\": 249.99,\n  \"quantity\": 1,\n  \"category\": \"Electronics\"\n}\n</code></pre> <p>Context is captured automatically for all events:</p> <pre><code>// Same for every event\n\"context\": {\n  \"page_url\": \"...\",\n  \"device\": { \"type\": \"mobile\", \"os\": \"iOS\" },\n  \"location\": { \"country\": \"US\" },\n  \"campaign\": { \"source\": \"google\", \"medium\": \"cpc\" }\n}\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#identity-resolution","title":"Identity Resolution","text":"<p>Users interact with products across devices and sessions, often before creating an account. Event systems must handle:</p> <ul> <li>Anonymous ID: Device or browser identifier before login</li> <li>User ID: Authenticated identifier after login</li> <li>Session ID: Groups events within a single visit</li> <li>Identity Stitching: Connecting anonymous activity to known users after authentication</li> </ul> <pre><code>// Before login\n{\n  \"event_name\": \"product_viewed\",\n  \"anonymous_id\": \"anon_device_123\",\n  \"user_id\": null\n}\n\n// After login (same session)\n{\n  \"event_name\": \"user_logged_in\",\n  \"anonymous_id\": \"anon_device_123\",\n  \"user_id\": \"usr_abc456\"\n}\n\n// Subsequent events (user identified)\n{\n  \"event_name\": \"product_purchased\",\n  \"anonymous_id\": \"anon_device_123\",\n  \"user_id\": \"usr_abc456\"\n}\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#145-from-events-to-features-the-data-science-connection","title":"1.4.5 From Events to Features: The Data Science Connection","text":"<p>For data scientists, events are the raw material for behavioral features:</p> <pre><code>import pandas as pd\n\n# Raw events\nevents = pd.DataFrame([\n    {\"user_id\": \"u1\", \"event\": \"page_view\", \"timestamp\": \"2025-01-01 10:00:00\", \n     \"properties\": {\"page\": \"home\"}},\n    {\"user_id\": \"u1\", \"event\": \"product_viewed\", \"timestamp\": \"2025-01-01 10:05:00\",\n     \"properties\": {\"product_id\": \"p1\", \"price\": 99.99}},\n    {\"user_id\": \"u1\", \"event\": \"add_to_cart\", \"timestamp\": \"2025-01-01 10:07:00\",\n     \"properties\": {\"product_id\": \"p1\"}},\n    {\"user_id\": \"u1\", \"event\": \"purchase\", \"timestamp\": \"2025-01-01 10:15:00\",\n     \"properties\": {\"order_total\": 99.99}},\n    {\"user_id\": \"u2\", \"event\": \"page_view\", \"timestamp\": \"2025-01-01 11:00:00\",\n     \"properties\": {\"page\": \"home\"}},\n    {\"user_id\": \"u2\", \"event\": \"page_view\", \"timestamp\": \"2025-01-01 11:30:00\",\n     \"properties\": {\"page\": \"pricing\"}},\n])\n\n# Transform to user-level features for ML\nuser_features = events.groupby('user_id').agg(\n    total_events=('event', 'count'),\n    unique_event_types=('event', 'nunique'),\n    session_duration_minutes=('timestamp', lambda x: \n        (pd.to_datetime(x).max() - pd.to_datetime(x).min()).seconds / 60),\n    viewed_products=('event', lambda x: (x == 'product_viewed').sum()),\n    made_purchase=('event', lambda x: 'purchase' in x.values),\n).reset_index()\n\nprint(user_features)\n</code></pre> <p>Output: <pre><code>  user_id  total_events  unique_event_types  session_duration_minutes  viewed_products  made_purchase\n0      u1             4                   4                      15.0                1           True\n1      u2             2                   1                      30.0                0          False\n</code></pre></p>"},{"location":"lectures/Lecture01_Foundations/#15-summary-and-key-concepts","title":"1.5 Summary and Key Concepts","text":"<p>This chapter established the foundational concepts necessary for understanding data engineering:</p>"},{"location":"lectures/Lecture01_Foundations/#the-data-lifecycle","title":"The Data Lifecycle","text":"<p>Data flows through eight stages: Generation \u2192 Collection \u2192 Storage \u2192 Transformation \u2192 Analysis \u2192 Serving \u2192 Maintenance \u2192 Archival. Understanding this complete journey enables you to trace data quality issues, design better systems, and collaborate effectively across roles.</p>"},{"location":"lectures/Lecture01_Foundations/#roles-in-the-data-ecosystem","title":"Roles in the Data Ecosystem","text":"<p>Multiple specialized roles contribute to the data lifecycle. As a data scientist, developing data engineering literacy makes you more effective and self-sufficient.</p>"},{"location":"lectures/Lecture01_Foundations/#events-as-foundational-data","title":"Events as Foundational Data","text":"<p>Modern behavioral data is captured through events\u2014discrete records of actions with timestamps, identities, and rich contextual properties. Well-designed event architectures enable powerful analytics and machine learning.</p>"},{"location":"lectures/Lecture01_Foundations/#16-further-reading-and-resources","title":"1.6 Further Reading and Resources","text":""},{"location":"lectures/Lecture01_Foundations/#books","title":"Books","text":"<ul> <li>Reis, J. &amp; Housley, M. (2022). Fundamentals of Data Engineering. O'Reilly Media. The definitive introduction to data engineering concepts and practices.</li> <li>Kleppmann, M. (2017). Designing Data-Intensive Applications. O'Reilly Media. Deep exploration of the principles underlying modern data systems.</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#papers","title":"Papers","text":"<ul> <li>Zha, D. et al. (2023). \"Data-centric Artificial Intelligence: A Survey.\" arXiv:2303.10158. Comprehensive overview of data-centric approaches to AI. https://arxiv.org/abs/2303.10158</li> <li>Nazabal, A. et al. (2020). \"Data Engineering for Data Analytics: A Classification of the Issues, and Case Studies.\" arXiv:2004.12929. Classification of data engineering tasks with practical examples. https://arxiv.org/abs/2004.12929</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#online-resources","title":"Online Resources","text":"<ul> <li>Data Engineering Handbook (GitHub): Comprehensive collection of resources, tools, and learning paths. https://github.com/DataExpert-io/data-engineer-handbook</li> <li>DataTalks.Club Data Engineering Zoomcamp: Free, project-based course covering modern data engineering tools and practices. https://github.com/DataTalksClub/data-engineering-zoomcamp</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#17-exercises","title":"1.7 Exercises","text":"<p>Exercise 1.1: Data Lifecycle Mapping Choose a dataset you have worked with recently. Trace its journey through the data lifecycle: - Where was the data originally generated? - How was it collected and by whom? - Where is it stored? - What transformations were applied before you received it? - What limitations might exist due to decisions made at each stage?</p> <p>Exercise 1.2: Event Schema Design Design an event tracking schema for one of the following scenarios: - A music streaming application (like Spotify) - A food delivery service (like DoorDash) - An online learning platform (like Coursera)</p> <p>For each scenario: 1. Identify 10-15 key events to track 2. Define properties for each event 3. Document your naming conventions 4. Consider what context should be captured automatically</p> <p>Exercise 1.3: Event to Feature Transformation Given the following raw events, create five user-level features that could be useful for predicting user churn:</p> <pre><code>{\"user_id\": \"u1\", \"event\": \"app_opened\", \"timestamp\": \"2025-01-01T10:00:00Z\"}\n{\"user_id\": \"u1\", \"event\": \"content_viewed\", \"timestamp\": \"2025-01-01T10:05:00Z\", \"properties\": {\"content_type\": \"video\", \"duration_seconds\": 300}}\n{\"user_id\": \"u1\", \"event\": \"content_liked\", \"timestamp\": \"2025-01-01T10:10:00Z\"}\n{\"user_id\": \"u1\", \"event\": \"app_closed\", \"timestamp\": \"2025-01-01T10:30:00Z\"}\n{\"user_id\": \"u2\", \"event\": \"app_opened\", \"timestamp\": \"2025-01-01T11:00:00Z\"}\n{\"user_id\": \"u2\", \"event\": \"error_occurred\", \"timestamp\": \"2025-01-01T11:01:00Z\"}\n{\"user_id\": \"u2\", \"event\": \"app_closed\", \"timestamp\": \"2025-01-01T11:02:00Z\"}\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#chapter-2-data-types-and-file-formats-the-building-blocks","title":"Chapter 2: Data Types and File Formats \u2014 The Building Blocks","text":"<p>\"The goal is to turn data into information, and information into insight.\" \u2014 Carly Fiorina, former CEO of Hewlett-Packard</p>"},{"location":"lectures/Lecture01_Foundations/#21-introduction-why-format-matters","title":"2.1 Introduction: Why Format Matters","text":"<p>When you load a dataset into pandas with <code>pd.read_csv()</code>, a complex series of operations occurs behind the scenes. The file is read from disk, bytes are decoded into characters, delimiters are detected, columns are parsed, and data types are inferred. This seemingly simple operation embodies fundamental decisions about how data is represented, stored, and accessed.</p> <p>The choice of data format affects:</p> <ul> <li>Storage costs: Some formats are 10x more compact than others</li> <li>Query performance: The right format can make queries 100x faster</li> <li>Processing efficiency: Format determines how data flows through pipelines</li> <li>Interoperability: Different systems support different formats</li> <li>Schema evolution: Some formats handle changes gracefully; others break</li> </ul> <p>This chapter provides a comprehensive understanding of data types and file formats\u2014knowledge that will inform every data engineering decision you make.</p>"},{"location":"lectures/Lecture01_Foundations/#22-the-data-structure-spectrum","title":"2.2 The Data Structure Spectrum","text":"<p>Not all data fits neatly into spreadsheet rows and columns. Understanding the spectrum of data structures helps you choose appropriate storage and processing strategies.</p>"},{"location":"lectures/Lecture01_Foundations/#221-structured-data","title":"2.2.1 Structured Data","text":"<p>Structured data conforms to a fixed schema with predefined columns and data types. Every record has the same fields in the same order.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id\u2502 name            \u2502 age     \u2502 email      \u2502 signup_dt \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1          \u2502 Alice Smith     \u2502 28      \u2502 alice@...  \u2502 2024-01-15\u2502\n\u2502 2          \u2502 Bob Johnson     \u2502 35      \u2502 bob@...    \u2502 2024-02-20\u2502\n\u2502 3          \u2502 Carol Williams  \u2502 42      \u2502 carol@...  \u2502 2024-03-10\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Characteristics: - Schema defined before data insertion (schema-on-write) - Fixed columns with enforced data types - Naturally maps to relational database tables - Easily queried with SQL - Efficient storage due to predictable structure</p> <p>Common sources: Relational databases, ERP systems, financial records, CRM data</p>"},{"location":"lectures/Lecture01_Foundations/#222-semi-structured-data","title":"2.2.2 Semi-Structured Data","text":"<p>Semi-structured data has some organizational properties but does not conform to a rigid schema. It is self-describing, with tags or keys that identify data elements.</p> <pre><code>{\n  \"customer_id\": 1,\n  \"name\": \"Alice Smith\",\n  \"age\": 28,\n  \"contacts\": {\n    \"email\": \"alice@example.com\",\n    \"phone\": \"+1-555-0123\",\n    \"preferences\": {\n      \"newsletter\": true,\n      \"sms\": false\n    }\n  },\n  \"orders\": [\n    {\"order_id\": \"o1\", \"total\": 150.00, \"items\": 3},\n    {\"order_id\": \"o2\", \"total\": 89.99, \"items\": 1}\n  ],\n  \"tags\": [\"premium\", \"early_adopter\"]\n}\n</code></pre> <p>Characteristics: - Flexible schema (fields can vary between records) - Supports nested structures (objects within objects) - Self-describing (keys explain the data) - Schema can evolve without breaking existing data - Requires more sophisticated querying</p> <p>Common sources: JSON APIs, event tracking data, log files, NoSQL databases, configuration files</p>"},{"location":"lectures/Lecture01_Foundations/#223-unstructured-data","title":"2.2.3 Unstructured Data","text":"<p>Unstructured data lacks any predefined organizational structure. It must be processed or analyzed to extract meaningful information.</p> <p>Examples: - Text: Emails, documents, social media posts, customer reviews - Images: Photographs, scanned documents, medical imaging - Audio: Call recordings, podcasts, voice messages - Video: Surveillance footage, user-generated content, training videos</p> <p>Characteristics: - No inherent schema - Requires specialized processing (NLP, computer vision, etc.) - Often the largest volume of organizational data - High potential value, but difficult to extract</p> <p>Processing unstructured data:</p> <pre><code># Text analysis example\nraw_text = \"I absolutely love this product! Fast shipping and great quality.\"\n\n# After NLP processing, we extract structured information:\nprocessed = {\n    \"sentiment\": \"positive\",\n    \"sentiment_score\": 0.92,\n    \"topics\": [\"product_quality\", \"shipping\"],\n    \"entities\": [],\n    \"language\": \"en\"\n}\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#224-the-queryability-spectrum","title":"2.2.4 The Queryability Spectrum","text":"<pre><code>STRUCTURED                SEMI-STRUCTURED              UNSTRUCTURED\n    \u2502                           \u2502                            \u2502\n    \u2502  SELECT * FROM users      \u2502  $.orders[*].total         \u2502  [Requires ML/AI]\n    \u2502  WHERE age &gt; 25           \u2502  WHERE name = 'Alice'      \u2502  \n    \u2502                           \u2502                            \u2502\n    \u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n    Easy to Query                                    Difficult to Query\n    Rigid Schema                                     No Schema\n    Small Storage                                    Large Storage\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#23-file-formats-a-comprehensive-guide","title":"2.3 File Formats: A Comprehensive Guide","text":"<p>The choice of file format is one of the most consequential decisions in data engineering. This section provides deep coverage of the formats you will encounter.</p>"},{"location":"lectures/Lecture01_Foundations/#231-text-based-formats","title":"2.3.1 Text-Based Formats","text":""},{"location":"lectures/Lecture01_Foundations/#csv-comma-separated-values","title":"CSV (Comma-Separated Values)","text":"<p>CSV is the oldest and most universal data exchange format. Its simplicity is both its greatest strength and its most significant limitation.</p> <p>Structure: <pre><code>customer_id,name,age,signup_date,is_active\n1,Alice Smith,28,2024-01-15,true\n2,\"Johnson, Bob\",35,2024-02-20,true\n3,Carol Williams,42,2024-03-10,false\n</code></pre></p> <p>Technical Details: - Encoding: Typically UTF-8, but varies (beware of Excel's default encoding) - Delimiter: Comma by default, but tab (<code>\\t</code>) and pipe (<code>|</code>) are common alternatives - Quoting: Fields containing delimiters or newlines must be quoted - Headers: First row typically contains column names (but not guaranteed) - Types: All values stored as strings; types must be inferred or specified</p> <p>Advantages: - Universal compatibility (every tool can read CSV) - Human readable and editable - Simple to generate and parse - No dependencies or special libraries required</p> <p>Disadvantages:</p> <p>As noted in technical discussions: \"CSV is just a string, meaning the dataset is larger by storing all characters according to the file-encoding; there is no type-information or schema associated with the data, and it will always be parsed while deserialized\" (Stack Overflow, 2022).</p> <p>Additional limitations: - No standard specification (many dialects exist) - No support for nested or hierarchical data - Poor compression (text is verbose) - Ambiguous handling of null values - Date/time formats vary wildly - Encoding issues are common</p> <p>When to use: - Small datasets (under 100MB) - Data exchange with non-technical stakeholders - Legacy system integration - Quick prototyping and exploration</p> <p>Python usage: <pre><code>import pandas as pd\n\n# Reading with explicit options for reliability\ndf = pd.read_csv(\n    'data.csv',\n    encoding='utf-8',\n    dtype={'customer_id': 'int32', 'name': 'string'},\n    parse_dates=['signup_date'],\n    na_values=['', 'NULL', 'N/A']\n)\n\n# Writing with consistent formatting\ndf.to_csv('output.csv', index=False, date_format='%Y-%m-%d')\n</code></pre></p>"},{"location":"lectures/Lecture01_Foundations/#json-javascript-object-notation","title":"JSON (JavaScript Object Notation)","text":"<p>JSON emerged from JavaScript but has become the lingua franca of web APIs and configuration files. Its ability to represent nested structures makes it far more expressive than CSV.</p> <p>Structure: <pre><code>{\n  \"customers\": [\n    {\n      \"customer_id\": 1,\n      \"name\": \"Alice Smith\",\n      \"age\": 28,\n      \"contacts\": {\n        \"email\": \"alice@example.com\",\n        \"phone\": \"+1-555-0123\"\n      },\n      \"orders\": [\n        {\"order_id\": \"o1\", \"total\": 150.00},\n        {\"order_id\": \"o2\", \"total\": 89.99}\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"extracted_at\": \"2025-11-29T10:30:00Z\",\n    \"record_count\": 1\n  }\n}\n</code></pre></p> <p>Technical Details: - Data types: strings, numbers, booleans, null, arrays, objects - Encoding: Must be UTF-8 (per RFC 8259) - No comments: JSON does not support comments (though some parsers allow them) - Strict syntax: Trailing commas and single quotes are invalid</p> <p>Variants: - JSON Lines (JSONL/NDJSON): One JSON object per line, enabling streaming - GeoJSON: JSON format for geographic data - JSON Schema: Specification for validating JSON structure</p> <p>Advantages: - Human readable (more so than CSV for complex data) - Native support for nested structures - Self-describing (keys provide context) - Universal support in programming languages - Natural fit for API responses</p> <p>Disadvantages: - Verbose (keys repeated for every record) - No native date/time type (dates are strings) - Slow for large-scale analytics - Memory-intensive for large files (must often parse entirely)</p> <p>When to use: - API responses and requests - Configuration files - Document storage - Data interchange between systems - Event data (before analytical processing)</p> <p>Python usage: <pre><code>import json\nimport pandas as pd\nfrom pandas import json_normalize\n\n# Reading JSON\nwith open('data.json', 'r') as f:\n    data = json.load(f)\n\n# Flattening nested JSON to DataFrame\ndf = json_normalize(\n    data['customers'],\n    record_path='orders',\n    meta=['customer_id', 'name', ['contacts', 'email']],\n    meta_prefix='customer_'\n)\n\n# Reading JSON Lines (streaming-friendly)\ndf = pd.read_json('data.jsonl', lines=True)\n\n# Writing JSON\ndf.to_json('output.json', orient='records', indent=2, date_format='iso')\n</code></pre></p>"},{"location":"lectures/Lecture01_Foundations/#232-binary-columnar-formats","title":"2.3.2 Binary Columnar Formats","text":"<p>Binary columnar formats represent a paradigm shift from row-based storage. They are essential for modern analytics and big data processing.</p>"},{"location":"lectures/Lecture01_Foundations/#understanding-columnar-storage","title":"Understanding Columnar Storage","text":"<p>The fundamental insight of columnar storage is that analytical queries typically access many rows but few columns:</p> <pre><code>-- This query touches only 2 columns out of potentially dozens\nSELECT AVG(price), COUNT(*) \nFROM transactions \nWHERE transaction_date &gt;= '2025-01-01'\n</code></pre> <p>Row-based storage (CSV, Avro): <pre><code>Row 1: [id=1, name=\"Alice\", age=28, city=\"NYC\", salary=75000, ...]\nRow 2: [id=2, name=\"Bob\", age=35, city=\"LA\", salary=82000, ...]\nRow 3: [id=3, name=\"Carol\", age=42, city=\"Chicago\", salary=91000, ...]\n\nTo read 'age' column: Must scan through entire rows\n</code></pre></p> <p>Column-based storage (Parquet, ORC): <pre><code>id column:     [1, 2, 3, 4, 5, ...]\nname column:   [\"Alice\", \"Bob\", \"Carol\", ...]\nage column:    [28, 35, 42, ...]    \u2190 Read only this!\ncity column:   [\"NYC\", \"LA\", \"Chicago\", ...]\nsalary column: [75000, 82000, 91000, ...]\n\nTo read 'age' column: Read only the age column block\n</code></pre></p> <p>Benefits of columnar storage: 1. Column pruning: Read only the columns needed for a query 2. Better compression: Similar values in a column compress well together 3. Vectorized processing: CPUs can process columns more efficiently 4. Predicate pushdown: Skip entire row groups based on column statistics</p>"},{"location":"lectures/Lecture01_Foundations/#parquet","title":"Parquet","text":"<p>Apache Parquet is the dominant columnar format for analytics and the default format for Apache Spark. Understanding Parquet is essential for any data practitioner.</p> <p>File Structure:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           PARQUET FILE                                   \u2502\n\u2502                                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                        ROW GROUP 1                                 \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502  \u2502\n\u2502  \u2502  \u2502 Column Chunk\u2502  \u2502 Column Chunk\u2502  \u2502 Column Chunk\u2502                \u2502  \u2502\n\u2502  \u2502  \u2502   (id)      \u2502  \u2502   (name)    \u2502  \u2502   (age)     \u2502                \u2502  \u2502\n\u2502  \u2502  \u2502             \u2502  \u2502             \u2502  \u2502             \u2502                \u2502  \u2502\n\u2502  \u2502  \u2502 \u2022 Data Pages\u2502  \u2502 \u2022 Data Pages\u2502  \u2502 \u2022 Data Pages\u2502                \u2502  \u2502\n\u2502  \u2502  \u2502 \u2022 Dict Page \u2502  \u2502 \u2022 Dict Page \u2502  \u2502 \u2022 Statistics\u2502                \u2502  \u2502\n\u2502  \u2502  \u2502 \u2022 Statistics\u2502  \u2502 \u2022 Statistics\u2502  \u2502   (min/max) \u2502                \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                        ROW GROUP 2                                 \u2502  \u2502\n\u2502  \u2502  (same structure as Row Group 1)                                   \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                          FOOTER                                    \u2502  \u2502\n\u2502  \u2502  \u2022 File metadata                                                   \u2502  \u2502\n\u2502  \u2502  \u2022 Schema definition                                               \u2502  \u2502\n\u2502  \u2502  \u2022 Row group metadata                                              \u2502  \u2502\n\u2502  \u2502  \u2022 Column chunk locations                                          \u2502  \u2502\n\u2502  \u2502  \u2022 Key-value metadata (custom)                                     \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Concepts:</p> <ul> <li>Row Groups: Horizontal partitions of the data (typically 128MB-1GB each)</li> <li>Column Chunks: A column's data within a row group</li> <li>Pages: The smallest unit of storage within a column chunk (typically 1MB)</li> <li>Footer: Metadata including schema, statistics, and locations</li> </ul> <p>Schema Embedded in File:</p> <p>\"Parquet stores the file schema in the file metadata. CSV files don't store file metadata, so readers need to either be supplied with the schema or the schema needs to be inferred\" (MrPowers, Stack Overflow).</p> <p>Compression and Encoding:</p> <p>Parquet supports multiple compression codecs: - Snappy: Fast compression/decompression, moderate ratio (default) - Gzip: Higher compression ratio, slower - LZ4: Very fast, lower ratio - Zstd: Good balance of speed and ratio</p> <p>Parquet also uses encoding schemes optimized for each data type: - Dictionary encoding: For columns with repeated values - Run-length encoding (RLE): For sequences of repeated values - Delta encoding: For sorted or incrementing values</p> <p>Predicate Pushdown:</p> <p>Parquet's column statistics enable query engines to skip irrelevant data:</p> <pre><code># Query: SELECT * FROM sales WHERE sale_date = '2025-11-29'\n\n# Row Group 1 statistics: sale_date min='2025-01-01', max='2025-06-30'\n#   \u2192 SKIP (date not in range)\n\n# Row Group 2 statistics: sale_date min='2025-07-01', max='2025-12-31'\n#   \u2192 READ (date might be in range)\n</code></pre> <p>When to use Parquet: - Analytical queries and data warehousing - Data lake storage - Spark, Presto, Athena, BigQuery workloads - Any dataset over 100MB used for analysis - Long-term data archival</p> <p>\"Parquet is a default data file format for Spark\" (Towards Data Science, 2023).</p> <p>Python usage: <pre><code>import pandas as pd\nimport pyarrow.parquet as pq\n\n# Reading Parquet\ndf = pd.read_parquet('data.parquet')\n\n# Reading specific columns only (column pruning)\ndf = pd.read_parquet('data.parquet', columns=['customer_id', 'age', 'revenue'])\n\n# Writing Parquet with options\ndf.to_parquet(\n    'output.parquet',\n    engine='pyarrow',\n    compression='snappy',\n    index=False\n)\n\n# Advanced: Reading with filters (predicate pushdown)\ndf = pd.read_parquet(\n    'data.parquet',\n    filters=[('year', '=', 2025), ('country', 'in', ['US', 'UK'])]\n)\n\n# PySpark usage\nspark_df = spark.read.parquet('s3://bucket/data/')\nspark_df.write.parquet('s3://bucket/output/', mode='overwrite')\n</code></pre></p>"},{"location":"lectures/Lecture01_Foundations/#avro","title":"Avro","text":"<p>Apache Avro is a row-based binary format designed for data serialization, particularly in streaming contexts.</p> <p>Structure:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            AVRO FILE                                     \u2502\n\u2502                                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                           HEADER                                   \u2502  \u2502\n\u2502  \u2502  \u2022 Magic bytes (Obj1)                                              \u2502  \u2502\n\u2502  \u2502  \u2022 Schema (JSON)                                                   \u2502  \u2502\n\u2502  \u2502  \u2022 Sync marker (16 bytes)                                          \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                        DATA BLOCK 1                                \u2502  \u2502\n\u2502  \u2502  \u2022 Object count                                                    \u2502  \u2502\n\u2502  \u2502  \u2022 Serialized objects (compressed)                                 \u2502  \u2502\n\u2502  \u2502  \u2022 Sync marker                                                     \u2502  \u2502\n\u2502  \u2502                                                                    \u2502  \u2502\n\u2502  \u2502  Row 1: [1, \"Alice\", 28, \"2024-01-15\"]                            \u2502  \u2502\n\u2502  \u2502  Row 2: [2, \"Bob\", 35, \"2024-02-20\"]                               \u2502  \u2502\n\u2502  \u2502  ...                                                               \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                        DATA BLOCK 2                                \u2502  \u2502\n\u2502  \u2502  (same structure)                                                  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Schema Definition:</p> <p>Avro schemas are defined in JSON:</p> <pre><code>{\n  \"type\": \"record\",\n  \"name\": \"Customer\",\n  \"namespace\": \"com.example\",\n  \"fields\": [\n    {\"name\": \"customer_id\", \"type\": \"int\"},\n    {\"name\": \"name\", \"type\": \"string\"},\n    {\"name\": \"age\", \"type\": [\"null\", \"int\"], \"default\": null},\n    {\"name\": \"signup_date\", \"type\": {\n      \"type\": \"int\",\n      \"logicalType\": \"date\"\n    }},\n    {\"name\": \"tags\", \"type\": {\"type\": \"array\", \"items\": \"string\"}}\n  ]\n}\n</code></pre> <p>Schema Evolution:</p> <p>Avro's killer feature is robust schema evolution. You can: - Add fields (with defaults) without breaking readers - Remove fields without breaking readers (if they have defaults) - Rename fields using aliases</p> <p>This makes Avro ideal for streaming systems where producers and consumers may be updated independently.</p> <p>Compression:</p> <p>\"Avro uses a binary format which benefits data compaction. Binary data is highly compact compared to JSON or XML formats. Hence the speed of serialization and deserialization also increases\" (Hevo Data, 2024).</p> <p>When to use Avro: - Apache Kafka message serialization - Real-time streaming pipelines - Systems requiring schema evolution - Write-heavy workloads - Data interchange between services</p> <p>Python usage: <pre><code>from fastavro import writer, reader, parse_schema\n\n# Define schema\nschema = {\n    \"type\": \"record\",\n    \"name\": \"Customer\",\n    \"fields\": [\n        {\"name\": \"customer_id\", \"type\": \"int\"},\n        {\"name\": \"name\", \"type\": \"string\"},\n        {\"name\": \"age\", \"type\": \"int\"}\n    ]\n}\nparsed_schema = parse_schema(schema)\n\n# Write Avro\nrecords = [\n    {\"customer_id\": 1, \"name\": \"Alice\", \"age\": 28},\n    {\"customer_id\": 2, \"name\": \"Bob\", \"age\": 35}\n]\n\nwith open('customers.avro', 'wb') as f:\n    writer(f, parsed_schema, records)\n\n# Read Avro\nwith open('customers.avro', 'rb') as f:\n    for record in reader(f):\n        print(record)\n</code></pre></p>"},{"location":"lectures/Lecture01_Foundations/#233-format-comparison","title":"2.3.3 Format Comparison","text":"<p>Storage Efficiency:</p> <p>Testing with identical data reveals dramatic differences:</p> <p>\"Both CSV and JSON are losing a lot compared to Avro and Parquet, however, this is expected because both Avro and Parquet are binary formats (they also use compression) while CSV and JSON are not compressed\" (DataCrump, 2023).</p> Format Relative Size Notes JSON 140% Keys repeated, no compression CSV 100% Baseline (uncompressed) CSV (gzip) 15-25% Compressed text Avro 25-40% Binary, row-based Parquet 10-20% Binary, columnar, excellent compression <p>Query Performance:</p> <p>For analytical queries selecting specific columns:</p> Format Relative Query Time CSV 100% (baseline) JSON 120-150% Avro 50-70% Parquet 5-15% <p>The dramatic improvement with Parquet comes from column pruning and predicate pushdown.</p> <p>Decision Framework:</p> <p>\"CSV and JSON are suitable for small datasets (&lt;1,000,000 rows) or quick implementations, while Parquet, Avro, or ORC are better for large datasets with specific data behaviors\" (Medium, 2024).</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502         FORMAT SELECTION GUIDE          \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                       \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502        Is the data &gt; 100MB?             \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                      \u2502 NO                              \u2502 YES\n                      \u25bc                                 \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Need human editing?  \u2502          \u2502 Streaming/real-time? \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                      \u2502                              \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502 YES            NO   \u2502        \u2502 YES           NO  \u2502\n           \u25bc                \u25bc    \u2502        \u25bc               \u25bc   \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n        \u2502 CSV  \u2502        \u2502 JSON \u2502\u2502     \u2502 AVRO \u2502       \u2502PARQUET\u2502\n        \u2502      \u2502        \u2502      \u2502\u2502     \u2502      \u2502       \u2502      \u2502\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n                                \u2502                            \u2502\n                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#24-data-types-precision-and-efficiency","title":"2.4 Data Types: Precision and Efficiency","text":"<p>Within any format, data is represented using specific types. Understanding types improves both correctness and performance.</p>"},{"location":"lectures/Lecture01_Foundations/#241-numeric-types","title":"2.4.1 Numeric Types","text":"Type Size Range Use Case <code>int8</code> / <code>tinyint</code> 1 byte -128 to 127 Age, small counts <code>int16</code> / <code>smallint</code> 2 bytes -32,768 to 32,767 Year, medium counts <code>int32</code> / <code>int</code> 4 bytes \u00b12.1 billion Most integers <code>int64</code> / <code>bigint</code> 8 bytes \u00b19.2 quintillion Large IDs, timestamps <code>float32</code> / <code>float</code> 4 bytes ~7 decimal digits Approximate values <code>float64</code> / <code>double</code> 8 bytes ~15 decimal digits Scientific, financial <code>decimal</code> Variable Exact precision Currency, percentages <p>Choosing the right numeric type:</p> <pre><code>import pandas as pd\nimport numpy as np\n\n# Memory comparison\nn = 1_000_000\n\n# Using default int64 (8 bytes \u00d7 1M = 8MB)\nages_int64 = pd.Series(np.random.randint(0, 100, n), dtype='int64')\nprint(f\"int64: {ages_int64.memory_usage() / 1e6:.1f} MB\")  # 8.0 MB\n\n# Using int8 (1 byte \u00d7 1M = 1MB) - ages fit in 0-127\nages_int8 = ages_int64.astype('int8')\nprint(f\"int8: {ages_int8.memory_usage() / 1e6:.1f} MB\")    # 1.0 MB\n\n# 8x memory savings!\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#242-string-types","title":"2.4.2 String Types","text":"<p>Strings are often the largest memory consumers in datasets:</p> <pre><code># String optimization with categories\ndf = pd.DataFrame({\n    'country': np.random.choice(['USA', 'UK', 'Germany', 'France'], 1_000_000)\n})\n\n# Default object dtype\nprint(f\"Object: {df['country'].memory_usage(deep=True) / 1e6:.1f} MB\")  # ~64 MB\n\n# Category dtype (for low-cardinality strings)\ndf['country'] = df['country'].astype('category')\nprint(f\"Category: {df['country'].memory_usage(deep=True) / 1e6:.1f} MB\")  # ~1 MB\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#243-date-and-time-types","title":"2.4.3 Date and Time Types","text":"<p>Temporal data requires careful handling:</p> Type Storage Precision Example <code>date</code> 4 bytes Day 2025-11-29 <code>datetime64[ns]</code> 8 bytes Nanosecond 2025-11-29 14:30:15.123456789 <code>timestamp</code> 8 bytes Varies Unix epoch milliseconds <code>time</code> Variable Sub-second 14:30:15.123 <code>interval</code> Variable Duration 3 days, 4 hours <p>Best practices: - Store timestamps in UTC - Use ISO 8601 format for string representation - Be explicit about timezone handling - Choose appropriate precision (do you need nanoseconds?)</p> <pre><code>import pandas as pd\n\n# Parse dates explicitly\ndf = pd.read_csv('data.csv', parse_dates=['created_at', 'updated_at'])\n\n# Handle timezones\ndf['created_at'] = pd.to_datetime(df['created_at'], utc=True)\ndf['created_at_local'] = df['created_at'].dt.tz_convert('America/New_York')\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#244-boolean-and-null-types","title":"2.4.4 Boolean and Null Types","text":"<ul> <li>Boolean: <code>true</code>/<code>false</code> - ideally 1 bit, often stored as 1 byte</li> <li>Null/None: Represents missing data - handling varies by system</li> </ul> <p>Null handling strategies:</p> Strategy When to Use Keep as null Value is genuinely unknown Default value Reasonable default exists (0 for counts) Sentinel value -1, \"UNKNOWN\", empty string Imputation Statistical estimate appropriate Exclusion Analysis can proceed without the record"},{"location":"lectures/Lecture01_Foundations/#25-working-with-nested-data","title":"2.5 Working with Nested Data","text":"<p>Semi-structured data with nested objects and arrays requires special handling to prepare for analysis.</p>"},{"location":"lectures/Lecture01_Foundations/#251-the-nested-data-challenge","title":"2.5.1 The Nested Data Challenge","text":"<p>Consider this common e-commerce data structure:</p> <pre><code>{\n  \"order_id\": \"ord_12345\",\n  \"customer\": {\n    \"id\": \"cust_789\",\n    \"name\": \"Alice Smith\",\n    \"address\": {\n      \"street\": \"123 Main St\",\n      \"city\": \"San Francisco\",\n      \"state\": \"CA\",\n      \"zip\": \"94102\"\n    }\n  },\n  \"items\": [\n    {\n      \"product_id\": \"prod_001\",\n      \"name\": \"Wireless Headphones\",\n      \"price\": 249.99,\n      \"quantity\": 1\n    },\n    {\n      \"product_id\": \"prod_002\",\n      \"name\": \"Phone Case\",\n      \"price\": 29.99,\n      \"quantity\": 2\n    }\n  ],\n  \"totals\": {\n    \"subtotal\": 309.97,\n    \"tax\": 27.90,\n    \"shipping\": 0,\n    \"total\": 337.87\n  }\n}\n</code></pre> <p>This cannot be directly loaded into a flat table. We need flattening strategies.</p>"},{"location":"lectures/Lecture01_Foundations/#252-flattening-strategies","title":"2.5.2 Flattening Strategies","text":"<p>Strategy 1: Dot notation flattening (nested objects)</p> <pre><code>from pandas import json_normalize\n\ndata = {...}  # The JSON above\n\n# Flatten nested objects\ndf = json_normalize(data)\n\n# Result:\n# order_id | customer.id | customer.name | customer.address.street | ... | items | totals.total\n# ord_12345| cust_789    | Alice Smith   | 123 Main St             | ... | [...]  | 337.87\n</code></pre> <p>Strategy 2: Exploding arrays</p> <pre><code># Explode the items array\ndf = json_normalize(\n    data,\n    record_path='items',                    # Array to explode\n    meta=['order_id',                       # Fields to keep from parent\n          ['customer', 'id'],\n          ['customer', 'name'],\n          ['totals', 'total']],\n    meta_prefix='order_'                    # Prefix for parent fields\n)\n\n# Result:\n# product_id | name               | price  | quantity | order_order_id | order_customer.id | order_totals.total\n# prod_001   | Wireless Headphones| 249.99 | 1        | ord_12345      | cust_789          | 337.87\n# prod_002   | Phone Case         | 29.99  | 2        | ord_12345      | cust_789          | 337.87\n</code></pre> <p>Strategy 3: PySpark approach</p> <pre><code>from pyspark.sql import functions as F\n\n# Read nested JSON\ndf = spark.read.json(\"orders.json\")\n\n# Access nested fields with dot notation\ndf.select(\n    \"order_id\",\n    \"customer.id\",\n    \"customer.name\",\n    \"customer.address.city\"\n)\n\n# Explode arrays\ndf_items = df.select(\n    \"order_id\",\n    \"customer.id\",\n    F.explode(\"items\").alias(\"item\")\n).select(\n    \"order_id\",\n    \"id\",\n    \"item.product_id\",\n    \"item.name\",\n    \"item.price\",\n    \"item.quantity\"\n)\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#253-when-to-flatten-vs-keep-nested","title":"2.5.3 When to Flatten vs. Keep Nested","text":"Keep Nested Flatten Document databases (MongoDB) Analytical queries API responses Machine learning features Event archives Reporting and BI Schema flexibility needed SQL-based analysis"},{"location":"lectures/Lecture01_Foundations/#26-summary-and-key-concepts","title":"2.6 Summary and Key Concepts","text":"<p>This chapter provided comprehensive coverage of data types and file formats:</p>"},{"location":"lectures/Lecture01_Foundations/#data-structure-spectrum","title":"Data Structure Spectrum","text":"<ul> <li>Structured: Fixed schema, tables, SQL-queryable</li> <li>Semi-structured: Flexible, nested, self-describing (JSON, XML)</li> <li>Unstructured: No schema, requires ML/NLP to process</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#file-format-selection","title":"File Format Selection","text":"<ul> <li>CSV: Universal but limited\u2014use for small data and interchange</li> <li>JSON: Flexible, nested support\u2014use for APIs and configuration</li> <li>Parquet: Columnar, compressed\u2014use for analytics and data lakes</li> <li>Avro: Row-based, schema evolution\u2014use for streaming and Kafka</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#key-insight","title":"Key Insight","text":"<p>\"JSON has the largest footprint because it stores the schema attributes for each row. For this reason, I rarely store JSON or CSV formats in curated and transformed zone in a data lake\" (Towards Data Science, 2023).</p>"},{"location":"lectures/Lecture01_Foundations/#27-further-reading-and-resources","title":"2.7 Further Reading and Resources","text":""},{"location":"lectures/Lecture01_Foundations/#papers-and-articles","title":"Papers and Articles","text":"<ul> <li>Mbata, A. et al. (2024). \"A Survey of Pipeline Tools for Data Engineering.\" arXiv:2406.08335. Comprehensive survey of data engineering tools and formats. https://arxiv.org/abs/2406.08335</li> <li>Wickham, H. (2014). \"Tidy Data.\" Journal of Statistical Software. Foundational paper on data organization principles. https://vita.had.co.nz/papers/tidy-data.pdf</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#technical-documentation","title":"Technical Documentation","text":"<ul> <li>Apache Parquet Documentation: https://parquet.apache.org/docs/</li> <li>Apache Avro Specification: https://avro.apache.org/docs/current/spec.html</li> <li>JSON Specification (RFC 8259): https://datatracker.ietf.org/doc/html/rfc8259</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#practical-comparisons","title":"Practical Comparisons","text":"<ul> <li>DataCrump: CSV vs Parquet vs JSON vs Avro: Hands-on performance comparison with benchmarks. https://datacrump.com/csv-parquet-json-avro/</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#books_1","title":"Books","text":"<ul> <li>Reis, J. &amp; Housley, M. (2022). Fundamentals of Data Engineering. O'Reilly Media. Chapter 5 covers data formats in depth.</li> <li>Kleppmann, M. (2017). Designing Data-Intensive Applications. O'Reilly Media. Chapter 4 discusses encoding and evolution.</li> </ul>"},{"location":"lectures/Lecture01_Foundations/#28-exercises","title":"2.8 Exercises","text":"<p>Exercise 2.1: Format Conversion and Comparison</p> <p>Take a CSV file of at least 100,000 rows and: 1. Convert it to JSON, Parquet, and Avro formats 2. Compare file sizes for each format 3. Measure read times for loading into pandas 4. Apply compression (gzip for CSV/JSON, snappy for Parquet) and compare again</p> <pre><code>import pandas as pd\nimport time\nimport os\n\n# Your code here\ndf = pd.read_csv('your_data.csv')\n\n# Save in different formats and measure\nformats = {\n    'csv': lambda: df.to_csv('output.csv', index=False),\n    'json': lambda: df.to_json('output.json', orient='records'),\n    'parquet': lambda: df.to_parquet('output.parquet'),\n}\n\nfor name, save_func in formats.items():\n    start = time.time()\n    save_func()\n    elapsed = time.time() - start\n    size = os.path.getsize(f'output.{name}') / 1e6\n    print(f\"{name}: {size:.2f} MB, {elapsed:.2f} seconds\")\n</code></pre> <p>Exercise 2.2: Nested Data Transformation</p> <p>Given this nested JSON structure, create a flat DataFrame suitable for analysis:</p> <pre><code>{\n  \"users\": [\n    {\n      \"id\": 1,\n      \"name\": \"Alice\",\n      \"profile\": {\n        \"age\": 28,\n        \"occupation\": \"Engineer\"\n      },\n      \"sessions\": [\n        {\"date\": \"2025-01-01\", \"duration_minutes\": 45, \"pages_viewed\": 12},\n        {\"date\": \"2025-01-02\", \"duration_minutes\": 30, \"pages_viewed\": 8}\n      ]\n    },\n    {\n      \"id\": 2,\n      \"name\": \"Bob\",\n      \"profile\": {\n        \"age\": 35,\n        \"occupation\": \"Designer\"\n      },\n      \"sessions\": [\n        {\"date\": \"2025-01-01\", \"duration_minutes\": 60, \"pages_viewed\": 20}\n      ]\n    }\n  ]\n}\n</code></pre> <p>Create two DataFrames: 1. User-level DataFrame with profile information 2. Session-level DataFrame with user context</p> <p>Exercise 2.3: Schema Design</p> <p>Design a data schema for a ride-sharing application (like Uber/Lyft). Consider: - What events should be tracked? - What fields should each event have? - What data types are appropriate for each field? - How should nested data (e.g., route waypoints) be structured?</p> <p>Document your schema in JSON Schema format.</p> <p>Exercise 2.4: Type Optimization</p> <p>Given a DataFrame with default types, optimize memory usage by selecting appropriate types:</p> <pre><code>import pandas as pd\nimport numpy as np\n\n# Create sample data with suboptimal types\ndf = pd.DataFrame({\n    'user_id': np.random.randint(1, 1000000, 1000000),        # Could be int32\n    'age': np.random.randint(18, 80, 1000000),                # Could be int8\n    'country': np.random.choice(['US', 'UK', 'DE', 'FR'], 1000000),  # Could be category\n    'is_premium': np.random.choice([True, False], 1000000),   # Already boolean\n    'balance': np.random.uniform(0, 10000, 1000000),          # Could be float32\n})\n\nprint(f\"Original memory: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n\n# Your optimization code here\n# Target: Reduce memory by at least 50%\n</code></pre>"},{"location":"lectures/Lecture01_Foundations/#end-of-part-i-foundations","title":"End of Part I: Foundations","text":"<p>The next part of this course will cover Data Collection and Storage, including: - Chapter 3: Data Sources and Collection Methods - Chapter 4: Relational Databases and SQL - Chapter 5: Data Warehouses, Data Lakes, and Modern Storage Architectures</p>"},{"location":"lectures/course_outline/","title":"Data Science Applications Course","text":"<p>Duration: 4 months</p>"},{"location":"lectures/course_outline/#module-1-data-science-fundamentals-and-exploratory-data-analysis-eda","title":"Module 1: Data Science Fundamentals and Exploratory Data Analysis (EDA)","text":""},{"location":"lectures/course_outline/#11-introduction-to-data-science-tools-and-environments","title":"1.1 Introduction to Data Science Tools and Environments","text":"<p>Key Topics: - Overview of Data Science Roles and Tools</p>"},{"location":"lectures/course_outline/#12-big-data-technologies-and-data-engineering","title":"1.2 Big Data Technologies and Data Engineering","text":"<p>Key Topics: - Introduction to Big Data Ecosystem (Hadoop, Spark) - Intro to Databases - Introduction to SQL for Data Retrieval and Manipulation - Introduction to NoSQL Databases (MongoDB) - Data Processing with Apache Spark (PySpark) - Building ETL Pipelines for Big Data - Practical Sessions: Working with Big Data Tools and Techniques</p>"},{"location":"lectures/course_outline/#13-full-etl-and-eda-on-real-world-datasets","title":"1.3 Full ETL and EDA on Real-World Datasets","text":""},{"location":"lectures/course_outline/#module-2-advanced-statistical-methods-and-machine-learning-for-data-science","title":"Module 2: Advanced Statistical Methods and Machine Learning for Data Science","text":""},{"location":"lectures/course_outline/#21-advanced-statistical-methods","title":"2.1 Advanced Statistical Methods","text":"<p>Key Topics: - Hypothesis Testing (T-tests, Chi-Square Tests) - Analysis of Variance (ANOVA) - Regression Analysis (Linear, Logistic, and Polynomial Regression) - Time Series Analysis and Forecasting - Practice: Applying Advanced Statistical Methods</p>"},{"location":"lectures/course_outline/#22-machine-learning-algorithms-for-data-science","title":"2.2 Machine Learning Algorithms for Data Science","text":"<p>Key Topics: - Supervised Learning Algorithms (Decision Trees, Random Forests, Gradient Boosting Machines) - Unsupervised Learning Algorithms (K-Means, Hierarchical Clustering, DBSCAN) - Introduction to Ensemble Methods - Model Evaluation and Validation Techniques (Cross-Validation, ROC-AUC, Precision-Recall) - Practice: Implementing ML Algorithms on Real Datasets</p>"},{"location":"lectures/course_outline/#module-3-deep-learning-for-data-science","title":"Module 3: Deep Learning for Data Science","text":""},{"location":"lectures/course_outline/#31-introduction-to-deep-learning-and-specialized-applications","title":"3.1 Introduction to Deep Learning and Specialized Applications","text":"<p>Key Topics: - Fundamentals of Neural Networks and Deep Learning - Building and Training Deep Learning Models (Using TensorFlow/Keras, PyTorch) - Convolutional Neural Networks (CNNs) for Image Data - Recurrent Neural Networks (RNNs) for Time-Series Data - Natural Language Processing (NLP) for Text Data - Advanced Time Series Analysis and Forecasting Techniques - Practical Sessions: Developing and Tuning Deep Learning Models</p>"},{"location":"lectures/course_outline/#module-4-capstone-project","title":"Module 4: Capstone Project","text":""},{"location":"lectures/course_outline/#41-capstone-project-preparation-execution-and-implementation","title":"4.1 Capstone Project Preparation, Execution, and Implementation","text":"<p>Key Topics: 1. Project Planning and Design 2. Data Collection and Preprocessing Strategies 3. Model Selection and Evaluation Plan 4. Proposal Presentation and Feedback 5. Building and Refining the Data Science Solution 6. Model Training, Tuning, and Evaluation 7. Deployment of the Model (on Cloud or On-Premise) 8. Preparing Final Presentation and Documentation</p>"},{"location":"lectures/course_outline/#42-capstone-project-completion-presentation-and-review","title":"4.2 Capstone Project Completion, Presentation, and Review","text":"<p>Key Topics: 1. Project Presentation to Peers and Instructors 2. Feedback and Q&amp;A Sessions 3. Final Grading and Evaluation 4. Course Wrap-Up and Future Directions in Data Science</p>"},{"location":"lectures/course_outline/#course-overview-summary","title":"Course Overview Summary","text":"Module Focus Area Duration Module 1 Data Science Fundamentals &amp; EDA ~4-6 weeks Module 2 Advanced Statistics &amp; Machine Learning ~6-8 weeks Module 3 Deep Learning ~4-6 weeks Module 4 Capstone Project 4 weeks Total Complete Course 4 months"},{"location":"lectures/course_outline/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this course, students will be able to:</p> <p>\u2705 Work with big data technologies (Hadoop, Spark, PySpark) \u2705 Design and implement ETL pipelines \u2705 Perform comprehensive exploratory data analysis \u2705 Apply advanced statistical methods and hypothesis testing \u2705 Build and evaluate machine learning models \u2705 Develop deep learning solutions for various data types \u2705 Deploy data science solutions to production \u2705 Complete an end-to-end data science project from conception to deployment</p>"},{"location":"lectures/course_outline/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic Python programming knowledge</li> <li>Fundamental statistics and mathematics</li> <li>Understanding of basic data structures and algorithms</li> <li>Familiarity with Jupyter notebooks (recommended)</li> </ul>"},{"location":"lectures/course_outline/#tools-and-technologies-covered","title":"Tools and Technologies Covered","text":""},{"location":"lectures/course_outline/#programming-languages","title":"Programming Languages","text":"<ul> <li>Python (primary)</li> <li>SQL</li> </ul>"},{"location":"lectures/course_outline/#big-data-technologies","title":"Big Data Technologies","text":"<ul> <li>Apache Hadoop</li> <li>Apache Spark</li> <li>PySpark</li> </ul>"},{"location":"lectures/course_outline/#databases","title":"Databases","text":"<ul> <li>SQL databases</li> <li>MongoDB (NoSQL)</li> </ul>"},{"location":"lectures/course_outline/#machine-learning-frameworks","title":"Machine Learning Frameworks","text":"<ul> <li>Scikit-learn</li> <li>TensorFlow</li> <li>Keras</li> <li>PyTorch</li> </ul>"},{"location":"lectures/course_outline/#development-tools","title":"Development Tools","text":"<ul> <li>Jupyter Notebooks</li> <li>Git/GitHub</li> <li>Cloud platforms (AWS/Azure/GCP)</li> </ul>"},{"location":"lectures/course_outline/#assessment-methods","title":"Assessment Methods","text":"<ol> <li>Hands-on Assignments - Practical exercises throughout each module</li> <li>Project Work - Real-world dataset analysis and model building</li> <li>Capstone Project - Comprehensive end-to-end data science project</li> <li>Presentations - Project proposals and final presentations</li> <li>Peer Review - Collaborative learning and feedback sessions</li> </ol>"}]}