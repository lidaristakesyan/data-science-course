{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Data Science Course","text":"<p>Welcome to the Data Science Course \ud83c\udf31</p> <p>This site contains:</p> <ul> <li>\ud83d\udcda Lecture notes  </li> <li>\ud83d\udcbb Code &amp; notebooks  </li> <li>\ud83d\udcdd Homework &amp; exercises  </li> <li>\ud83d\udcca Slides &amp; visualizations  </li> </ul>"},{"location":"#structure","title":"Structure","text":"<ul> <li>Lectures \u2013 theory and examples  </li> <li>Notebooks \u2013 hands-on code in Jupyter  </li> <li>Homework \u2013 assignments and solutions  </li> <li>Slides \u2013 PDF / HTML slides for each week</li> </ul>"},{"location":"lectures/","title":"Course structure","text":"<p>Complete notes for the PA Academy Data Science course.</p>"},{"location":"lectures/#course-outline","title":"Course Outline","text":"<ul> <li>Course Outline</li> </ul>"},{"location":"lectures/#lecture-notes","title":"Lecture Notes","text":"<ul> <li>Lecture 01: Introduction to Data</li> <li>Lecture 01: Introduction to Big Data Ecosystem</li> </ul>"},{"location":"lectures/#slides","title":"Slides","text":"<ul> <li>Lecture 01 Slides</li> </ul> <p>More lectures will be added throughout the course period.</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/","title":"Introduction to Big Data Ecosystem (Hadoop &amp; Spark)","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Understanding Big Data</li> <li>Why Traditional Databases Fail</li> <li>The Big Data Solution: Hadoop</li> <li>Understanding MapReduce</li> <li>Evolution to Apache Spark</li> <li>Hadoop vs Spark Comparison</li> <li>Hands-On Setup Guide</li> <li>Real-World Examples</li> <li>Learning Path &amp; Resources</li> </ol>"},{"location":"lectures/Lecture01_big_data_ecosystem/#1-understanding-big-data","title":"1. Understanding Big Data","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#what-is-big-data","title":"What is Big Data?","text":"<p>Big Data refers to datasets that are so large, complex, or fast-moving that traditional data processing tools cannot handle them effectively.</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#the-5-vs-of-big-data","title":"The 5 V's of Big Data","text":"<p>Volume - Massive amounts of data - Example: Facebook generates 4 petabytes of data per day - Traditional databases: Handle gigabytes to terabytes - Big Data: Handle petabytes to exabytes</p> <p>Velocity - Speed of data generation and processing - Example: Twitter generates 500 million tweets per day - Traditional: Batch processing (hours/days) - Big Data: Real-time streaming processing</p> <p>Variety - Different types of data - Structured: SQL databases, spreadsheets (rows &amp; columns) - Semi-structured: JSON, XML, logs - Unstructured: Videos, images, social media posts, text</p> <p>Veracity - Data quality and trustworthiness - Dealing with incomplete or inconsistent data - Example: Sensor data with missing readings</p> <p>Value - Extracting meaningful insights - Raw data \u2192 Processed data \u2192 Actionable insights - Example: Netflix recommendations from viewing history</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#real-world-big-data-examples","title":"Real-World Big Data Examples","text":"Industry Use Case Data Volume E-commerce Amazon product recommendations 100+ million products Social Media Facebook user analytics 3 billion users Healthcare Medical imaging analysis Terabytes per hospital Finance Fraud detection Millions of transactions/second Transportation Uber ride matching 15 million trips/day"},{"location":"lectures/Lecture01_big_data_ecosystem/#2-why-traditional-databases-fail","title":"2. Why Traditional Databases Fail","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#the-traditional-approach-problem","title":"The Traditional Approach Problem","text":"<p>Scenario: You have 1 TB of customer transaction data to analyze</p> <p>Traditional Relational Database (MySQL/PostgreSQL): <pre><code>Single Server Limitations:\n\u251c\u2500\u2500 Storage: Limited to one machine's disk capacity\n\u251c\u2500\u2500 Processing: Limited to one machine's CPU cores\n\u251c\u2500\u2500 Memory: Limited to one machine's RAM\n\u2514\u2500\u2500 Failure Point: If server crashes, everything stops\n</code></pre></p> <p>Problems: 1. Vertical Scaling Only (Scale Up)    - Buy bigger, more expensive servers    - Eventually hit hardware limits    - Very costly ($100,000+ for high-end servers)</p> <ol> <li>Single Point of Failure</li> <li>One server = one failure point</li> <li> <p>No redundancy</p> </li> <li> <p>Slow Processing</p> </li> <li>Processing 1 TB on single machine: Hours/Days</li> <li> <p>Can't process multiple files simultaneously</p> </li> <li> <p>Schema Rigidity</p> </li> <li>Must define fixed structure (tables, columns)</li> <li>Hard to handle unstructured data (videos, logs, JSON)</li> </ol>"},{"location":"lectures/Lecture01_big_data_ecosystem/#example-the-facebook-problem-2008","title":"Example: The Facebook Problem (2008)","text":"<p>Challenge: Process billions of photos uploaded daily</p> <p>Traditional Database Approach: - Single powerful server: $500,000 - Processing time: 24 hours for daily batch - Storage capacity: 10 TB maximum - Result: Can't keep up with growth</p> <p>Big Data Approach (Hadoop/Spark): - 1,000 commodity servers: $500,000 total - Processing time: 1-2 hours (parallel processing) - Storage capacity: 10 PB+ (easily expandable) - Result: Scales with data growth</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#3-the-big-data-solution-hadoop","title":"3. The Big Data Solution: Hadoop","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#what-is-hadoop","title":"What is Hadoop?","text":"<p>Apache Hadoop is an open-source framework written in Java that allows distributed storage and processing of large datasets across clusters of computers using simple programming models.</p> <p>Key Concept: Instead of bringing data to one big computer, Hadoop brings computation to where data already lives (distributed processing).</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#core-hadoop-components","title":"Core Hadoop Components","text":"<pre><code>Hadoop Ecosystem\n\u2502\n\u251c\u2500\u2500 HDFS (Hadoop Distributed File System)\n\u2502   \u2514\u2500\u2500 Storage Layer: How data is stored\n\u2502\n\u251c\u2500\u2500 YARN (Yet Another Resource Negotiator)\n\u2502   \u2514\u2500\u2500 Resource Management: Manages cluster resources\n\u2502\n\u2514\u2500\u2500 MapReduce\n    \u2514\u2500\u2500 Processing Layer: How data is processed\n</code></pre>"},{"location":"lectures/Lecture01_big_data_ecosystem/#31-hdfs-hadoop-distributed-file-system","title":"3.1 HDFS: Hadoop Distributed File System","text":"<p>HDFS breaks up large data into smaller chunks and distributes those chunks across different nodes in a cluster, keeping multiple copies of data on different nodes for redundancy.</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#how-hdfs-works","title":"How HDFS Works","text":"<p>Traditional File System: <pre><code>Single Server:\nfile.txt (1 GB) \u2192 Stored on one disk\n</code></pre></p> <p>HDFS Approach: <pre><code>Cluster of 4 Servers:\n\nfile.txt (1 GB) split into:\n\u251c\u2500\u2500 Block 1 (128 MB) \u2192 Server 1, Server 2, Server 3 (replicated)\n\u251c\u2500\u2500 Block 2 (128 MB) \u2192 Server 2, Server 3, Server 4 (replicated)\n\u251c\u2500\u2500 Block 3 (128 MB) \u2192 Server 1, Server 3, Server 4 (replicated)\n\u2514\u2500\u2500 ... more blocks\n</code></pre></p> <p>Default Block Size: 128 MB (configurable) Default Replication Factor: 3 copies of each block</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#hdfs-architecture","title":"HDFS Architecture","text":"<p>Two Main Components:</p> <p>1. NameNode (Master) - Brain of HDFS - Stores metadata (file locations, permissions) - Tracks which DataNodes have which blocks - Does NOT store actual data</p> <p>2. DataNodes (Workers/Slaves) - Store actual data blocks - Send heartbeats to NameNode every 3 seconds - If DataNode fails, NameNode redistributes its blocks</p> <p>Diagram: <pre><code>                    [NameNode]\n                   (Metadata)\n                        |\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        |               |               |\n   [DataNode 1]    [DataNode 2]    [DataNode 3]\n   Block A1        Block A2        Block A1\n   Block B2        Block A1        Block B1\n   Block C1        Block C2        Block C1\n</code></pre></p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#benefits-of-hdfs","title":"Benefits of HDFS","text":"<p>\u2705 Fault Tolerance: If DataNode fails, data is still available from replicas \u2705 Scalability: Add more DataNodes to increase storage \u2705 High Throughput: Read/write data in parallel from multiple nodes \u2705 Cost-Effective: Uses commodity hardware (\\(500-\\)1000 per server)</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#real-example-storing-a-1-gb-file","title":"Real Example: Storing a 1 GB File","text":"<pre><code># Upload file to HDFS\nhadoop fs -put large_file.txt /user/data/\n\n# What happens behind the scenes:\n# 1. File split into 8 blocks (1 GB / 128 MB = 8)\n# 2. Each block replicated 3 times\n# 3. Total storage used: 3 GB (1 GB \u00d7 3 replicas)\n# 4. Blocks distributed across cluster\n\n# View file in HDFS\nhadoop fs -ls /user/data/\n# Output: large_file.txt (1 GB)\n\n# View actual blocks\nhdfs fsck /user/data/large_file.txt -files -blocks -locations\n# Output: Shows 8 blocks, each with 3 replicas on different DataNodes\n</code></pre>"},{"location":"lectures/Lecture01_big_data_ecosystem/#32-mapreduce-processing-framework","title":"3.2 MapReduce: Processing Framework","text":"<p>MapReduce is a programming model for processing large datasets in parallel across a Hadoop cluster, consisting of two main functions: Map and Reduce.</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#the-mapreduce-concept","title":"The MapReduce Concept","text":"<p>Problem: Count word frequency in 1 TB of text files</p> <p>Traditional Approach (Single Computer): <pre><code>word_count = {}\nfor file in all_files:  # Takes days\n    for word in file:\n        word_count[word] = word_count.get(word, 0) + 1\n</code></pre> Time: Days to weeks</p> <p>MapReduce Approach (100 computers): <pre><code>Split work across 100 computers \u2192 Each processes 10 GB\nCombine results \u2192 Final word count\n</code></pre> Time: Hours</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#mapreduce-phases","title":"MapReduce Phases","text":"<p>1. Map Phase (Transformation) - Input: Raw data - Process: Transform each record independently - Output: Key-value pairs</p> <p>2. Shuffle &amp; Sort Phase (Automatic) - Group all values with same key together - Sort by key</p> <p>3. Reduce Phase (Aggregation) - Input: Grouped key-value pairs - Process: Aggregate values for each key - Output: Final result</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#word-count-example-step-by-step","title":"Word Count Example (Step-by-Step)","text":"<p>Input Data (3 files on 3 different DataNodes): <pre><code>File 1: \"Hello World\"\nFile 2: \"Hello Hadoop\"\nFile 3: \"World of Hadoop\"\n</code></pre></p> <p>Step 1: Map Phase (Runs on each DataNode) <pre><code>Mapper 1 (File 1):\n\"Hello World\" \u2192 (Hello, 1), (World, 1)\n\nMapper 2 (File 2):\n\"Hello Hadoop\" \u2192 (Hello, 1), (Hadoop, 1)\n\nMapper 3 (File 3):\n\"World of Hadoop\" \u2192 (World, 1), (of, 1), (Hadoop, 1)\n</code></pre></p> <p>Step 2: Shuffle &amp; Sort (Automatic by Hadoop) <pre><code>Group by Key:\nHello \u2192 [1, 1]\nWorld \u2192 [1, 1]\nHadoop \u2192 [1, 1]\nof \u2192 [1]\n</code></pre></p> <p>Step 3: Reduce Phase (Aggregation) <pre><code>Reducer:\nHello \u2192 sum([1, 1]) \u2192 (Hello, 2)\nWorld \u2192 sum([1, 1]) \u2192 (World, 2)\nHadoop \u2192 sum([1, 1]) \u2192 (Hadoop, 2)\nof \u2192 sum([1]) \u2192 (of, 1)\n</code></pre></p> <p>Final Output: <pre><code>Hello: 2\nWorld: 2\nHadoop: 2\nof: 1\n</code></pre></p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#mapreduce-code-example-python","title":"MapReduce Code Example (Python)","text":"<pre><code># mapper.py\nimport sys\n\nfor line in sys.stdin:  # Read input line by line\n    words = line.strip().split()  # Split into words\n    for word in words:\n        print(f\"{word}\\t1\")  # Emit: key=word, value=1\n\n# reducer.py\nimport sys\n\ncurrent_word = None\ncurrent_count = 0\n\nfor line in sys.stdin:  # Read mapper output\n    word, count = line.strip().split('\\t')\n    count = int(count)\n\n    if current_word == word:\n        current_count += count  # Accumulate count\n    else:\n        if current_word:\n            print(f\"{current_word}\\t{current_count}\")  # Output result\n        current_word = word\n        current_count = count\n\n# Output last word\nif current_word:\n    print(f\"{current_word}\\t{current_count}\")\n</code></pre> <p>Run MapReduce Job: <pre><code># Upload input files to HDFS\nhadoop fs -put input.txt /user/data/input/\n\n# Run MapReduce job\nhadoop jar /path/to/hadoop-streaming.jar \\\n  -input /user/data/input \\\n  -output /user/data/output \\\n  -mapper mapper.py \\\n  -reducer reducer.py\n\n# View results\nhadoop fs -cat /user/data/output/part-00000\n</code></pre></p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#mapreduce-architecture","title":"MapReduce Architecture","text":"<pre><code>               [JobTracker/ResourceManager]\n                     (Master)\n                        |\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        |               |               |\n[TaskTracker 1]  [TaskTracker 2]  [TaskTracker 3]\n Map Task 1       Map Task 2       Map Task 3\n Reduce Task      Reduce Task      Reduce Task\n</code></pre> <p>JobTracker/ResourceManager: Manages jobs and schedules tasks TaskTracker/NodeManager: Executes Map and Reduce tasks on DataNodes</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#33-yarn-resource-manager","title":"3.3 YARN: Resource Manager","text":"<p>YARN (Yet Another Resource Negotiator) is the resource management layer in Hadoop 2.x that manages and allocates cluster resources (CPU, memory, disk) to different applications running on the cluster.</p> <p>YARN Components: - ResourceManager: Global resource scheduler - NodeManager: Per-node resource manager - ApplicationMaster: Per-application coordinator</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#4-evolution-to-apache-spark","title":"4. Evolution to Apache Spark","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#why-spark-was-created","title":"Why Spark Was Created","text":"<p>Hadoop MapReduce Limitations:</p> <p>\u274c Slow for Iterative Algorithms <pre><code>Machine Learning Iteration:\nStep 1: Read data from HDFS \u2192 Process \u2192 Write to HDFS\nStep 2: Read data from HDFS \u2192 Process \u2192 Write to HDFS\nStep 3: Read data from HDFS \u2192 Process \u2192 Write to HDFS\n...\nProblem: Excessive disk I/O between iterations\n</code></pre></p> <p>\u274c Not Suitable for Real-Time Processing - Batch-oriented (processes data in large chunks) - High latency (minutes to hours)</p> <p>\u274c Complex to Code - Need separate Map and Reduce functions - Limited high-level APIs</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#what-is-apache-spark","title":"What is Apache Spark?","text":"<p>Apache Spark is a unified analytics engine for large-scale data processing, providing high-level APIs in Java, Scala, Python and R, with an optimized engine that supports general execution graphs.</p> <p>Key Innovation: In-memory computing - Spark stores data in RAM (memory) instead of constantly reading/writing to disk, making it 10-100x faster than Hadoop MapReduce.</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#spark-core-concepts","title":"Spark Core Concepts","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#1-rdd-resilient-distributed-dataset","title":"1. RDD (Resilient Distributed Dataset)","text":"<p>Definition: Immutable, distributed collection of objects that can be processed in parallel</p> <p>Example: <pre><code># Create RDD from text file\nrdd = sc.textFile(\"hdfs://data/logs.txt\")\n\n# RDD is distributed across cluster\nNode 1: [\"line 1\", \"line 2\", \"line 3\"]\nNode 2: [\"line 4\", \"line 5\", \"line 6\"]\nNode 3: [\"line 7\", \"line 8\", \"line 9\"]\n</code></pre></p> <p>RDD Properties: - Resilient: Automatically recovers from failures - Distributed: Split across multiple nodes - Dataset: Collection of data</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#2-dataframe-spark-sql","title":"2. DataFrame (Spark SQL)","text":"<p>Definition: Distributed collection of data organized into named columns (like database table)</p> <p>Spark SQL allows you to seamlessly mix SQL queries with Spark programs, using the same underlying execution engine.</p> <p>Example: <pre><code># Create DataFrame\ndf = spark.read.json(\"hdfs://data/users.json\")\n\n# Use SQL-like operations\ndf.select(\"name\", \"age\").filter(df.age &gt; 25).show()\n\n# Or use SQL directly\ndf.createOrReplaceTempView(\"users\")\nspark.sql(\"SELECT name, age FROM users WHERE age &gt; 25\").show()\n</code></pre></p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#3-spark-architecture","title":"3. Spark Architecture","text":"<pre><code>              [Driver Program]\n                (Your Code)\n                     |\n              [Spark Context]\n                     |\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        |            |            |\n   [Executor 1]  [Executor 2]  [Executor 3]\n   (Worker)      (Worker)      (Worker)\n    Tasks         Tasks         Tasks\n    Cache         Cache         Cache\n</code></pre> <p>Driver: Runs main() function, creates SparkContext Executors: Run tasks and store data in memory Cluster Manager: Allocates resources (YARN, Mesos, Kubernetes)</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#41-spark-core-components","title":"4.1 Spark Core Components","text":"<pre><code>Spark Ecosystem\n\u2502\n\u251c\u2500\u2500 Spark Core (Foundation)\n\u2502   \u2514\u2500\u2500 RDD API, Task Scheduling, Memory Management\n\u2502\n\u251c\u2500\u2500 Spark SQL (Structured Data)\n\u2502   \u2514\u2500\u2500 DataFrames, SQL Queries\n\u2502\n\u251c\u2500\u2500 Spark Streaming (Real-time)\n\u2502   \u2514\u2500\u2500 Process live data streams\n\u2502\n\u251c\u2500\u2500 MLlib (Machine Learning)\n\u2502   \u2514\u2500\u2500 Classification, Regression, Clustering\n\u2502\n\u2514\u2500\u2500 GraphX (Graph Processing)\n    \u2514\u2500\u2500 Social network analysis, Page Rank\n</code></pre>"},{"location":"lectures/Lecture01_big_data_ecosystem/#42-spark-word-count-example","title":"4.2 Spark Word Count Example","text":"<p>PySpark Code (Much simpler than MapReduce):</p> <pre><code>from pyspark import SparkContext\n\n# Initialize Spark\nsc = SparkContext(\"local\", \"WordCount\")\n\n# Read input file\nlines = sc.textFile(\"hdfs://data/input.txt\")\n\n# Perform word count\nword_counts = (lines\n    .flatMap(lambda line: line.split())  # Split into words\n    .map(lambda word: (word, 1))         # Create (word, 1) pairs\n    .reduceByKey(lambda a, b: a + b))    # Sum counts\n\n# Save results\nword_counts.saveAsTextFile(\"hdfs://data/output\")\n\n# Or collect to driver\nresults = word_counts.collect()\nfor word, count in results:\n    print(f\"{word}: {count}\")\n</code></pre> <p>Same Logic, Different Approach:</p> <p>Hadoop MapReduce: ~50 lines of Java code + setup Spark: 10 lines of Python code</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#43-spark-performance-in-memory-computing","title":"4.3 Spark Performance: In-Memory Computing","text":"<p>Scenario: Run algorithm 10 times on 1 GB dataset</p> <p>Hadoop MapReduce: <pre><code>Iteration 1: Read 1 GB from disk \u2192 Process \u2192 Write 1 GB to disk\nIteration 2: Read 1 GB from disk \u2192 Process \u2192 Write 1 GB to disk\n...\nIteration 10: Read 1 GB from disk \u2192 Process \u2192 Write 1 GB to disk\n\nTotal Disk I/O: 20 GB read + 20 GB write = 40 GB\nTime: ~30 minutes\n</code></pre></p> <p>Apache Spark: <pre><code>Initial Load: Read 1 GB from disk \u2192 Load into RAM\nIteration 1-10: Process data directly in RAM\nFinal Save: Write 1 GB to disk\n\nTotal Disk I/O: 1 GB read + 1 GB write = 2 GB\nTime: ~2 minutes\n</code></pre></p> <p>Result: Spark is 10-100x faster for iterative algorithms</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#5-hadoop-vs-spark-comparison","title":"5. Hadoop vs Spark Comparison","text":"Feature Hadoop MapReduce Apache Spark Speed Slower (disk-based) 10-100x faster (in-memory) Processing Batch only Batch + Streaming + Interactive Ease of Use Complex (lots of boilerplate code) Simple (high-level APIs) Languages Primarily Java Python, Scala, Java, R, SQL Latency High (minutes to hours) Low (seconds to minutes) Machine Learning External tools (Mahout) Built-in (MLlib) Storage HDFS required Works with HDFS, S3, Cassandra, etc. Recovery Task-level recovery RDD lineage (faster recovery) Cost Lower (disk is cheaper) Higher (needs more RAM)"},{"location":"lectures/Lecture01_big_data_ecosystem/#when-to-use-what","title":"When to Use What?","text":"<p>Use Hadoop MapReduce When: - \u2705 Simple batch processing - \u2705 Write-once, read-rarely data - \u2705 Budget constraints (limited RAM) - \u2705 Large archival processing</p> <p>Use Apache Spark When: - \u2705 Iterative algorithms (Machine Learning) - \u2705 Real-time stream processing - \u2705 Interactive data analysis - \u2705 Complex data pipelines - \u2705 Fast processing required</p> <p>Best Practice: Use both together! - Store data in HDFS - Process with Spark - Result: Best of both worlds</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#6-getting-started-hands-on-setup","title":"6. Getting Started - Hands-On Setup","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#option-1-local-setup-learning","title":"Option 1: Local Setup (Learning)","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#install-spark-locally","title":"Install Spark Locally","text":"<p>Prerequisites: - Python 3.7+ - Java 8 or 11</p> <p>Installation Steps:</p> <pre><code># 1. Download Spark\nwget https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n\n# 2. Extract\ntar -xzf spark-3.5.0-bin-hadoop3.tgz\nmv spark-3.5.0-bin-hadoop3 /opt/spark\n\n# 3. Set environment variables\nexport SPARK_HOME=/opt/spark\nexport PATH=$PATH:$SPARK_HOME/bin\nexport PYSPARK_PYTHON=python3\n\n# 4. Install PySpark\npip install pyspark\n\n# 5. Test installation\npyspark --version\n</code></pre>"},{"location":"lectures/Lecture01_big_data_ecosystem/#your-first-pyspark-program","title":"Your First PySpark Program","text":"<pre><code># hello_spark.py\nfrom pyspark.sql import SparkSession\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"HelloSpark\") \\\n    .master(\"local[*]\") \\\n    .getOrCreate()\n\n# Create simple dataset\ndata = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\ncolumns = [\"name\", \"age\"]\n\n# Create DataFrame\ndf = spark.createDataFrame(data, columns)\n\n# Show data\nprint(\"Original Data:\")\ndf.show()\n\n# Filter data\nprint(\"\\nPeople over 28:\")\ndf.filter(df.age &gt; 28).show()\n\n# Group and aggregate\nprint(\"\\nAverage age:\")\ndf.agg({\"age\": \"avg\"}).show()\n\n# Stop Spark\nspark.stop()\n</code></pre> <p>Run: <pre><code>python hello_spark.py\n</code></pre></p> <p>Expected Output: <pre><code>Original Data:\n+-------+---+\n|   name|age|\n+-------+---+\n|  Alice| 25|\n|    Bob| 30|\n|Charlie| 35|\n+-------+---+\n\nPeople over 28:\n+-------+---+\n|   name|age|\n+-------+---+\n|    Bob| 30|\n|Charlie| 35|\n+-------+---+\n\nAverage age:\n+--------+\n|avg(age)|\n+--------+\n|    30.0|\n+--------+\n</code></pre></p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#option-2-cloud-based-no-installation","title":"Option 2: Cloud-Based (No Installation)","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#google-colab-with-pyspark","title":"Google Colab with PySpark","text":"<pre><code># Install PySpark in Colab\n!pip install pyspark\n\n# Import and use\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"ColabSpark\") \\\n    .getOrCreate()\n\n# Your code here\ndf = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\ndf.show()\n</code></pre>"},{"location":"lectures/Lecture01_big_data_ecosystem/#databricks-community-edition-free","title":"Databricks Community Edition (Free)","text":"<ol> <li>Go to https://community.cloud.databricks.com</li> <li>Sign up for free account</li> <li>Create notebook</li> <li>Start coding in Spark!</li> </ol> <p>Advantages: - No setup required - Pre-configured cluster - Notebooks interface - Free tier available</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#7-practical-examples","title":"7. Practical Examples","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#example-1-log-file-analysis","title":"Example 1: Log File Analysis","text":"<p>Scenario: Analyze 1 TB of web server logs to find top 10 most visited pages</p> <p>Input (access.log): <pre><code>192.168.1.1 - - [10/Oct/2024:13:55:36] \"GET /home.html HTTP/1.1\" 200\n192.168.1.2 - - [10/Oct/2024:13:55:37] \"GET /about.html HTTP/1.1\" 200\n192.168.1.1 - - [10/Oct/2024:13:55:38] \"GET /home.html HTTP/1.1\" 200\n...\n</code></pre></p> <p>PySpark Solution: <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"LogAnalysis\").getOrCreate()\n\n# Read log file\nlogs = spark.read.text(\"hdfs://logs/access.log\")\n\n# Extract page URLs using regex\nfrom pyspark.sql.functions import regexp_extract\n\npages = logs.select(\n    regexp_extract('value', r'GET (\\S+) HTTP', 1).alias('page')\n)\n\n# Count page visits\npage_counts = pages.groupBy('page').count()\n\n# Get top 10\ntop_pages = page_counts.orderBy('count', ascending=False).limit(10)\n\n# Show results\ntop_pages.show()\n\n# Save results\ntop_pages.write.csv(\"hdfs://output/top_pages\")\n</code></pre></p> <p>Output: <pre><code>+--------------+-----+\n|          page|count|\n+--------------+-----+\n|   /home.html| 5000|\n|  /about.html| 3500|\n|/products.html| 2800|\n...\n</code></pre></p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#example-2-real-time-twitter-stream-analysis","title":"Example 2: Real-Time Twitter Stream Analysis","text":"<p>Scenario: Count hashtags from live Twitter stream</p> <p>Spark Streaming Code: <pre><code>from pyspark.streaming import StreamingContext\nfrom pyspark import SparkContext\n\nsc = SparkContext(\"local[2]\", \"TwitterHashtags\")\nssc = StreamingContext(sc, 10)  # 10-second batches\n\n# Create DStream from Twitter\ntweets = ssc.socketTextStream(\"localhost\", 9999)\n\n# Extract hashtags\nhashtags = tweets.flatMap(lambda tweet: \n    [word for word in tweet.split() if word.startswith('#')])\n\n# Count hashtags in each batch\nhashtag_counts = hashtags.map(lambda tag: (tag, 1)) \\\n                         .reduceByKey(lambda a, b: a + b)\n\n# Print top 5 hashtags every 10 seconds\nhashtag_counts.transform(lambda rdd: \n    rdd.sortBy(lambda x: x[1], ascending=False)) \\\n    .pprint(5)\n\nssc.start()\nssc.awaitTermination()\n</code></pre></p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#example-3-machine-learning-with-spark-mllib","title":"Example 3: Machine Learning with Spark MLlib","text":"<p>Scenario: Customer segmentation using K-means clustering</p> <pre><code>from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler\n\n# Load customer data\ncustomers = spark.read.csv(\"hdfs://data/customers.csv\", \n                          header=True, inferSchema=True)\n\n# Select features: age, income, spending_score\nassembler = VectorAssembler(\n    inputCols=['age', 'income', 'spending_score'],\n    outputCol='features'\n)\n\n# Transform data\ncustomer_features = assembler.transform(customers)\n\n# Train K-means model (3 clusters)\nkmeans = KMeans(k=3, seed=42)\nmodel = kmeans.fit(customer_features)\n\n# Make predictions\npredictions = model.transform(customer_features)\n\n# Show customer segments\npredictions.select('customer_id', 'age', 'income', \n                  'spending_score', 'prediction').show()\n\n# Cluster centers\nprint(\"Cluster Centers:\")\nfor center in model.clusterCenters():\n    print(center)\n</code></pre> <p>Output: <pre><code>+-----------+---+------+--------------+----------+\n|customer_id|age|income|spending_score|prediction|\n+-----------+---+------+--------------+----------+\n|          1| 25| 50000|            80|         0|\n|          2| 45| 80000|            30|         1|\n|          3| 35| 60000|            70|         0|\n...\n\nCluster Centers:\n[28.5, 52000, 75.0]  # Young, medium income, high spending\n[48.0, 85000, 25.0]  # Older, high income, low spending\n[35.0, 58000, 50.0]  # Middle-aged, medium income, medium spending\n</code></pre></p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#8-learning-resources","title":"8. Learning Resources","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#official-documentation","title":"Official Documentation","text":"Resource URL Best For Apache Hadoop Docs https://hadoop.apache.org/docs/stable/ Complete Hadoop reference Hadoop MapReduce Tutorial https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html MapReduce programming Apache Spark Docs https://spark.apache.org/docs/latest/ Complete Spark reference PySpark API https://spark.apache.org/docs/latest/api/python/ Python Spark programming Spark Quick Start https://spark.apache.org/docs/latest/quick-start.html 15-minute Spark introduction"},{"location":"lectures/Lecture01_big_data_ecosystem/#interactive-tutorials","title":"Interactive Tutorials","text":"<ol> <li>Databricks Academy (Free)</li> <li>Hands-on Spark courses</li> <li>Includes notebooks and datasets</li> <li> <p>Certification available</p> </li> <li> <p>DataCamp: Introduction to PySpark</p> </li> <li>Step-by-step lessons</li> <li>Interactive coding exercises</li> <li> <p>Real-world projects</p> </li> <li> <p>Coursera: Big Data Specialization</p> </li> <li>University-level courses</li> <li>Covers Hadoop and Spark</li> <li>Free to audit</li> </ol>"},{"location":"lectures/Lecture01_big_data_ecosystem/#books-free-online","title":"Books (Free Online)","text":"<ol> <li>\"Learning Spark\" by Databricks</li> <li>Comprehensive Spark guide</li> <li>Updated for Spark 3.x</li> <li> <p>Available: spark.apache.org/docs/latest/</p> </li> <li> <p>\"Hadoop: The Definitive Guide\"</p> </li> <li>In-depth Hadoop coverage</li> <li>Architecture details</li> <li>Available: O'Reilly</li> </ol>"},{"location":"lectures/Lecture01_big_data_ecosystem/#video-resources","title":"Video Resources","text":"<ol> <li>Apache Spark YouTube Channel</li> <li>Official tutorials</li> <li>Conference talks</li> <li> <p>Latest features</p> </li> <li> <p>Spark Summit Sessions</p> </li> <li>Real-world use cases</li> <li>Best practices</li> <li>Advanced topics</li> </ol>"},{"location":"lectures/Lecture01_big_data_ecosystem/#9-complete-learning-path-2-weeks","title":"9. Complete Learning Path (2 Weeks)","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#week-1-hadoop-fundamentals","title":"Week 1: Hadoop Fundamentals","text":"<p>Days 1-2: Big Data Concepts - \u2713 Understand 5 V's of Big Data - \u2713 Learn why traditional databases fail - \u2713 Study distributed computing basics</p> <p>Days 3-4: HDFS - \u2713 Install Hadoop locally or use Docker - \u2713 Practice HDFS commands - \u2713 Upload/download files - \u2713 Understand block replication</p> <p>Days 5-7: MapReduce - \u2713 Write word count program - \u2713 Understand Map and Reduce phases - \u2713 Run MapReduce jobs - \u2713 Analyze job logs</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#week-2-apache-spark","title":"Week 2: Apache Spark","text":"<p>Days 8-9: Spark Basics - \u2713 Install PySpark - \u2713 Learn RDD operations - \u2713 Practice transformations and actions - \u2713 Understand lazy evaluation</p> <p>Days 10-11: Spark DataFrames - \u2713 Create DataFrames - \u2713 Write Spark SQL queries - \u2713 Perform aggregations - \u2713 Join multiple DataFrames</p> <p>Days 12-13: Advanced Spark - \u2713 Spark Streaming basics - \u2713 MLlib for machine learning - \u2713 Performance optimization - \u2713 Real-world project</p> <p>Day 14: Review &amp; Project - \u2713 Build end-to-end data pipeline - \u2713 Review concepts - \u2713 Explore advanced topics</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#10-key-takeaways","title":"10. Key Takeaways","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#core-concepts-summary","title":"Core Concepts Summary","text":"<p>Big Data: - Too large/complex for traditional databases - Requires distributed processing - Characterized by 5 V's: Volume, Velocity, Variety, Veracity, Value</p> <p>Hadoop Ecosystem: - HDFS: Distributed storage (splits files into blocks) - MapReduce: Distributed processing (Map \u2192 Shuffle \u2192 Reduce) - YARN: Resource management</p> <p>Apache Spark: - In-memory processing (10-100x faster than MapReduce) - Unified engine (batch, streaming, ML, SQL) - Simple APIs (Python, Scala, Java, R)</p> <p>When to Use: - Hadoop: Simple batch jobs, archival processing, budget-conscious - Spark: Machine learning, real-time processing, interactive analysis - Best: Use both together (HDFS + Spark)</p>"},{"location":"lectures/Lecture01_big_data_ecosystem/#11-practice-exercises","title":"11. Practice Exercises","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#exercise-1-hdfs-commands","title":"Exercise 1: HDFS Commands","text":"<pre><code># Create directory\nhadoop fs -mkdir /user/yourname/data\n\n# Upload file\nhadoop fs -put local_file.txt /user/yourname/data/\n\n# List files\nhadoop fs -ls /user/yourname/data/\n\n# View file content\nhadoop fs -cat /user/yourname/data/local_file.txt\n\n# Download file\nhadoop fs -get /user/yourname/data/local_file.txt ./\n\n# Delete file\nhadoop fs -rm /user/yourname/data/local_file.txt\n</code></pre>"},{"location":"lectures/Lecture01_big_data_ecosystem/#exercise-2-word-count-in-pyspark","title":"Exercise 2: Word Count in PySpark","text":"<pre><code># Read text file\ntext = spark.read.text(\"input.txt\")\n\n# Count words\nword_count = text.rdd \\\n    .flatMap(lambda line: line.value.split()) \\\n    .map(lambda word: (word.lower(), 1)) \\\n    .reduceByKey(lambda a, b: a + b) \\\n    .sortBy(lambda x: x[1], ascending=False)\n\n# Show top 10\nword_count.take(10)\n</code></pre>"},{"location":"lectures/Lecture01_big_data_ecosystem/#exercise-3-data-analysis","title":"Exercise 3: Data Analysis","text":"<pre><code># Load CSV\ndf = spark.read.csv(\"sales.csv\", header=True, inferSchema=True)\n\n# Total sales by category\ndf.groupBy(\"category\").sum(\"amount\").show()\n\n# Average order value\ndf.agg({\"amount\": \"avg\"}).show()\n\n# Top 5 customers\ndf.groupBy(\"customer_id\").sum(\"amount\") \\\n  .orderBy(\"sum(amount)\", ascending=False) \\\n  .limit(5).show()\n</code></pre>"},{"location":"lectures/Lecture01_big_data_ecosystem/#12-next-steps","title":"12. Next Steps","text":""},{"location":"lectures/Lecture01_big_data_ecosystem/#advanced-topics-to-explore","title":"Advanced Topics to Explore","text":"<ol> <li>Spark Performance Optimization</li> <li>Partitioning strategies</li> <li>Caching and persistence</li> <li>Broadcast variables</li> <li> <p>Avoiding shuffles</p> </li> <li> <p>Spark Streaming</p> </li> <li>Structured Streaming</li> <li>Kafka integration</li> <li> <p>Real-time dashboards</p> </li> <li> <p>Spark ML Pipelines</p> </li> <li>Feature engineering</li> <li>Model training and tuning</li> <li> <p>Model deployment</p> </li> <li> <p>Cluster Management</p> </li> <li>YARN configuration</li> <li>Kubernetes deployment</li> <li> <p>Cloud platforms (EMR, Dataproc)</p> </li> <li> <p>Advanced HDFS</p> </li> <li>Federation</li> <li>High Availability</li> <li>Erasure Coding</li> </ol>"},{"location":"lectures/Lecture01_big_data_ecosystem/#certifications","title":"Certifications","text":"<ul> <li>Databricks Certified Associate Developer</li> <li>Cloudera Certified Data Engineer</li> <li>AWS Big Data Specialty</li> <li>Google Cloud Data Engineer</li> </ul>"},{"location":"lectures/Lecture01_big_data_ecosystem/#conclusion","title":"Conclusion","text":"<p>You now have a comprehensive understanding of the Big Data ecosystem, from fundamental concepts to hands-on implementations. The combination of Hadoop and Spark provides a powerful platform for processing massive datasets efficiently.</p> <p>Remember: - Start small with local installations - Practice with real datasets - Build projects to solidify understanding - Join community forums for support - Stay updated with latest releases</p> <p>Happy Learning! \ud83d\ude80</p>"},{"location":"lectures/Lecture01_data_and_sources/","title":"Introduction to Data Science","text":""},{"location":"lectures/Lecture01_data_and_sources/#understanding-data-roles-and-career-paths","title":"Understanding Data, Roles, and Career Paths","text":"<p>Duration: 30-minute introduction Course: Advanced Data Science Techniques and Applications</p>"},{"location":"lectures/Lecture01_data_and_sources/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is Data?</li> <li>Types of Data</li> <li>How Data is Generated</li> <li>Small Data vs Big Data</li> <li>Data Quality and Preparation</li> <li>Roles in Data</li> <li>What is Data Science?</li> <li>Course Overview</li> <li>Career Paths</li> </ol>"},{"location":"lectures/Lecture01_data_and_sources/#1-what-is-data","title":"1. What is Data?","text":"<p>Data is raw facts, observations, or measurements that represent real-world phenomena.</p>"},{"location":"lectures/Lecture01_data_and_sources/#simple-definition","title":"Simple Definition","text":"<p>Data is any information that can be stored, processed, and analyzed to gain insights or make decisions.</p>"},{"location":"lectures/Lecture01_data_and_sources/#examples-from-daily-life","title":"Examples from Daily Life","text":"<ul> <li>Shopping: Your purchase history (what, when, how much)</li> <li>Social Media: Your posts, likes, comments, followers</li> <li>Health: Heart rate, blood pressure, sleep patterns</li> <li>Transportation: GPS locations, travel times, routes</li> <li>Banking: Transactions, account balances, payment history</li> </ul>"},{"location":"lectures/Lecture01_data_and_sources/#why-data-matters","title":"Why Data Matters","text":"<pre><code>Raw Data \u2192 Information \u2192 Knowledge \u2192 Wisdom \u2192 Decisions\n\nExample:\nRaw Data:      Temperature readings: 36.5\u00b0C, 38.2\u00b0C, 39.1\u00b0C\nInformation:   Temperature is rising\nKnowledge:     Patient likely has fever\nWisdom:        Based on pattern, might be viral infection\nDecision:      Recommend fever medication and rest\n</code></pre> <p>Key Insight: Good data is the foundation of all data science work. Bad data = Bad insights = Bad decisions.</p>"},{"location":"lectures/Lecture01_data_and_sources/#2-types-of-data","title":"2. Types of Data","text":"<p>Data comes in many forms. Understanding data types is crucial for choosing the right analysis techniques and algorithms.</p>"},{"location":"lectures/Lecture01_data_and_sources/#21-by-structure","title":"2.1 By Structure","text":""},{"location":"lectures/Lecture01_data_and_sources/#structured-data-organized-table-format","title":"Structured Data (Organized, Table Format)","text":"<ul> <li>Stored in rows and columns</li> <li>Easy to search and analyze</li> <li>Examples: Databases, spreadsheets, CSV files</li> </ul> <pre><code>| CustomerID | Name    | Age | Purchase |\n|------------|---------|-----|----------|\n| 001        | Alice   | 25  | $150     |\n| 002        | Bob     | 30  | $200     |\n</code></pre>"},{"location":"lectures/Lecture01_data_and_sources/#semi-structured-data-partially-organized","title":"Semi-Structured Data (Partially Organized)","text":"<ul> <li>Has some structure but not rigid</li> <li>Examples: JSON, XML, logs</li> </ul> <pre><code>{\n  \"customer\": \"Alice\",\n  \"age\": 25,\n  \"purchases\": [\n    {\"item\": \"laptop\", \"price\": 800},\n    {\"item\": \"mouse\", \"price\": 20}\n  ]\n}\n</code></pre>"},{"location":"lectures/Lecture01_data_and_sources/#unstructured-data-no-predefined-structure","title":"Unstructured Data (No Predefined Structure)","text":"<ul> <li>Most complex to analyze</li> <li>Requires special processing techniques</li> <li>Examples: Text, images, videos, audio</li> </ul>"},{"location":"lectures/Lecture01_data_and_sources/#22-by-mathematical-representation","title":"2.2 By Mathematical Representation","text":"<p>Understanding how data is represented mathematically is essential for algorithm development.</p>"},{"location":"lectures/Lecture01_data_and_sources/#scalar-single-value","title":"Scalar (Single Value)","text":"<p>A single numerical value.</p> <pre><code># Examples\ntemperature = 36.5      # Body temperature\nprice = 99.99           # Product price\nage = 25                # Person's age\n</code></pre> <p>When Used: Individual measurements, single predictions</p>"},{"location":"lectures/Lecture01_data_and_sources/#vector-1d-array","title":"Vector (1D Array)","text":"<p>A list of numbers representing multiple values of the same type.</p> <pre><code># Examples\ntemperatures = [36.5, 36.8, 37.1, 36.9]  # Daily temperatures\nprices = [10, 25, 50, 100]                # Product prices\nfeatures = [25, 170, 70]                  # Age, height, weight\n\n# Real-world: Customer feature vector\ncustomer = [25, 50000, 3, 1]  \n# [age, income, years_customer, has_premium]\n</code></pre> <p>When Used: Time series, feature vectors for ML, word embeddings</p>"},{"location":"lectures/Lecture01_data_and_sources/#matrix-2d-array","title":"Matrix (2D Array)","text":"<p>A table of numbers arranged in rows and columns.</p> <pre><code># Example: Student grades (rows=students, columns=subjects)\ngrades = [\n    [85, 90, 78],  # Student 1: Math, Science, English\n    [92, 88, 95],  # Student 2\n    [78, 85, 82]   # Student 3\n]\n\n# Example: Image (grayscale 3x3 pixels)\nimage = [\n    [0, 128, 255],\n    [64, 192, 32],\n    [255, 0, 128]\n]\n</code></pre> <p>When Used: Datasets, images (grayscale), spreadsheets, correlation matrices</p>"},{"location":"lectures/Lecture01_data_and_sources/#tensor-multi-dimensional-array","title":"Tensor (Multi-Dimensional Array)","text":"<p>Extension of matrices to 3 or more dimensions.</p> <pre><code># Example: Color image (Height \u00d7 Width \u00d7 Channels)\n# 2x2 RGB image\ncolor_image = [\n    [[255, 0, 0], [0, 255, 0]],      # Row 1: Red, Green\n    [[0, 0, 255], [255, 255, 0]]     # Row 2: Blue, Yellow\n]\n# Shape: (2, 2, 3) \u2192 2 rows, 2 columns, 3 color channels\n\n# Example: Video (Time \u00d7 Height \u00d7 Width \u00d7 Channels)\nvideo = [frame1, frame2, frame3, ...]\n# Shape: (30, 1920, 1080, 3) \u2192 30 frames, 1920x1080 pixels, RGB\n</code></pre> <p>When Used: Deep learning, images, videos, time-series with multiple features</p>"},{"location":"lectures/Lecture01_data_and_sources/#23-by-data-modality","title":"2.3 By Data Modality","text":""},{"location":"lectures/Lecture01_data_and_sources/#tabular-data-most-common-in-data-science","title":"Tabular Data (Most Common in Data Science)","text":"<ul> <li>Structured in rows and columns</li> <li>Each row = observation/record</li> <li>Each column = feature/variable</li> </ul> <pre><code>| Date       | Product | Sales | Region |\n|------------|---------|-------|--------|\n| 2024-01-01 | Laptop  | 50    | North  |\n| 2024-01-01 | Phone   | 120   | South  |\n</code></pre> <p>Tools: Pandas, SQL, Excel Algorithms: Linear Regression, Random Forest, XGBoost</p>"},{"location":"lectures/Lecture01_data_and_sources/#text-data-natural-language","title":"Text Data (Natural Language)","text":"<ul> <li>Unstructured language data</li> <li>Requires preprocessing (tokenization, cleaning)</li> </ul> <pre><code>Examples:\n- Customer reviews: \"Great product! Fast shipping.\"\n- Emails, social media posts, articles\n- Chat conversations, documentation\n</code></pre> <p>Tools: NLTK, spaCy, Transformers Algorithms: NLP models, BERT, GPT, Sentiment Analysis</p>"},{"location":"lectures/Lecture01_data_and_sources/#image-data","title":"Image Data","text":"<ul> <li>2D array of pixels (grayscale) or 3D array (color)</li> <li>Each pixel has intensity values</li> </ul> <pre><code># Grayscale image: Height \u00d7 Width (2D matrix)\ngrayscale = [[0, 128, 255], [64, 192, 32]]  # 2x3 image\n\n# Color image: Height \u00d7 Width \u00d7 Channels (3D tensor)\ncolor = [\n    [[R1, G1, B1], [R2, G2, B2]],  # Row 1\n    [[R3, G3, B3], [R4, G4, B4]]   # Row 2\n]\n</code></pre> <p>Applications: Medical imaging, face recognition, object detection Tools: OpenCV, PIL, TensorFlow Algorithms: CNNs, ResNet, YOLO</p>"},{"location":"lectures/Lecture01_data_and_sources/#video-data","title":"Video Data","text":"<ul> <li>Sequence of images (frames) over time</li> <li>4D tensor: Time \u00d7 Height \u00d7 Width \u00d7 Channels</li> </ul> <pre><code># Video = sequence of frames\nvideo = [frame1, frame2, frame3, ...]\n# Each frame is an image (H \u00d7 W \u00d7 C)\n</code></pre> <p>Applications: Surveillance, action recognition, video analysis Tools: OpenCV, FFmpeg, PyTorch Algorithms: 3D CNNs, RNNs, Video Transformers</p>"},{"location":"lectures/Lecture01_data_and_sources/#audio-data","title":"Audio Data","text":"<ul> <li>Sound represented as waveforms or spectrograms</li> <li>1D signal (time series) or 2D (spectrogram)</li> </ul> <pre><code># Audio waveform: amplitude values over time\naudio_signal = [0.1, 0.5, -0.3, 0.8, ...]  # Vector\n\n# Spectrogram: frequency components over time\nspectrogram = [\n    [freq1_t1, freq2_t1, freq3_t1],  # Time 1\n    [freq1_t2, freq2_t2, freq3_t2]   # Time 2\n]  # Matrix\n</code></pre> <p>Applications: Speech recognition, music classification Tools: Librosa, PyAudio, TensorFlow Audio Algorithms: RNNs, WaveNet, Speech-to-Text</p>"},{"location":"lectures/Lecture01_data_and_sources/#time-series-data","title":"Time Series Data","text":"<ul> <li>Data points indexed by time</li> <li>Sequential observations with temporal dependencies</li> </ul> <pre><code># Stock prices over time\ndates = ['2024-01-01', '2024-01-02', '2024-01-03']\nprices = [150.5, 152.3, 149.8]  # Vector\n\n# Multivariate time series (multiple sensors)\nsensor_data = [\n    [temp1, humidity1, pressure1],  # Time 1\n    [temp2, humidity2, pressure2],  # Time 2\n    [temp3, humidity3, pressure3]   # Time 3\n]  # Matrix\n</code></pre> <p>Applications: Stock prediction, weather forecasting, IoT Tools: Pandas, Prophet, statsmodels Algorithms: ARIMA, LSTM, GRU</p>"},{"location":"lectures/Lecture01_data_and_sources/#graphnetwork-data","title":"Graph/Network Data","text":"<ul> <li>Nodes (entities) and edges (relationships)</li> <li>Represents connections and interactions</li> </ul> <pre><code>Example: Social Network\nNodes: People (Alice, Bob, Charlie)\nEdges: Friendships (Alice \u2192 Bob, Bob \u2192 Charlie)\n\n    Alice \u2500\u2500\u2500\u2500 Bob\n              /\n         Charlie\n</code></pre> <p>Applications: Social networks, recommendation systems Tools: NetworkX, Neo4j, PyTorch Geometric Algorithms: Graph Neural Networks (GNNs), PageRank</p>"},{"location":"lectures/Lecture01_data_and_sources/#summary-data-types-at-a-glance","title":"Summary: Data Types at a Glance","text":"Data Type Mathematical Form Example Common Tools Scalar Single value Temperature: 36.5 NumPy Vector 1D array [25, 170, 70] NumPy, Pandas Matrix 2D array Spreadsheet, grayscale image NumPy, Pandas Tensor 3D+ array Color image, video TensorFlow, PyTorch Tabular Rows \u00d7 Columns CSV, SQL database Pandas, SQL Text String/sequence \"Hello world\" NLTK, spaCy Image H \u00d7 W \u00d7 C Photo, X-ray OpenCV, PIL Video T \u00d7 H \u00d7 W \u00d7 C Movie, surveillance OpenCV, FFmpeg Audio Waveform/spectrogram Music, speech Librosa Time Series Temporal sequence Stock prices Pandas, Prophet Graph Nodes + Edges Social network NetworkX"},{"location":"lectures/Lecture01_data_and_sources/#3-how-data-is-generated","title":"3. How Data is Generated","text":"<p>Understanding data sources helps you identify quality issues and collection strategies.</p>"},{"location":"lectures/Lecture01_data_and_sources/#31-human-generated-data","title":"3.1 Human-Generated Data","text":"<p>Created directly by humans through interactions.</p> <p>Examples: - Social Media: Posts, comments, likes, shares - E-commerce: Product reviews, ratings, wishlists - Surveys: Customer feedback, questionnaires - Manual Entry: Forms, registrations, reports</p> <p>Characteristics: - \u2713 Rich contextual information - \u2713 Subjective opinions and sentiments - \u2717 Prone to errors, bias, and inconsistencies - \u2717 May have missing or incomplete data</p>"},{"location":"lectures/Lecture01_data_and_sources/#32-machine-generated-data","title":"3.2 Machine-Generated Data","text":"<p>Automatically created by systems and devices.</p> <p>Examples: - Sensors: Temperature, pressure, GPS, accelerometer - Logs: Web server logs, application logs, error logs - Transactions: Banking, e-commerce, point-of-sale - IoT Devices: Smart home sensors, wearables, industrial sensors</p> <p>Characteristics: - \u2713 High volume and velocity - \u2713 Consistent format - \u2713 Objective measurements - \u2717 May contain noise or sensor errors - \u2717 Requires context for interpretation</p>"},{"location":"lectures/Lecture01_data_and_sources/#33-process-generated-data","title":"3.3 Process-Generated Data","text":"<p>Created as byproduct of business processes and operations.</p> <p>Examples: - Business Operations: Inventory levels, employee attendance - Healthcare: Patient records, lab results, prescriptions - Education: Student grades, attendance, enrollment - Government: Census data, tax records, permits</p> <p>Characteristics: - \u2713 Well-structured and standardized - \u2713 Historical records for trend analysis - \u2717 May be siloed across departments - \u2717 Privacy and compliance constraints</p>"},{"location":"lectures/Lecture01_data_and_sources/#34-experimental-data","title":"3.4 Experimental Data","text":"<p>Collected through controlled experiments and research.</p> <p>Examples: - Scientific Research: Lab experiments, clinical trials - A/B Testing: Website variations, app features - Market Research: Product testing, focus groups</p> <p>Characteristics: - \u2713 High quality and controlled - \u2713 Clear methodology and documentation - \u2717 Limited sample size - \u2717 Expensive to collect</p>"},{"location":"lectures/Lecture01_data_and_sources/#data-generation-pipeline","title":"Data Generation Pipeline","text":"<pre><code>Data Sources \u2192 Collection \u2192 Storage \u2192 Processing \u2192 Analysis\n     \u2193              \u2193           \u2193          \u2193           \u2193\n  Sensors      APIs/Forms    Database    ETL      Insights\n  Humans       Scraping      Cloud       Clean    Reports\n  Systems      Streaming     Data Lake   Transform Models\n</code></pre>"},{"location":"lectures/Lecture01_data_and_sources/#4-traditional-data-vs-big-data","title":"4.  Traditional Data vs Big Data","text":"<p>Understanding the difference is crucial for choosing appropriate tools and techniques.</p>"},{"location":"lectures/Lecture01_data_and_sources/#41-traditional-data","title":"4.1 Traditional Data","text":"<p>Definition: Data that can be processed on a single machine with standard tools.</p> <p>Characteristics: - Volume: Gigabytes (GB) or less - Processing: Excel, single database, local analysis - Storage: Local disk, single server - Tools: Excel, R, Python (Pandas), SQL databases</p> <p>Example Scenarios: <pre><code>\u2713 Small business sales data (1,000 transactions/day)\n\u2713 Local hospital patient records (10,000 patients)\n\u2713 Survey data (5,000 responses)\n\u2713 Retail store inventory (50,000 products)\n</code></pre></p> <p>Processing Time: Minutes to hours on a laptop</p>"},{"location":"lectures/Lecture01_data_and_sources/#42-big-data","title":"4.2 Big Data","text":"<p>Definition: Data so large and complex that traditional tools cannot handle it efficiently.</p> <p>Characteristics: - Volume: Terabytes (TB) to Petabytes (PB) - Velocity: Real-time or near-real-time processing - Variety: Multiple formats (structured, unstructured) - Processing: Distributed systems (Hadoop, Spark) - Storage: Distributed file systems (HDFS, Cloud)</p> <p>Example Scenarios: <pre><code>\u2713 Netflix streaming data (billions of events/day)\n\u2713 Facebook user interactions (billions of posts/day)\n\u2713 Stock market tick data (millions of trades/second)\n\u2713 IoT sensor networks (millions of devices)\n\u2713 Genomic sequencing data (100+ GB per individual)\n</code></pre></p> <p>Processing Time: Minutes to hours using distributed clusters (100+ machines)</p>"},{"location":"lectures/Lecture01_data_and_sources/#comparison-table","title":"Comparison Table","text":"Aspect Small Data Big Data Volume MB to GB TB to PB Processing Single machine Distributed cluster Speed Minutes to hours Requires parallel processing Storage Local disk Distributed (HDFS, Cloud) Tools Excel, Pandas, MySQL Hadoop, Spark, NoSQL Cost Low (\\(100-\\)1000) High (\\(10,000-\\)1,000,000+) Expertise Basic SQL, Python Distributed systems knowledge Analysis Time Interactive (seconds) Batch processing (hours)"},{"location":"lectures/Lecture01_data_and_sources/#when-do-you-need-big-data-tools","title":"When Do You Need Big Data Tools?","text":"<p>Use Big Data Tools When: - \u2705 Data doesn't fit in memory (&gt;16-32 GB) - \u2705 Processing takes hours on single machine - \u2705 Real-time streaming analysis required - \u2705 Need to scale to multiple machines</p> <p>Use Small Data Tools When: - \u2705 Data fits in Excel or Pandas DataFrame - \u2705 Analysis completes in minutes - \u2705 Budget constraints - \u2705 Simple reporting and dashboards</p> <p>This Course Covers Both: We'll start with small data tools (Pandas, Scikit-Learn) and progress to big data technologies (Hadoop, Spark) in Month 10.</p>"},{"location":"lectures/Lecture01_data_and_sources/#5-understanding-events-and-data-generation","title":"5. Understanding Events and Data Generation","text":""},{"location":"lectures/Lecture01_data_and_sources/#what-is-an-event","title":"What is an Event?","text":"<p>An event represents something meaningful that happened at a specific point in time. Every piece of data in your analytical dataset began as a real-world event.</p> <p>Common Event Types: - User Interactions: Clicks, page views, form submissions, scrolls - Transactions: Purchases, payments, refunds, cart actions - System Events: API calls, errors, performance metrics - IoT/Sensor Data: Temperature readings, GPS locations, device status - Communication: Messages, emails, social posts, support tickets</p>"},{"location":"lectures/Lecture01_data_and_sources/#event-components","title":"Event Components","text":"<p>Every event should capture: 1. What happened - Event type (e.g., \"purchase_completed\") 2. When it happened - Timestamp (precise time) 3. Who/What was involved - User ID, session ID, device info 4. Context - Additional metadata (page URL, product ID, location)</p> <p>Example Event Structure: <pre><code>{\n  \"event_type\": \"product_view\",\n  \"timestamp\": \"2024-11-23T14:30:00Z\",\n  \"user_id\": \"user_12345\",\n  \"session_id\": \"sess_abc123\",\n  \"product_id\": \"prod_789\",\n  \"category\": \"electronics\",\n  \"device\": \"mobile\",\n  \"location\": \"homepage\"\n}\n</code></pre></p>"},{"location":"lectures/Lecture01_data_and_sources/#6the-data-journey-from-events-to-analysis","title":"6.The Data Journey - From Events to Analysis","text":""},{"location":"lectures/Lecture01_data_and_sources/#the-complete-pipeline","title":"The Complete Pipeline","text":"<pre><code>Real-World Event \u2192 Event Capture \u2192 Data Storage \u2192 \nProcessing/Transformation \u2192 Analysis \u2192 Insights\n</code></pre>"},{"location":"lectures/Lecture01_data_and_sources/#stage-1-event-capture","title":"Stage 1: Event Capture","text":"<p>Key Considerations: - Timing: Synchronous (immediate) vs Asynchronous (delayed) - Granularity: What level of detail to capture? - Performance: Impact on user experience - Reliability: Ensuring events aren't lost</p> <p>Common Capture Methods: - JavaScript tracking (web) - Mobile SDKs (iOS/Android) - Server-side logging - Message queues (Kafka, RabbitMQ)</p>"},{"location":"lectures/Lecture01_data_and_sources/#stage-2-data-storage","title":"Stage 2: Data Storage","text":"<p>Storage Options: - Real-time: Streaming platforms (Kafka, Kinesis) - Batch: Data warehouses (Snowflake, BigQuery, Redshift) - Raw Events: Data lakes (S3, HDFS) - Processed: Databases (PostgreSQL, MongoDB)</p>"},{"location":"lectures/Lecture01_data_and_sources/#stage-3-processing","title":"Stage 3: Processing","text":"<p>Typical Transformations: - Data cleaning (removing duplicates, invalid records) - Enrichment (adding derived fields) - Aggregation (summaries, rollups) - Join operations (combining multiple event streams)</p>"},{"location":"lectures/Lecture01_data_and_sources/#7-event-driven-architecture","title":"7: Event-Driven Architecture","text":""},{"location":"lectures/Lecture01_data_and_sources/#event-sourcing-pattern","title":"Event Sourcing Pattern","text":"<p>Concept: Store every event that has ever occurred, rather than just current state.</p> <p>Benefits: - Complete audit trail - Time-travel capabilities (rebuild state at any point) - Rich historical analysis - Easy debugging and replay</p> <p>Challenges: - High storage requirements - Query complexity - Privacy concerns (storing all historical data)</p>"},{"location":"lectures/Lecture01_data_and_sources/#multiple-data-streams-challenge","title":"Multiple Data Streams Challenge","text":"<p>A single real-world action often triggers multiple data streams:</p> <p>Example: Online Purchase <pre><code>Customer clicks \"Buy Now\"\n    \u2193\n\u251c\u2500\u2500 E-commerce platform (order details)\n\u251c\u2500\u2500 Payment processor (transaction)\n\u251c\u2500\u2500 Inventory system (stock update)\n\u251c\u2500\u2500 Shipping provider (tracking)\n\u251c\u2500\u2500 CRM system (customer interaction)\n\u2514\u2500\u2500 Analytics platform (behavior tracking)\n</code></pre></p> <p>Challenge: Each system captures different aspects with: - Different timestamps - Different schemas - Different update frequencies - Potential inconsistencies</p>"},{"location":"lectures/Lecture01_data_and_sources/#8-data-lineage","title":"8: Data Lineage","text":""},{"location":"lectures/Lecture01_data_and_sources/#what-is-data-lineage","title":"What is Data Lineage?","text":"<p>The complete journey data takes from its original source through all transformations until it reaches your analysis.</p> <p>Why It Matters: 1. Quality Assessment - Trace issues back to their source 2. Bias Detection - Understand collection biases 3. Anomaly Investigation - Distinguish real patterns from artifacts 4. Compliance - Document data origins for regulations (GDPR, CCPA)</p>"},{"location":"lectures/Lecture01_data_and_sources/#example-lineage-flow","title":"Example Lineage Flow","text":"<pre><code>Raw Click Event (Web Server) \u2192\n  Cleaned Event (ETL Pipeline) \u2192\n    Sessionized Data (Processing Layer) \u2192\n      User Behavior Table (Data Warehouse) \u2192\n        Your Analysis\n</code></pre> <p>Key Questions to Ask: - Where did this data originate? - What transformations were applied? - When was it collected? - What sampling or filtering occurred? - Are there known quality issues?</p>"},{"location":"lectures/Lecture01_data_and_sources/#9-common-challenges-in-event-tracking","title":"9: Common Challenges in Event Tracking","text":""},{"location":"lectures/Lecture01_data_and_sources/#1-timing-issues","title":"1. Timing Issues","text":"<ul> <li>Clock Synchronization: Different servers may have slightly different times</li> <li>Network Delays: Events may arrive out of order</li> <li>Processing Lag: Time between event occurrence and availability</li> <li>Time Zones: Consistent timestamp handling across regions</li> </ul> <p>Best Practice: Always use UTC timestamps and include both event time and ingestion time.</p>"},{"location":"lectures/Lecture01_data_and_sources/#2-data-volume","title":"2. Data Volume","text":"<ul> <li>Millions of events per second</li> <li>Storage costs escalate quickly</li> <li>Processing overhead</li> <li>Need for sampling strategies</li> </ul> <p>Best Practice: Implement tiered storage (hot/warm/cold) and intelligent sampling.</p>"},{"location":"lectures/Lecture01_data_and_sources/#3-privacy-compliance","title":"3. Privacy &amp; Compliance","text":"<ul> <li>GDPR: Right to be forgotten, consent management</li> <li>CCPA: Data deletion requests</li> <li>PII Protection: Personally Identifiable Information handling</li> <li>Data Anonymization: Removing or masking sensitive data</li> </ul> <p>Best Practice: Design privacy controls from the start, not as an afterthought.</p>"},{"location":"lectures/Lecture01_data_and_sources/#4-incomplete-event-capture","title":"4. Incomplete Event Capture","text":"<p>Common Causes: - Ad blockers - Tracking prevention (Safari, Firefox) - Offline users - Script loading failures - Network issues</p> <p>Best Practice: Implement server-side tracking for critical events; acknowledge limitations in your analysis.</p>"},{"location":"lectures/Lecture01_data_and_sources/#5-schema-evolution","title":"5. Schema Evolution","text":"<ul> <li>Event structures change over time</li> <li>New fields added</li> <li>Old fields deprecated</li> <li>Type changes</li> </ul> <p>Best Practice: Use versioning and maintain backward compatibility.</p>"},{"location":"lectures/Lecture01_data_and_sources/#10-best-practices-for-data-scientists","title":"10: Best Practices for Data Scientists","text":""},{"location":"lectures/Lecture01_data_and_sources/#1-understand-data-provenance","title":"1. Understand Data Provenance","text":"<p>Always ask: - How was this data collected? - What biases might exist in the collection process? - What events are NOT captured? - How representative is this data?</p>"},{"location":"lectures/Lecture01_data_and_sources/#2-validate-data-quality-early","title":"2. Validate Data Quality Early","text":"<p>Check for: - Missing events or data gaps - Duplicate records - Timestamp anomalies - Unexpected value distributions - Schema violations</p>"},{"location":"lectures/Lecture01_data_and_sources/#3-cross-reference-multiple-sources","title":"3. Cross-Reference Multiple Sources","text":"<p>Don't rely on a single data source when possible: - Compare metrics across systems - Validate critical business metrics - Identify discrepancies early</p>"},{"location":"lectures/Lecture01_data_and_sources/#4-document-assumptions","title":"4. Document Assumptions","text":"<p>Maintain clear documentation about: - Known data quality issues - Sampling rates - Filtering applied - Transformations performed - Business logic encoded</p>"},{"location":"lectures/Lecture01_data_and_sources/#5-consider-privacy-from-the-start","title":"5. Consider Privacy from the Start","text":"<p>Key Principles: - Collect only what you need - Anonymize where possible - Implement data retention policies - Handle PII with care - Plan for deletion requests</p>"},{"location":"lectures/Lecture01_data_and_sources/#6-design-for-evolution","title":"6. Design for Evolution","text":"<p>Build flexible pipelines: - Handle schema changes gracefully - Version your event definitions - Plan for new event types - Maintain backward compatibility</p>"},{"location":"lectures/Lecture01_data_and_sources/#7-monitor-data-freshness","title":"7. Monitor Data Freshness","text":"<p>Track: - Time lag between event and availability - Processing delays - Data pipeline health - Missing data periods</p>"},{"location":"lectures/Lecture01_data_and_sources/#11-event-design-guidelines","title":"11: Event Design Guidelines","text":""},{"location":"lectures/Lecture01_data_and_sources/#naming-conventions","title":"Naming Conventions","text":"<p>Use clear, consistent naming: - Good: <code>user_signup_completed</code>, <code>product_added_to_cart</code> - Avoid: <code>event1</code>, <code>click</code>, <code>action</code></p>"},{"location":"lectures/Lecture01_data_and_sources/#required-fields","title":"Required Fields","text":"<p>Every event should include: - <code>event_type</code> or <code>event_name</code> - <code>timestamp</code> (UTC) - <code>user_id</code> or <code>anonymous_id</code> - <code>session_id</code> - Event-specific properties</p>"},{"location":"lectures/Lecture01_data_and_sources/#context-fields","title":"Context Fields","text":"<p>Helpful metadata: - <code>device_type</code> (mobile, desktop, tablet) - <code>platform</code> (iOS, Android, web) - <code>app_version</code> - <code>user_agent</code> - <code>ip_address</code> (for geo-location) - <code>referrer</code> (traffic source)</p>"},{"location":"lectures/Lecture01_data_and_sources/#12-working-with-event-data","title":"12: Working with Event Data","text":""},{"location":"lectures/Lecture01_data_and_sources/#sessionization","title":"Sessionization","text":"<p>Group events into user sessions for analysis.</p> <p>Common Approach: - Session timeout: 30 minutes of inactivity - Track: session start, session end, duration, event count</p>"},{"location":"lectures/Lecture01_data_and_sources/#funnel-analysis","title":"Funnel Analysis","text":"<p>Track users through multi-step processes: <pre><code>Landing Page \u2192 Product View \u2192 Add to Cart \u2192 \nCheckout \u2192 Payment \u2192 Confirmation\n</code></pre></p> <p>Key Metrics: - Conversion rate at each step - Drop-off points - Time between steps</p>"},{"location":"lectures/Lecture01_data_and_sources/#cohort-analysis","title":"Cohort Analysis","text":"<p>Group users by shared characteristics: - Sign-up date - First purchase date - Feature usage</p> <p>Track over time: - Retention rates - Engagement patterns - Lifetime value</p>"},{"location":"lectures/Lecture01_data_and_sources/#attribution","title":"Attribution","text":"<p>Determine which events/touchpoints lead to conversions: - First-touch attribution - Last-touch attribution - Multi-touch attribution - Time-decay models</p>"},{"location":"lectures/Lecture01_data_and_sources/#13-key-metrics-to-monitor","title":"13: Key Metrics to Monitor","text":""},{"location":"lectures/Lecture01_data_and_sources/#what-are-metrics-trees","title":"What Are Metrics Trees?","text":"<p>Imagine you're an architect designing a building. You wouldn't just focus on whether the foundation is strong\u2014you'd also care about whether people want to live in the building, whether it meets safety codes, and whether it generates rental income for the owner. Metrics trees apply this same holistic thinking to machine learning systems.</p> <p>A metrics tree is a hierarchical framework that maps the relationships between different types of measurements in your ML system. It starts with your ultimate business goals at the top and branches down through increasingly specific metrics until you reach the technical details that engineers can directly optimize.</p> <p>The fundamental insight here is crucial: technical excellence in machine learning is only valuable when it translates into business success. A model with 99% accuracy that nobody uses is less valuable than a model with 85% accuracy that drives significant user engagement and revenue.</p>"},{"location":"lectures/Lecture01_data_and_sources/#the-architecture-of-metrics-trees","title":"The Architecture of Metrics Trees","text":"<p>Let's think about metrics trees as having four distinct levels, each serving a specific purpose in your measurement strategy.</p> <p>Level 1: Business Impact Metrics (The North Star) At the apex of your metrics tree sit the measurements that define success for your organization. These metrics directly reflect whether your ML system is achieving its intended business purpose. For a streaming service's recommendation engine, this might be subscriber retention rates and total viewing hours. For a financial institution's fraud detection system, it could be total losses prevented and regulatory compliance scores.</p> <p>These metrics matter because they're what executives and stakeholders care about most. When you're defending your ML project's budget or arguing for additional resources, these are the numbers that will make or break your case.</p> <p>Level 2: Product and User Experience Metrics (The Bridge) The second tier contains metrics that bridge the gap between business outcomes and technical performance. These measurements capture how users interact with your ML system and whether it's creating the intended experience.</p> <p>Think of these as leading indicators of your business metrics. If users are clicking on more recommendations, spending more time with recommended content, or providing positive feedback, these behaviors should eventually translate into improved business outcomes like increased revenue or user retention.</p> <p>For our streaming service example, this level might include click-through rates on recommended content, average session duration after viewing recommendations, and user satisfaction ratings. These metrics help you understand the user experience that ultimately drives business results.</p> <p>Level 3: Model Performance Metrics (The Technical Foundation) Here we find the traditional ML metrics that data scientists know well: precision, recall, F1-scores, AUC-ROC curves, mean squared error, and similar measurements. These metrics directly evaluate how well your model performs its intended technical task.</p> <p>While these metrics are essential for model development and debugging, remember that they're means to an end, not ends in themselves. A model with perfect technical metrics that doesn't improve user experience or business outcomes needs to be reconsidered.</p> <p>Level 4: Operational and System Metrics (The Infrastructure) The foundation of your metrics tree encompasses the operational health of your ML system. This includes model inference latency, system uptime, data quality scores, resource utilization, and model drift detection.</p> <p>These metrics ensure your technically sound model can actually deliver value in production. A model that takes thirty seconds to generate a recommendation is technically useless for real-time applications, regardless of its accuracy.</p>"},{"location":"lectures/Lecture01_data_and_sources/#why-metrics-trees-transform-ml-success","title":"Why Metrics Trees Transform ML Success","text":"<p>The power of metrics trees lies in their ability to create alignment and enable intelligent decision-making across your organization. When your data science team optimizes for a technical metric like recall, everyone can trace exactly how that improvement should flow upward through user engagement to business results.</p> <p>Consider this scenario: your fraud detection model can achieve higher precision but at the cost of increased inference latency. Without a metrics tree, you might make this decision in isolation, focusing only on the technical trade-off. With a clear metrics tree, you can trace the implications: higher latency might hurt user experience during checkout, potentially reducing conversion rates and ultimately affecting revenue.</p> <p>This systematic thinking prevents the common trap of optimizing technical metrics that don't translate into real-world value. It also helps you make informed decisions when trade-offs arise, which they inevitably will in production ML systems.</p>"},{"location":"lectures/Lecture01_data_and_sources/#comprehensive-tracking-in-mlai-projects","title":"Comprehensive Tracking in ML/AI Projects","text":"<p>Understanding what to measure requires thinking about your ML system from multiple perspectives, each contributing essential information about system health and business impact.</p> <p>Model Performance: Beyond Basic Accuracy While accuracy metrics form the technical foundation of your measurements, sophisticated ML systems require more nuanced evaluation. For classification problems, you'll track precision (how many of your positive predictions were correct), recall (how many actual positive cases you identified), and F1-score (the harmonic mean balancing precision and recall).</p> <p>But consider also tracking model confidence and uncertainty. A model that can communicate its uncertainty about predictions might be more valuable than one that appears confident but is frequently wrong. Additionally, monitor performance across different demographic groups or user segments to ensure your model works fairly and effectively for all users.</p> <p>For regression problems, move beyond simple error metrics to understand the distribution of errors. Are most errors small with occasional large outliers, or are errors more uniformly distributed? This understanding helps you set appropriate expectations and design downstream systems.</p> <p>Business Impact: Connecting ML to Value Creation Your business metrics should have clear, measurable relationships with your ML system's performance. For a content recommendation system, track metrics like user engagement time, content consumption rates, subscription renewals, and revenue per user.</p> <p>The critical requirement is establishing causal relationships between ML performance and business outcomes. If your recommendation engine improves its technical metrics, you should observe corresponding improvements in these business metrics within a reasonable timeframe. If you don't see this connection, you need to investigate whether your technical improvements are actually reaching users or whether you're optimizing for the wrong technical metrics.</p> <p>User Experience: The Human Side of ML Systems User-facing ML systems require careful attention to how people actually interact with your model's outputs. This includes explicit feedback like ratings, clicks, and direct user responses, as well as implicit signals like time spent engaging with recommendations, task completion rates, and return usage patterns.</p> <p>Pay particular attention to user trust and perceived value. Even a technically accurate model can fail if users don't trust its recommendations or find them irrelevant. Monitor metrics like user adoption rates, override frequencies (how often users ignore or modify model suggestions), and qualitative feedback about the system's usefulness.</p> <p>Operational Health: Keeping Systems Running Your ML system operates within a complex technical infrastructure, and its performance depends on that infrastructure functioning smoothly. Monitor model inference times to ensure users don't experience frustrating delays. Track data freshness to confirm your model has access to current information. Monitor computational resource usage to prevent performance degradation and control costs.</p> <p>Also implement monitoring for model drift\u2014the gradual degradation of model performance as the real world changes. This is particularly important for models operating in dynamic environments where user behavior, market conditions, or data distributions evolve over time.</p>"},{"location":"lectures/Lecture01_data_and_sources/#real-world-examples-metrics-trees-in-practice","title":"Real-World Examples: Metrics Trees in Practice","text":"<p>Let me walk you through detailed examples that illustrate how metrics trees work across different domains and use cases.</p> <p>Case Study 1: E-commerce Product Recommendation Engine</p> <p>Imagine you're working with a major online retailer implementing a new recommendation system for their product catalog. Their metrics tree demonstrates the full journey from technical performance to business value.</p> <p>At the business impact level, they track monthly revenue per user, customer lifetime value, and market share growth. These metrics directly reflect whether the recommendation system achieves its ultimate goal of driving profitable customer behavior and competitive advantage.</p> <p>The user experience tier monitors click-through rates on recommended products, average items per purchase session, time spent browsing recommended sections, and user satisfaction scores collected through surveys and feedback mechanisms. They also track recommendation diversity to ensure users see a good variety of products rather than repetitive suggestions.</p> <p>Technical model performance includes precision at k (how many of the top k recommendations were actually purchased), recall (what percentage of items the user eventually bought were included in recommendations), and diversity scores measuring how varied the recommendations are across different product categories.</p> <p>Operational metrics encompass recommendation generation latency, catalog freshness (how quickly new products appear in recommendations), system availability during peak shopping periods like Black Friday, and the computational cost per recommendation generated.</p> <p>The interconnections become clear when they need to make decisions. When considering a more sophisticated deep learning model that improves precision by 5% but doubles inference latency, they can trace the potential impact through their entire metrics tree. The improved precision should increase click-through rates and purchases, but the added latency might hurt user experience and reduce overall engagement. The metrics tree provides a framework for making this trade-off analytically rather than intuitively.</p> <p>Case Study 2: Healthcare Diagnostic AI System</p> <p>A hospital implementing an AI system to assist radiologists with medical imaging diagnosis requires a fundamentally different but equally well-structured metrics tree.</p> <p>Business impact metrics focus on patient outcomes and healthcare delivery efficiency: diagnostic accuracy rates, time from imaging to diagnosis, patient satisfaction with the diagnostic process, and cost per diagnosis. They also track integration success with existing clinical workflows and compliance with healthcare regulations.</p> <p>User experience metrics center on clinician adoption and workflow integration. They monitor how often radiologists accept, modify, or reject the AI's suggestions, time spent reviewing AI recommendations, and clinician confidence in the system's outputs measured through regular surveys and feedback sessions.</p> <p>Technical performance metrics include sensitivity (the percentage of actual conditions correctly identified), specificity (the percentage of healthy cases correctly identified), positive predictive value, and negative predictive value. Critically, they monitor performance across different patient demographics, imaging equipment types, and condition severities to ensure equitable and reliable care.</p> <p>Operational metrics cover image processing time, system uptime during critical hours, data quality scores for incoming medical images, and integration stability with the hospital's existing information systems.</p> <p>This metrics tree helps the hospital make evidence-based decisions about system deployment and optimization while maintaining focus on patient outcomes and clinical workflow integration.</p> <p>Case Study 3: Financial Risk Assessment Platform</p> <p>A bank deploying machine learning for loan approval decisions creates a metrics tree that balances business profitability with regulatory compliance and fairness considerations.</p> <p>Business metrics include loan portfolio performance, default rates, profitability per approved loan, and regulatory compliance scores. They also track operational efficiency gains and cost savings compared to manual underwriting processes.</p> <p>User experience focuses on application processing time, customer satisfaction with the approval process, appeal and dispute rates for rejected applications, and loan officer satisfaction when the system is used to augment human decision-making.</p> <p>Technical model performance includes standard classification metrics like AUC and precision/recall, but also fairness metrics ensuring equal treatment across protected demographic groups. They track calibration scores to ensure predicted probabilities match actual outcomes and monitor model stability across different economic conditions.</p> <p>Operational metrics monitor model inference time, data pipeline health, model drift detection (ensuring performance doesn't degrade as economic conditions change), and system security measures protecting sensitive financial data.</p>"},{"location":"lectures/Lecture01_data_and_sources/#14-common-pitfalls-to-avoid","title":"14: Common Pitfalls to Avoid","text":""},{"location":"lectures/Lecture01_data_and_sources/#dont-do-this","title":"\u274c Don't Do This:","text":"<ol> <li>Ignoring data quality - Always validate before analysis</li> <li>Assuming completeness - Account for missing data</li> <li>Treating all events equally - Some events are more reliable</li> <li>Forgetting privacy - Build in privacy controls early</li> <li>Over-collecting data - More isn't always better</li> <li>Ignoring sampling bias - Understand what's NOT captured</li> <li>Trusting timestamps blindly - Verify time accuracy</li> <li>Not documenting assumptions - Future you will thank you</li> </ol>"},{"location":"lectures/Lecture01_data_and_sources/#do-this-instead","title":"\u2705 Do This Instead:","text":"<ol> <li>Validate early and often - Catch issues before they propagate</li> <li>Document everything - Data sources, transformations, assumptions</li> <li>Cross-reference sources - Verify critical metrics</li> <li>Plan for failure - Handle missing data gracefully</li> <li>Design for privacy - Build GDPR/CCPA compliance in</li> <li>Version your schemas - Track changes over time</li> <li>Monitor data freshness - Alert on delays</li> <li>Understand the business context - Know what events mean</li> </ol>"},{"location":"lectures/Lecture01_data_and_sources/#15-data-tracking-checklist","title":"15: Data Tracking Checklist","text":""},{"location":"lectures/Lecture01_data_and_sources/#before-analysis","title":"Before Analysis","text":"<ul> <li>[ ] Understand data collection methodology</li> <li>[ ] Check data completeness and quality</li> <li>[ ] Verify timestamp accuracy</li> <li>[ ] Review known limitations</li> <li>[ ] Identify sampling or filtering</li> <li>[ ] Document data lineage</li> </ul>"},{"location":"lectures/Lecture01_data_and_sources/#during-analysis","title":"During Analysis","text":"<ul> <li>[ ] Cross-validate with other sources</li> <li>[ ] Account for missing data</li> <li>[ ] Check for outliers and anomalies</li> <li>[ ] Consider privacy implications</li> <li>[ ] Document assumptions made</li> <li>[ ] Test for temporal patterns</li> </ul>"},{"location":"lectures/Lecture01_data_and_sources/#after-analysis","title":"After Analysis","text":"<ul> <li>[ ] Validate findings make business sense</li> <li>[ ] Document data quality caveats</li> <li>[ ] Consider what's NOT in the data</li> <li>[ ] Plan for ongoing monitoring</li> <li>[ ] Share lineage documentation</li> <li>[ ] Update data quality metrics</li> </ul>"},{"location":"lectures/Lecture01_data_and_sources/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Every dataset has a story - Understanding how data was collected is as important as analyzing it</p> </li> <li> <p>Data quality matters more than data volume - Focus on reliable, well-understood data</p> </li> <li> <p>Privacy is not optional - Build privacy controls from the start, not as an afterthought</p> </li> <li> <p>Events are never perfectly captured - Account for missing data and biases in your analysis</p> </li> <li> <p>Data lineage is your friend - Always know where your data came from and how it was transformed</p> </li> <li> <p>Design for evolution - Systems and requirements change; build flexibility in</p> </li> <li> <p>Documentation saves time - Document assumptions, limitations, and transformations</p> </li> <li> <p>Validate, then trust - Always cross-reference and validate before making business decisions</p> </li> </ol>"},{"location":"lectures/Lecture01_data_and_sources/#data-storage","title":"Data Storage","text":"<ul> <li>Warehouses: Snowflake, BigQuery, Redshift</li> <li>Lakes: S3, Azure Data Lake, GCS</li> <li>Streaming: Kafka, Kinesis</li> </ul>"},{"location":"lectures/Lecture01_data_and_sources/#data-processing","title":"Data Processing","text":"<ul> <li>Batch: Spark, Hadoop, DBT</li> <li>Streaming: Flink, Spark Streaming, Kafka Streams</li> </ul>"},{"location":"lectures/Lecture01_data_and_sources/#analysis","title":"Analysis","text":"<ul> <li>SQL tools: Your data warehouse</li> <li>Python: Pandas, NumPy, Jupyter</li> <li>Visualization: Tableau, Looker, PowerBI</li> </ul>"},{"location":"lectures/Lecture01_data_and_sources/#further-topics","title":"Further topics","text":"<p>Focus Areas:</p> <ol> <li>Databases - </li> <li>Data Processing and manipulation(with SQL and pyspark)</li> <li>Data Modeling - Structuring data for analysis</li> <li>ETL/ELT Pipelines - How data moves and transforms</li> </ol> <p>Remember: Good data science starts with understanding your data's journey from the real world to your analysis.</p>"},{"location":"lectures/Lecture01_data_and_sources/#5-data-quality-and-preparation","title":"5. Data Quality and Preparation","text":"<p>\"Garbage In, Garbage Out\" - The quality of your insights depends entirely on the quality of your data.</p>"},{"location":"lectures/Lecture01_data_and_sources/#why-data-quality-matters","title":"Why Data Quality Matters","text":"<p>80% of data science work is data preparation, not building fancy algorithms.</p> <p>Real-World Impact: <pre><code>Bad Data \u2192 Wrong Model \u2192 Wrong Predictions \u2192 Bad Decisions \u2192 Financial Loss\n\nExample: Credit Scoring\n- Bad data: Income recorded as \"$50,000\" vs \"50000\" (inconsistent)\n- Wrong model: Treats them as different values\n- Wrong prediction: Denies loan to qualified customer\n- Financial loss: Lost business, customer churn\n</code></pre></p>"},{"location":"lectures/Lecture01_data_and_sources/#common-data-quality-issues","title":"Common Data Quality Issues","text":""},{"location":"lectures/Lecture01_data_and_sources/#1-missing-data","title":"1. Missing Data","text":"<pre><code># Example\ncustomer_data = [\n    {'name': 'Alice', 'age': 25, 'income': 50000},\n    {'name': 'Bob', 'age': None, 'income': 60000},  # Missing age\n    {'name': 'Charlie', 'age': 30, 'income': None}  # Missing income\n]\n</code></pre> <p>Impact: Can't use incomplete records for analysis Solution: Imputation (fill with mean/median), deletion, prediction</p>"},{"location":"lectures/Lecture01_data_and_sources/#2-inconsistent-formatting","title":"2. Inconsistent Formatting","text":"<pre><code># Date formats\ndates = ['2024-01-15', '01/15/2024', '15-Jan-2024']  # 3 formats!\n\n# Names\nnames = ['john doe', 'John Doe', 'JOHN DOE']  # Same person?\n\n# Units\ntemperatures = [98.6, 37, 310]  # Fahrenheit, Celsius, Kelvin\n</code></pre> <p>Impact: Fails to group/match correctly Solution: Standardization, normalization</p>"},{"location":"lectures/Lecture01_data_and_sources/#3-duplicate-records","title":"3. Duplicate Records","text":"<pre><code>records = [\n    {'id': 1, 'name': 'Alice', 'email': 'alice@email.com'},\n    {'id': 2, 'name': 'Alice', 'email': 'alice@email.com'},  # Duplicate!\n]\n</code></pre> <p>Impact: Inflates counts, biases analysis Solution: Deduplication based on unique keys</p>"},{"location":"lectures/Lecture01_data_and_sources/#4-outliers","title":"4. Outliers","text":"<pre><code># Employee salaries\nsalaries = [50000, 55000, 52000, 1000000, 48000]  # CEO salary!\n</code></pre> <p>Impact: Skews statistical measures (mean, std) Solution: Detection (IQR, Z-score), handling (cap, remove, separate)</p>"},{"location":"lectures/Lecture01_data_and_sources/#5-noise-and-errors","title":"5. Noise and Errors","text":"<pre><code># Sensor readings with noise\nsensor_data = [23.5, 23.7, 99.9, 23.6, 23.8]  # 99.9 is error\n</code></pre> <p>Impact: Incorrect patterns, wrong predictions Solution: Smoothing, filtering, validation rules</p>"},{"location":"lectures/Lecture01_data_and_sources/#data-preparation-steps","title":"Data Preparation Steps","text":""},{"location":"lectures/Lecture01_data_and_sources/#1-data-collection-gather-from-sources-2-data-cleaning-fix-errors-missing-values-outliers-3-data-integration-combine-from-multiple-sources-4-data-transformation-normalize-encode-scale-5-data-reduction-select-relevant-features-6-data-validation-verify-quality-and-consistency","title":"<pre><code>1. Data Collection    \u2192 Gather from sources\n2. Data Cleaning      \u2192 Fix errors, missing values, outliers\n3. Data Integration   \u2192 Combine from multiple sources\n4. Data Transformation \u2192 Normalize, encode, scale\n5. Data Reduction     \u2192 Select relevant features\n6. Data Validation    \u2192 Verify quality and consistency\n</code></pre>","text":""},{"location":"lectures/Lecture01_data_and_sources/#6-roles-in-data","title":"6. Roles in Data","text":"<p>The data ecosystem has distinct roles with different responsibilities and skill sets.</p>"},{"location":"lectures/Lecture01_data_and_sources/#61-data-analyst","title":"6.1 Data Analyst","text":"<p>Primary Focus: Analyzing historical data to answer business questions.</p> <p>Key Responsibilities: - Create reports and dashboards - Perform descriptive statistics - Visualize trends and patterns - Answer specific business questions</p> <p>Tools: SQL, Excel, Tableau, Power BI, Python (Pandas)</p> <p>Example Task: \"What were our sales by region last quarter?\"</p> <p>Skills: - \u2b50\u2b50\u2b50 SQL &amp; Database Queries - \u2b50\u2b50\u2b50 Data Visualization - \u2b50\u2b50 Statistical Analysis - \u2b50 Machine Learning</p>"},{"location":"lectures/Lecture01_data_and_sources/#62-data-engineer","title":"6.2 Data Engineer","text":"<p>Primary Focus: Building and maintaining data infrastructure and pipelines.</p> <p>Key Responsibilities: - Design data architecture - Build ETL/ELT pipelines - Manage databases and data warehouses - Ensure data quality and availability - Optimize data processing systems</p> <p>Tools: SQL, Python, Spark, Kafka, Airflow, AWS/Azure, Docker</p> <p>Example Task: \"Build a pipeline to ingest 10 TB of log data daily and make it available for analysis.\"</p> <p>Skills: - \u2b50\u2b50\u2b50 SQL &amp; Database Design - \u2b50\u2b50\u2b50 Distributed Systems (Spark, Hadoop) - \u2b50\u2b50\u2b50 ETL Pipeline Development - \u2b50\u2b50 Cloud Platforms - \u2b50 Machine Learning</p>"},{"location":"lectures/Lecture01_data_and_sources/#63-data-scientist","title":"6.3 Data Scientist","text":"<p>Primary Focus: Using statistical methods and machine learning to extract insights and build predictive models.</p> <p>Key Responsibilities: - Exploratory Data Analysis (EDA) - Build predictive models - Conduct statistical hypothesis testing - Feature engineering - Communicate findings to stakeholders - Deploy models to production</p> <p>Tools: Python (Scikit-Learn, Pandas), R, SQL, Jupyter, TensorFlow/PyTorch</p> <p>Example Task: \"Build a model to predict customer churn with 85% accuracy.\"</p> <p>Skills: - \u2b50\u2b50\u2b50 Statistics &amp; Probability - \u2b50\u2b50\u2b50 Machine Learning - \u2b50\u2b50\u2b50 Programming (Python/R) - \u2b50\u2b50 Data Visualization - \u2b50\u2b50 Business Communication - \u2b50\u2b50 SQL</p>"},{"location":"lectures/Lecture01_data_and_sources/#64-machine-learning-engineer","title":"6.4 Machine Learning Engineer","text":"<p>Primary Focus: Deploying, scaling, and maintaining ML models in production.</p> <p>Key Responsibilities: - Implement ML algorithms from scratch - Optimize model performance and latency - Build ML pipelines (training, deployment) - Scale models for production use - Monitor model performance - Integrate ML into applications (APIs)</p> <p>Tools: Python, TensorFlow, PyTorch, Kubernetes, MLflow, Docker, AWS SageMaker</p> <p>Example Task: \"Deploy a recommendation model that serves 1 million predictions per second with &lt;100ms latency.\"</p> <p>Skills: - \u2b50\u2b50\u2b50 Machine Learning Algorithms - \u2b50\u2b50\u2b50 Software Engineering - \u2b50\u2b50\u2b50 ML Deployment &amp; MLOps - \u2b50\u2b50 Distributed Systems - \u2b50\u2b50 Cloud Platforms - \u2b50 Statistics</p>"},{"location":"lectures/Lecture01_data_and_sources/#65-ml-scientistresearcher","title":"6.5 ML Scientist/Researcher","text":"<p>Primary Focus: Developing new ML algorithms and advancing the state-of-the-art.</p> <p>Key Responsibilities: - Research new ML techniques - Publish papers at conferences - Experiment with novel architectures - Prove theoretical properties of algorithms - Implement research prototypes</p> <p>Tools: Python, PyTorch, TensorFlow, CUDA, Research papers</p> <p>Example Task: \"Develop a new attention mechanism that improves transformer efficiency by 30%.\"</p> <p>Skills: - \u2b50\u2b50\u2b50 Deep ML/DL Theory - \u2b50\u2b50\u2b50 Mathematics (Linear Algebra, Calculus, Optimization) - \u2b50\u2b50\u2b50 Research &amp; Experimentation - \u2b50\u2b50 Programming - \u2b50 Production Deployment</p>"},{"location":"lectures/Lecture01_data_and_sources/#role-comparison-table","title":"Role Comparison Table","text":"Aspect Data Analyst Data Engineer Data Scientist ML Engineer ML Scientist Focus Reporting Infrastructure Modeling Production Research Math \u2b50 \u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 Coding \u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 ML \u2b50 \u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Stats \u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 DevOps \u2b50 \u2b50\u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50 \u2b50 Business \u2b50\u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50 \u2b50 \u2b50"},{"location":"lectures/Lecture01_data_and_sources/#data-scientist-vs-ml-engineer-key-differences","title":"Data Scientist vs ML Engineer: Key Differences","text":"Aspect Data Scientist ML Engineer Goal Build accurate models Deploy scalable models Environment Jupyter Notebooks Production systems Focus Experimentation Optimization &amp; Reliability Concerns Model accuracy Latency, throughput, uptime Deliverable Model prototype, insights Production API, deployed service Metrics Accuracy, F1-score, AUC Latency, QPS, uptime % Mindset \"Does it work well?\" \"Will it scale and stay up?\" <p>Example Workflow: <pre><code>Data Scientist: \n  Builds model in notebook \u2192 Achieves 92% accuracy \u2192 Saves model.pkl\n\nML Engineer:\n  Takes model.pkl \u2192 Optimizes code \u2192 Builds API \u2192 Deploys to K8s cluster\n  \u2192 Monitors performance \u2192 A/B tests \u2192 Scales to 1M requests/day\n</code></pre></p> <p>This Course: Prepares you primarily for Data Scientist role, with exposure to ML Engineering concepts in deployment modules.</p>"},{"location":"lectures/Lecture01_data_and_sources/#7-what-is-data-science","title":"7. What is Data Science?","text":"<p>Data Science is an interdisciplinary field that uses scientific methods, algorithms, and systems to extract knowledge and insights from structured and unstructured data.</p>"},{"location":"lectures/Lecture01_data_and_sources/#the-data-science-workflow","title":"The Data Science Workflow","text":"<pre><code>1. Business Problem    \u2192 What question are we answering?\n   \u2193\n2. Data Collection     \u2192 Where do we get data?\n   \u2193\n3. Data Cleaning       \u2192 Is the data ready to use?\n   \u2193\n4. Exploratory Analysis \u2192 What patterns exist?\n   \u2193\n5. Feature Engineering \u2192 What variables matter?\n   \u2193\n6. Modeling           \u2192 What algorithm to use?\n   \u2193\n7. Evaluation         \u2192 Is the model accurate?\n   \u2193\n8. Deployment         \u2192 How do we use it?\n   \u2193\n9. Monitoring         \u2192 Is it still working?\n</code></pre>"},{"location":"lectures/Lecture01_data_and_sources/#data-science-skillset-venn-diagram","title":"Data Science Skillset (Venn Diagram)","text":"<pre><code>        Mathematics/Statistics\n               /\\\n              /  \\\n             /    \\\n            /      \\\n           /   DS   \\\n          /__________\\\n  Programming    Domain Expertise\n</code></pre> <p>Data Scientist = Math + Coding + Business Understanding</p>"},{"location":"lectures/course_outline/","title":"Data Science Applications Course","text":"<p>Duration: 4 months</p>"},{"location":"lectures/course_outline/#module-1-data-science-fundamentals-and-exploratory-data-analysis-eda","title":"Module 1: Data Science Fundamentals and Exploratory Data Analysis (EDA)","text":""},{"location":"lectures/course_outline/#11-introduction-to-data-science-tools-and-environments","title":"1.1 Introduction to Data Science Tools and Environments","text":"<p>Key Topics: - Overview of Data Science Roles and Tools</p>"},{"location":"lectures/course_outline/#12-big-data-technologies-and-data-engineering","title":"1.2 Big Data Technologies and Data Engineering","text":"<p>Key Topics: - Introduction to Big Data Ecosystem (Hadoop, Spark) - Intro to Databases - Introduction to SQL for Data Retrieval and Manipulation - Introduction to NoSQL Databases (MongoDB) - Data Processing with Apache Spark (PySpark) - Building ETL Pipelines for Big Data - Practical Sessions: Working with Big Data Tools and Techniques</p>"},{"location":"lectures/course_outline/#13-full-etl-and-eda-on-real-world-datasets","title":"1.3 Full ETL and EDA on Real-World Datasets","text":""},{"location":"lectures/course_outline/#module-2-advanced-statistical-methods-and-machine-learning-for-data-science","title":"Module 2: Advanced Statistical Methods and Machine Learning for Data Science","text":""},{"location":"lectures/course_outline/#21-advanced-statistical-methods","title":"2.1 Advanced Statistical Methods","text":"<p>Key Topics: - Hypothesis Testing (T-tests, Chi-Square Tests) - Analysis of Variance (ANOVA) - Regression Analysis (Linear, Logistic, and Polynomial Regression) - Time Series Analysis and Forecasting - Practice: Applying Advanced Statistical Methods</p>"},{"location":"lectures/course_outline/#22-machine-learning-algorithms-for-data-science","title":"2.2 Machine Learning Algorithms for Data Science","text":"<p>Key Topics: - Supervised Learning Algorithms (Decision Trees, Random Forests, Gradient Boosting Machines) - Unsupervised Learning Algorithms (K-Means, Hierarchical Clustering, DBSCAN) - Introduction to Ensemble Methods - Model Evaluation and Validation Techniques (Cross-Validation, ROC-AUC, Precision-Recall) - Practice: Implementing ML Algorithms on Real Datasets</p>"},{"location":"lectures/course_outline/#module-3-deep-learning-for-data-science","title":"Module 3: Deep Learning for Data Science","text":""},{"location":"lectures/course_outline/#31-introduction-to-deep-learning-and-specialized-applications","title":"3.1 Introduction to Deep Learning and Specialized Applications","text":"<p>Key Topics: - Fundamentals of Neural Networks and Deep Learning - Building and Training Deep Learning Models (Using TensorFlow/Keras, PyTorch) - Convolutional Neural Networks (CNNs) for Image Data - Recurrent Neural Networks (RNNs) for Time-Series Data - Natural Language Processing (NLP) for Text Data - Advanced Time Series Analysis and Forecasting Techniques - Practical Sessions: Developing and Tuning Deep Learning Models</p>"},{"location":"lectures/course_outline/#module-4-capstone-project","title":"Module 4: Capstone Project","text":""},{"location":"lectures/course_outline/#41-capstone-project-preparation-execution-and-implementation","title":"4.1 Capstone Project Preparation, Execution, and Implementation","text":"<p>Key Topics: 1. Project Planning and Design 2. Data Collection and Preprocessing Strategies 3. Model Selection and Evaluation Plan 4. Proposal Presentation and Feedback 5. Building and Refining the Data Science Solution 6. Model Training, Tuning, and Evaluation 7. Deployment of the Model (on Cloud or On-Premise) 8. Preparing Final Presentation and Documentation</p>"},{"location":"lectures/course_outline/#42-capstone-project-completion-presentation-and-review","title":"4.2 Capstone Project Completion, Presentation, and Review","text":"<p>Key Topics: 1. Project Presentation to Peers and Instructors 2. Feedback and Q&amp;A Sessions 3. Final Grading and Evaluation 4. Course Wrap-Up and Future Directions in Data Science</p>"},{"location":"lectures/course_outline/#course-overview-summary","title":"Course Overview Summary","text":"Module Focus Area Duration Module 1 Data Science Fundamentals &amp; EDA ~4-6 weeks Module 2 Advanced Statistics &amp; Machine Learning ~6-8 weeks Module 3 Deep Learning ~4-6 weeks Module 4 Capstone Project 4 weeks Total Complete Course 4 months"},{"location":"lectures/course_outline/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this course, students will be able to:</p> <p>\u2705 Work with big data technologies (Hadoop, Spark, PySpark) \u2705 Design and implement ETL pipelines \u2705 Perform comprehensive exploratory data analysis \u2705 Apply advanced statistical methods and hypothesis testing \u2705 Build and evaluate machine learning models \u2705 Develop deep learning solutions for various data types \u2705 Deploy data science solutions to production \u2705 Complete an end-to-end data science project from conception to deployment</p>"},{"location":"lectures/course_outline/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic Python programming knowledge</li> <li>Fundamental statistics and mathematics</li> <li>Understanding of basic data structures and algorithms</li> <li>Familiarity with Jupyter notebooks (recommended)</li> </ul>"},{"location":"lectures/course_outline/#tools-and-technologies-covered","title":"Tools and Technologies Covered","text":""},{"location":"lectures/course_outline/#programming-languages","title":"Programming Languages","text":"<ul> <li>Python (primary)</li> <li>SQL</li> </ul>"},{"location":"lectures/course_outline/#big-data-technologies","title":"Big Data Technologies","text":"<ul> <li>Apache Hadoop</li> <li>Apache Spark</li> <li>PySpark</li> </ul>"},{"location":"lectures/course_outline/#databases","title":"Databases","text":"<ul> <li>SQL databases</li> <li>MongoDB (NoSQL)</li> </ul>"},{"location":"lectures/course_outline/#machine-learning-frameworks","title":"Machine Learning Frameworks","text":"<ul> <li>Scikit-learn</li> <li>TensorFlow</li> <li>Keras</li> <li>PyTorch</li> </ul>"},{"location":"lectures/course_outline/#development-tools","title":"Development Tools","text":"<ul> <li>Jupyter Notebooks</li> <li>Git/GitHub</li> <li>Cloud platforms (AWS/Azure/GCP)</li> </ul>"},{"location":"lectures/course_outline/#assessment-methods","title":"Assessment Methods","text":"<ol> <li>Hands-on Assignments - Practical exercises throughout each module</li> <li>Project Work - Real-world dataset analysis and model building</li> <li>Capstone Project - Comprehensive end-to-end data science project</li> <li>Presentations - Project proposals and final presentations</li> <li>Peer Review - Collaborative learning and feedback sessions</li> </ol>"}]}