{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Data Science Course","text":"<p>Welcome to the Data Science Course \ud83c\udf31</p> <p>This site contains:</p> <ul> <li>\ud83d\udcda Lecture notes  </li> <li>\ud83d\udcbb Code &amp; notebooks  </li> <li>\ud83d\udcdd Homework &amp; exercises  </li> <li>\ud83d\udcca Slides &amp; visualizations  </li> </ul>"},{"location":"#structure","title":"Structure","text":"<ul> <li>Lectures \u2013 theory and examples  </li> <li>Notebooks \u2013 hands-on code in Jupyter  </li> <li>Homework \u2013 assignments and solutions  </li> <li>Slides \u2013 PDF / HTML slides for each week</li> </ul>"},{"location":"lectures/","title":"Course structure","text":"<p>Complete notes for the PA Academy Data Science course.</p>"},{"location":"lectures/#course-outline","title":"Course Outline","text":"<ul> <li>Course Outline</li> </ul>"},{"location":"lectures/#lecture-notes","title":"Lecture Notes","text":"<ul> <li>Lecture 01: Introduction to Data Science Notes</li> </ul>"},{"location":"lectures/#slides","title":"Slides","text":"<ul> <li>Lecture 01: Introduction to Data Science</li> </ul> <p>More lectures will be added throughout the course period.</p>"},{"location":"lectures/Lecture01_Intro/","title":"Introduction to Data Science","text":""},{"location":"lectures/Lecture01_Intro/#a-comprehensive-guide-for-aspiring-data-professionals","title":"A Comprehensive Guide for Aspiring Data Professionals","text":""},{"location":"lectures/Lecture01_Intro/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is Data Science?</li> <li>Data Roles in the Industry</li> <li>Skill Sets by Role</li> <li>The Data Workflow</li> <li>The Data Scientist Role</li> <li>Real-Life Data Science Projects</li> <li>Course Outline</li> <li>Data Types and Classification</li> <li>Data Collection Methods</li> <li>Events and Data Generation</li> <li>Data Lineage and Quality</li> <li>Event-Driven Architecture</li> <li>Traditional Data vs Big Data</li> <li>The 3 V's of Big Data</li> <li>Big Data Architecture</li> <li>MapReduce</li> <li>Apache Spark</li> <li>Database Fundamentals</li> <li>OLTP vs OLAP</li> <li>SQL vs NoSQL</li> <li>Relational Database Concepts</li> <li>Essential SQL for Data Scientists</li> <li>NoSQL Database Types</li> <li>Key Takeaways</li> </ol>"},{"location":"lectures/Lecture01_Intro/#1-what-is-data-science","title":"1. What is Data Science?","text":""},{"location":"lectures/Lecture01_Intro/#definition","title":"Definition","text":"<p>Data Science is an interdisciplinary field that combines three core disciplines to extract meaningful insights from data:</p> <ol> <li>Statistical Modeling \u2014 The mathematical and statistical techniques used to analyze data, find patterns, and make predictions</li> <li>Computing Skills \u2014 The programming and technical abilities needed to process, store, and manipulate data</li> <li>Domain Knowledge \u2014 Understanding of the specific industry or business context where data is being applied</li> </ol>"},{"location":"lectures/Lecture01_Intro/#the-data-science-venn-diagram","title":"The Data Science Venn Diagram","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Statistical   \u2502\n                    \u2502    Modeling     \u2502\n                    \u2502                 \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502              \u2502              \u2502\n              \u2502      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n              \u2502      \u2502  DATA SCIENCE \u2502      \u2502\n              \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n              \u2502              \u2502              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510          \u2502          \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Computing  \u2502          \u2502          \u2502   Domain    \u2502\n    \u2502   Skills    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  Knowledge  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#why-the-intersection-matters","title":"Why the Intersection Matters","text":"<p>The magic of data science happens at the intersection of these three areas:</p> <ul> <li>Modeling + Computing (without Domain) = You can build sophisticated models but may solve the wrong problems</li> <li>Modeling + Domain (without Computing) = You understand what to solve but can't implement solutions at scale</li> <li>Computing + Domain (without Modeling) = You can build systems but lack the analytical rigor for insights</li> </ul> <p>A true data scientist needs competency in all three areas, though the emphasis may vary based on the specific role and organization.</p>"},{"location":"lectures/Lecture01_Intro/#the-evolution-of-data-science","title":"The Evolution of Data Science","text":"<p>Data Science as a formal discipline emerged in the 2000s-2010s, driven by:</p> <ol> <li>Explosion of data \u2014 Digital transformation created unprecedented amounts of data</li> <li>Computing power \u2014 Cloud computing and GPUs made large-scale analysis feasible</li> <li>Open-source tools \u2014 Python, R, and associated libraries democratized advanced analytics</li> <li>Business demand \u2014 Organizations recognized the competitive advantage of data-driven decisions</li> </ol>"},{"location":"lectures/Lecture01_Intro/#2-data-roles-in-the-industry","title":"2. Data Roles in the Industry","text":"<p>The data ecosystem has evolved to include multiple specialized roles. Understanding these roles helps you identify where you want to focus your career.</p>"},{"location":"lectures/Lecture01_Intro/#primary-data-roles","title":"Primary Data Roles","text":""},{"location":"lectures/Lecture01_Intro/#data-engineer","title":"Data Engineer","text":"<p>Focus: Computing + Domain</p> <p>Data Engineers are the architects and builders of data infrastructure. They ensure that data flows smoothly from source systems to destinations where it can be analyzed.</p> <p>Key Responsibilities: - Design, build, and maintain data pipelines (ETL/ELT processes) - Create and manage data warehouses and data lakes - Ensure data quality, reliability, and availability - Optimize database and query performance - Manage big data infrastructure (Hadoop, Spark clusters) - Implement data security and governance measures</p> <p>Day-to-Day Work: - Writing code to extract data from various sources - Transforming and cleaning data for downstream use - Monitoring pipeline health and fixing failures - Collaborating with data scientists on data requirements - Optimizing storage costs and query performance</p> <p>Why This Role Exists: Data scientists and analysts cannot do their work without clean, reliable, accessible data. Data engineers make this possible.</p>"},{"location":"lectures/Lecture01_Intro/#data-scientist","title":"Data Scientist","text":"<p>Focus: Modeling + Computing + Domain (All Three)</p> <p>Data Scientists are often called the \"Swiss Army Knife\" of data because they work across the entire data lifecycle. They combine statistical expertise with programming skills and business understanding.</p> <p>Key Responsibilities: - Define business problems in measurable, analytical terms - Collect, clean, and transform data for analysis - Perform exploratory data analysis (EDA) - Build and evaluate predictive models and ML solutions - Extract actionable insights from complex data - Communicate findings to technical and non-technical stakeholders - Deploy models to production (often with ML Engineers)</p> <p>Day-to-Day Work: - Writing Python/R code for data analysis and modeling - Building and testing machine learning models - Creating visualizations to explain findings - Meeting with stakeholders to understand business needs - Presenting results to leadership teams - Iterating on models based on feedback</p> <p>Why This Role Exists: Organizations need people who can turn raw data into business value through analysis, prediction, and insight.</p>"},{"location":"lectures/Lecture01_Intro/#data-analyst","title":"Data Analyst","text":"<p>Focus: Domain + Basic Modeling</p> <p>Data Analysts focus on understanding what has happened and why, using data to answer business questions and support decision-making.</p> <p>Key Responsibilities: - Create reports and dashboards for business stakeholders - Perform exploratory data analysis - Answer ad-hoc business questions with data - Track key performance indicators (KPIs) and metrics - Identify trends and patterns in historical data - Support strategic decision-making with data-driven insights</p> <p>Day-to-Day Work: - Writing SQL queries to extract data - Building dashboards in Tableau, Power BI, or Looker - Creating Excel reports and presentations - Meeting with business teams to understand their questions - Analyzing A/B test results - Monitoring business metrics and flagging anomalies</p> <p>Why This Role Exists: Business leaders need clear, accurate information about what's happening in their organization to make good decisions.</p>"},{"location":"lectures/Lecture01_Intro/#machine-learning-engineer","title":"Machine Learning Engineer","text":"<p>Focus: Computing + Modeling</p> <p>ML Engineers bridge the gap between data science prototypes and production systems. They focus on making models work reliably at scale.</p> <p>Key Responsibilities: - Deploy machine learning models to production environments - Build scalable ML systems and APIs - Optimize model performance for latency and throughput - Create ML pipelines (MLOps) for training and inference - Monitor model performance and detect drift - Implement automated retraining workflows</p> <p>Day-to-Day Work: - Writing production-quality code (Python, Scala, Java) - Containerizing models with Docker - Setting up model serving infrastructure - Creating CI/CD pipelines for ML - Monitoring model performance in production - Debugging issues in deployed models</p> <p>Why This Role Exists: A model in a Jupyter notebook doesn't create business value. ML Engineers make models work in the real world.</p>"},{"location":"lectures/Lecture01_Intro/#additional-data-roles","title":"Additional Data Roles","text":""},{"location":"lectures/Lecture01_Intro/#data-architect","title":"Data Architect","text":"<p>Focus: Computing + Domain Strategy</p> <p>Data Architects take a high-level view of an organization's entire data ecosystem, designing systems that meet both current and future needs.</p> <p>Key Responsibilities: - Design overall data architecture and infrastructure - Define data standards, policies, and best practices - Plan and select technology stack - Ensure scalability, security, and compliance - Create data models and schemas - Guide technical decisions across data teams</p>"},{"location":"lectures/Lecture01_Intro/#business-intelligence-bi-analyst","title":"Business Intelligence (BI) Analyst","text":"<p>Focus: Domain + Visualization</p> <p>BI Analysts specialize in creating visual reports and dashboards that help executives and managers understand business performance.</p> <p>Key Responsibilities: - Build and maintain BI dashboards (Tableau, Power BI, Looker) - Create executive-level reports and presentations - Define and track business metrics and KPIs - Support strategic planning with data insights - Train business users on self-service analytics tools</p>"},{"location":"lectures/Lecture01_Intro/#research-scientist","title":"Research Scientist","text":"<p>Focus: Deep Modeling + Theory</p> <p>Research Scientists push the boundaries of what's possible with data and algorithms. They typically work at technology companies or research institutions.</p> <p>Key Responsibilities: - Develop new algorithms and methodologies - Publish research papers at academic conferences - Push state-of-the-art performance on benchmarks - Prototype novel approaches before productionization - Collaborate with academic institutions</p>"},{"location":"lectures/Lecture01_Intro/#data-governance-specialist","title":"Data Governance Specialist","text":"<p>Focus: Compliance + Quality</p> <p>Data Governance Specialists ensure that organizations handle data responsibly, legally, and with high quality.</p> <p>Key Responsibilities: - Ensure regulatory compliance (GDPR, CCPA, HIPAA) - Manage data lineage and documentation - Define and enforce access controls - Monitor and improve data quality - Create data catalogs and metadata management - Train employees on data policies</p>"},{"location":"lectures/Lecture01_Intro/#role-comparison-summary","title":"Role Comparison Summary","text":"Role Primary Focus Key Output Tools Data Engineer Building infrastructure Data pipelines, warehouses Spark, Airflow, SQL, Python Data Scientist Analysis &amp; prediction Models, insights, reports Python, R, SQL, Jupyter Data Analyst Business understanding Dashboards, reports SQL, Excel, Tableau, Power BI ML Engineer Production systems Deployed models, APIs Python, Docker, Kubernetes, MLflow Data Architect System design Architecture plans, standards Cloud platforms, modeling tools BI Analyst Visualization Dashboards, KPI tracking Tableau, Power BI, Looker"},{"location":"lectures/Lecture01_Intro/#3-skill-sets-by-role","title":"3. Skill Sets by Role","text":""},{"location":"lectures/Lecture01_Intro/#detailed-skill-matrix","title":"Detailed Skill Matrix","text":"Skill Category Data Engineer Data Scientist Data Analyst ML Engineer Programming Python, Scala, SQL, Java Python, R, SQL SQL, Python basics Python, C++, SQL Data Tools Spark, Airflow, Kafka, dbt Pandas, Jupyter, Git Excel, Tableau, Power BI Docker, K8s, MLflow Cloud AWS/GCP/Azure (deep) AWS/GCP (working) Basic cloud Cloud ML services Modeling Basic ML understanding ML, Stats, Deep Learning Descriptive stats ML algorithms, optimization Databases Design, optimization, all types Query, basic design Query Query, feature stores Soft Skills System design, debugging Communication, business acumen Storytelling, domain expertise DevOps, system design"},{"location":"lectures/Lecture01_Intro/#skill-levels-explained","title":"Skill Levels Explained","text":"<p>For Data Scientists specifically, skills can be categorized by importance:</p> <p>CRITICAL (Cannot work without these): - SQL queries: SELECT, JOIN, WHERE, GROUP BY, ORDER BY - Python/R programming for data manipulation - Basic statistics: mean, median, distributions, hypothesis testing - Data visualization: matplotlib, seaborn, or ggplot2 - Understanding of machine learning fundamentals</p> <p>IMPORTANT (Makes you effective): - Advanced ML algorithms and when to use them - Feature engineering techniques - Model evaluation and validation methods - Version control with Git - Cloud platform basics (AWS, GCP, or Azure) - Communication and presentation skills</p> <p>HELPFUL (Makes you valuable): - Deep learning frameworks (TensorFlow, PyTorch) - Big data tools (Spark, Hadoop basics) - Database design principles - A/B testing and experimentation - Domain expertise in specific industries - Leadership and project management</p>"},{"location":"lectures/Lecture01_Intro/#4-the-data-workflow","title":"4. The Data Workflow","text":"<p>Understanding the end-to-end data workflow helps you see how different roles collaborate and where your work fits in.</p>"},{"location":"lectures/Lecture01_Intro/#the-complete-data-pipeline","title":"The Complete Data Pipeline","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Data     \u2502    \u2502     Data     \u2502    \u2502   Storage &amp;  \u2502    \u2502  Analysis &amp;  \u2502\n\u2502  Collection  \u2502 \u2192  \u2502  Engineering \u2502 \u2192  \u2502  Processing  \u2502 \u2192  \u2502  Exploration \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                    \u2502\n                                                                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Insights &amp;  \u2502    \u2502              \u2502    \u2502  Modeling &amp;  \u2502    \u2502              \u2502\n\u2502  Decisions   \u2502 \u2190  \u2502  Deployment  \u2502 \u2190  \u2502     ML       \u2502 \u2190  \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#stage-by-stage-breakdown","title":"Stage-by-Stage Breakdown","text":""},{"location":"lectures/Lecture01_Intro/#stage-1-data-collection","title":"Stage 1: Data Collection","text":"<p>What happens: Raw data is captured from various sources\u2014user actions, sensors, transactions, external APIs, etc.</p> <p>Who's involved: Data Engineers, sometimes specialized data collection teams</p> <p>Activities: - Setting up tracking systems (web analytics, event logging) - Connecting to external data sources (APIs, vendor feeds) - Configuring database connections - Implementing data capture mechanisms</p> <p>Outputs: Raw data flowing into initial storage systems</p>"},{"location":"lectures/Lecture01_Intro/#stage-2-data-engineering","title":"Stage 2: Data Engineering","text":"<p>What happens: Raw data is cleaned, transformed, and organized for downstream use.</p> <p>Who's involved: Data Engineers</p> <p>Activities: - Building ETL (Extract, Transform, Load) pipelines - Data cleaning: handling missing values, fixing formats - Data transformation: aggregations, joins, calculations - Data validation: ensuring quality and consistency - Scheduling and orchestrating data jobs</p> <p>Outputs: Clean, structured data in warehouses or lakes</p>"},{"location":"lectures/Lecture01_Intro/#stage-3-storage-processing","title":"Stage 3: Storage &amp; Processing","text":"<p>What happens: Data is stored in appropriate systems and made available for querying.</p> <p>Who's involved: Data Engineers, Data Architects</p> <p>Activities: - Designing data models (star schema, snowflake schema) - Choosing appropriate storage systems (RDBMS, data lake, warehouse) - Optimizing for query performance - Managing data partitioning and indexing - Implementing data retention policies</p> <p>Outputs: Queryable, well-organized data repositories</p>"},{"location":"lectures/Lecture01_Intro/#stage-4-analysis-exploration","title":"Stage 4: Analysis &amp; Exploration","text":"<p>What happens: Data is examined to understand patterns, trends, and anomalies.</p> <p>Who's involved: Data Scientists, Data Analysts</p> <p>Activities: - Exploratory Data Analysis (EDA) - Statistical testing and validation - Creating visualizations - Answering ad-hoc business questions - Identifying opportunities for ML/prediction</p> <p>Outputs: Insights, reports, dashboards, hypotheses</p>"},{"location":"lectures/Lecture01_Intro/#stage-5-modeling-ml","title":"Stage 5: Modeling &amp; ML","text":"<p>What happens: Predictive models are built to automate decisions or generate forecasts.</p> <p>Who's involved: Data Scientists, ML Engineers</p> <p>Activities: - Feature engineering - Model selection and training - Hyperparameter tuning - Model evaluation and validation - Iteration based on results</p> <p>Outputs: Trained models ready for deployment</p>"},{"location":"lectures/Lecture01_Intro/#stage-6-deployment","title":"Stage 6: Deployment","text":"<p>What happens: Models are put into production where they can make real predictions.</p> <p>Who's involved: ML Engineers, Data Scientists</p> <p>Activities: - Containerizing models (Docker) - Setting up serving infrastructure - Creating APIs for model access - Implementing monitoring and logging - Setting up automated retraining</p> <p>Outputs: Production models serving predictions</p>"},{"location":"lectures/Lecture01_Intro/#stage-7-insights-decisions","title":"Stage 7: Insights &amp; Decisions","text":"<p>What happens: Results from analysis and models drive business actions.</p> <p>Who's involved: Data Scientists, Data Analysts, Business Stakeholders</p> <p>Activities: - Presenting findings to stakeholders - Creating recommendation reports - Monitoring business impact - Iterating based on feedback - Measuring ROI of data initiatives</p> <p>Outputs: Business decisions, strategy changes, process improvements</p>"},{"location":"lectures/Lecture01_Intro/#5-the-data-scientist-role","title":"5. The Data Scientist Role","text":""},{"location":"lectures/Lecture01_Intro/#the-swiss-army-knife-of-data","title":"The \"Swiss Army Knife\" of Data","text":"<p>Data Scientists are unique because they span the entire data workflow. While specialists focus deeply on one area, data scientists maintain breadth across multiple disciplines.</p>"},{"location":"lectures/Lecture01_Intro/#core-responsibilities-in-detail","title":"Core Responsibilities in Detail","text":""},{"location":"lectures/Lecture01_Intro/#1-problem-definition","title":"1. Problem Definition","text":"<p>Before any analysis begins, data scientists must understand and frame the business problem correctly.</p> <p>What this involves: - Meeting with stakeholders to understand their needs - Translating business questions into analytical problems - Defining success metrics and KPIs - Scoping what's feasible given data and time constraints - Prioritizing which problems to solve first</p> <p>Example: A business leader says \"We're losing customers.\" A data scientist translates this into: \"Predict which customers will churn in the next 30 days with at least 80% precision.\"</p>"},{"location":"lectures/Lecture01_Intro/#2-data-collection-and-preparation","title":"2. Data Collection and Preparation","text":"<p>Data scientists spend 60-80% of their time on data work\u2014finding, cleaning, and preparing data for analysis.</p> <p>What this involves: - Identifying relevant data sources - Writing SQL queries to extract data - Joining data from multiple tables/sources - Handling missing values and outliers - Feature engineering: creating new variables - Data validation and quality checks</p> <p>Why it takes so long: - Data is scattered across multiple systems - Different systems have different formats - Data quality issues are common - Business logic must be correctly implemented - Documentation is often incomplete</p>"},{"location":"lectures/Lecture01_Intro/#3-exploratory-data-analysis-eda","title":"3. Exploratory Data Analysis (EDA)","text":"<p>EDA is the process of examining data to understand its characteristics and discover patterns.</p> <p>What this involves: - Calculating summary statistics (mean, median, distributions) - Creating visualizations (histograms, scatter plots, box plots) - Identifying correlations between variables - Detecting anomalies and outliers - Forming hypotheses about relationships</p> <p>Key questions to answer: - What does the data look like? - Are there any obvious patterns? - What's the distribution of key variables? - Are there data quality issues? - What relationships exist between variables?</p>"},{"location":"lectures/Lecture01_Intro/#4-model-building","title":"4. Model Building","text":"<p>When prediction or classification is needed, data scientists build machine learning models.</p> <p>What this involves: - Selecting appropriate algorithms for the problem - Splitting data into training and test sets - Training models on historical data - Tuning hyperparameters for optimal performance - Evaluating models with appropriate metrics - Comparing multiple approaches</p> <p>Common algorithms used: - Linear/Logistic Regression - Decision Trees and Random Forests - Gradient Boosting (XGBoost, LightGBM) - Neural Networks (for complex patterns) - Clustering algorithms (K-means, DBSCAN)</p>"},{"location":"lectures/Lecture01_Intro/#5-communication-and-storytelling","title":"5. Communication and Storytelling","text":"<p>Perhaps the most underrated skill\u2014data scientists must effectively communicate their findings.</p> <p>What this involves: - Creating clear, compelling visualizations - Writing reports that non-technical people can understand - Presenting findings to executives and stakeholders - Translating statistical results into business implications - Making actionable recommendations</p> <p>Why it matters: The best analysis is worthless if it doesn't lead to action. Communication is what turns insights into impact.</p>"},{"location":"lectures/Lecture01_Intro/#6-deployment-and-monitoring","title":"6. Deployment and Monitoring","text":"<p>Increasingly, data scientists are expected to help deploy their models.</p> <p>What this involves: - Working with ML Engineers to productionize models - Setting up monitoring dashboards - Tracking model performance over time - Identifying when models need retraining - Documenting model behavior and limitations</p>"},{"location":"lectures/Lecture01_Intro/#a-day-in-the-life-of-a-data-scientist","title":"A Day in the Life of a Data Scientist","text":"<p>Morning: - Check Slack/email for urgent requests - Review overnight model performance dashboards - Stand-up meeting with team</p> <p>Mid-Morning: - Deep work: coding, analysis, or modeling - Writing SQL queries for new analysis - Debugging data pipeline issues</p> <p>Afternoon: - Meeting with product manager about upcoming feature - Presenting preliminary findings to stakeholders - Code review with teammates</p> <p>Late Afternoon: - Documentation and cleanup - Planning tomorrow's work - Reading about new techniques or tools</p>"},{"location":"lectures/Lecture01_Intro/#6-real-life-data-science-projects","title":"6. Real-Life Data Science Projects","text":"<p>Understanding how data science works in practice helps illustrate its value. Here are four detailed examples from different industries.</p>"},{"location":"lectures/Lecture01_Intro/#project-1-retail-demand-forecasting","title":"Project 1: Retail \u2014 Demand Forecasting","text":"<p>Company Examples: Walmart, Amazon, Target</p> <p>The Business Problem: Retailers need to know how much of each product to stock in each store. Too little inventory means lost sales; too much means wasted money on storage and eventual markdowns.</p> <p>The Data Science Solution:</p> <p>Data Used: - Historical sales data (daily/weekly by product and location) - Seasonal patterns (holidays, back-to-school, etc.) - Promotional calendars (when items were on sale) - External factors (weather, local events, economic indicators) - Product attributes (category, price point, brand)</p> <p>Methods Applied: - Time series analysis (ARIMA, exponential smoothing) - Machine learning models (XGBoost, Random Forest) - Deep learning (LSTM networks for sequential patterns) - Prophet (Facebook's forecasting tool)</p> <p>Implementation: 1. Built separate models for different product categories 2. Incorporated seasonality and trend components 3. Added promotional lift factors 4. Created ensemble models combining multiple approaches 5. Deployed forecasts to inventory management systems</p> <p>Business Impact: - Reduced stockouts by 30% - Cut excess inventory costs by 25% - Improved customer satisfaction (products available when needed) - Saved millions in logistics costs through better planning</p>"},{"location":"lectures/Lecture01_Intro/#project-2-finance-fraud-detection","title":"Project 2: Finance \u2014 Fraud Detection","text":"<p>Company Examples: PayPal, Visa, Stripe, Banks</p> <p>The Business Problem: Financial institutions process millions of transactions daily. A small percentage are fraudulent, costing billions annually. The challenge is detecting fraud in real-time without blocking legitimate transactions.</p> <p>The Data Science Solution:</p> <p>Data Used: - Transaction details (amount, merchant, location, time) - User behavior patterns (typical spending, usual locations) - Device information (IP address, device fingerprint) - Historical fraud labels (known fraudulent transactions) - Network features (merchant reputation, user connections)</p> <p>Methods Applied: - Anomaly detection (Isolation Forest, One-Class SVM) - Supervised learning (Random Forest, Neural Networks) - Graph analysis (detecting fraud rings) - Rules-based systems (for known fraud patterns) - Real-time scoring (sub-100ms latency requirements)</p> <p>Implementation: 1. Created features capturing normal behavior patterns 2. Built models to score transactions in real-time 3. Implemented cascading system: rules first, then ML models 4. Set up human review queues for borderline cases 5. Continuous feedback loop from confirmed fraud cases</p> <p>Business Impact: - Block 99% of fraudulent transactions - Reduce false positive rate (legitimate transactions blocked) - Save billions in fraud losses annually - Improve customer trust and satisfaction</p>"},{"location":"lectures/Lecture01_Intro/#project-3-healthcare-disease-prediction","title":"Project 3: Healthcare \u2014 Disease Prediction","text":"<p>Company Examples: Google Health, IBM Watson Health, PathAI</p> <p>The Business Problem: Early detection of diseases like cancer dramatically improves patient outcomes. However, radiologists have limited time and can miss subtle signs. AI can help by providing a \"second opinion\" on medical images.</p> <p>The Data Science Solution:</p> <p>Data Used: - Medical images (X-rays, MRIs, CT scans, pathology slides) - Patient demographics and history - Laboratory test results - Physician diagnoses (ground truth labels) - Clinical outcomes (what actually happened to patients)</p> <p>Methods Applied: - Convolutional Neural Networks (CNNs) for image analysis - Transfer learning (using pre-trained models) - Image segmentation (identifying regions of interest) - Ensemble methods (combining multiple models) - Explainability techniques (showing why model made prediction)</p> <p>Implementation: 1. Collected and labeled large datasets of medical images 2. Trained deep learning models on GPU clusters 3. Validated against expert radiologist diagnoses 4. Implemented explainability to show suspicious regions 5. Deployed as a decision support tool (not replacement for doctors)</p> <p>Business Impact: - Detect cancer earlier than human doctors alone - Reduce missed diagnoses - Help doctors prioritize urgent cases - Enable screening in areas with radiologist shortages - Improve patient outcomes through early intervention</p>"},{"location":"lectures/Lecture01_Intro/#project-4-entertainment-recommendation-systems","title":"Project 4: Entertainment \u2014 Recommendation Systems","text":"<p>Company Examples: Netflix, Spotify, Amazon, YouTube</p> <p>The Business Problem: With millions of products/songs/movies available, users can't browse everything. Personalized recommendations help users discover content they'll enjoy, increasing engagement and retention.</p> <p>The Data Science Solution:</p> <p>Data Used: - User interaction history (views, listens, purchases, ratings) - Content metadata (genre, actors, director, audio features) - User demographics (age, location, preferences) - Contextual information (time of day, device, mood) - Social signals (what friends like)</p> <p>Methods Applied: - Collaborative filtering (users who liked X also liked Y) - Content-based filtering (recommend similar items) - Matrix factorization (finding latent factors) - Deep learning (neural collaborative filtering) - Reinforcement learning (optimizing for long-term engagement)</p> <p>Implementation: 1. Built user and item embeddings capturing preferences 2. Created multiple recommendation algorithms 3. Combined approaches for different contexts (home page, search, \"because you watched\") 4. A/B tested extensively to measure impact 5. Continuously retrained as new data arrived</p> <p>Business Impact: - 80% of Netflix views come from recommendations - Increased user engagement and time spent - Reduced churn (users stay subscribed longer) - Helped surface long-tail content - Personalized experience for each user</p>"},{"location":"lectures/Lecture01_Intro/#7-course-outline","title":"7. Course Outline","text":"<p>This 4-month Data Science Applications course covers the full journey from fundamentals to deployment.</p>"},{"location":"lectures/Lecture01_Intro/#module-1-data-science-fundamentals-eda-4-6-weeks","title":"Module 1: Data Science Fundamentals &amp; EDA (4-6 weeks)","text":"<p>1.1 Introduction to Data Science Tools and Environments - Overview of the data science landscape - Setting up your development environment - Python ecosystem: Jupyter, pandas, numpy, matplotlib - Version control with Git</p> <p>1.2 Big Data Technologies and Data Engineering - Introduction to Big Data ecosystem - Hadoop architecture and HDFS - Apache Spark fundamentals - Introduction to databases (SQL and NoSQL) - SQL for data retrieval and manipulation - MongoDB basics - Data processing with PySpark - Building ETL pipelines</p> <p>1.3 Full ETL and EDA on Real-World Datasets - End-to-end project work - Data cleaning and preprocessing - Exploratory data analysis techniques - Data visualization best practices</p>"},{"location":"lectures/Lecture01_Intro/#module-2-advanced-statistics-machine-learning-6-8-weeks","title":"Module 2: Advanced Statistics &amp; Machine Learning (6-8 weeks)","text":"<p>2.1 Advanced Statistical Methods - Probability theory review - Hypothesis testing (t-tests, chi-square tests) - Analysis of Variance (ANOVA) - Linear regression (simple and multiple) - Logistic regression for classification - Polynomial regression - Time series analysis and forecasting - Statistical inference and confidence intervals</p> <p>2.2 Machine Learning Algorithms - Supervised learning fundamentals - Decision Trees and Random Forests - Gradient Boosting Machines (XGBoost, LightGBM) - Support Vector Machines - Unsupervised learning: K-Means clustering - Hierarchical clustering - DBSCAN and density-based methods - Dimensionality reduction (PCA, t-SNE) - Ensemble methods - Model evaluation metrics (accuracy, precision, recall, F1, AUC-ROC) - Cross-validation techniques - Hyperparameter tuning</p>"},{"location":"lectures/Lecture01_Intro/#module-3-deep-learning-4-6-weeks","title":"Module 3: Deep Learning (4-6 weeks)","text":"<p>3.1 Neural Network Fundamentals - Perceptrons and activation functions - Backpropagation and gradient descent - Building neural networks from scratch - Regularization techniques (dropout, batch normalization)</p> <p>3.2 Deep Learning Frameworks - TensorFlow and Keras - PyTorch basics - Model training best practices - GPU utilization</p> <p>3.3 Specialized Architectures - Convolutional Neural Networks (CNNs) for images - Recurrent Neural Networks (RNNs) for sequences - Long Short-Term Memory (LSTM) networks - Natural Language Processing (NLP) fundamentals - Word embeddings and transformers - Advanced time series with deep learning</p>"},{"location":"lectures/Lecture01_Intro/#module-4-capstone-project-4-weeks","title":"Module 4: Capstone Project (4 weeks)","text":"<p>4.1 Project Preparation - Selecting a project topic - Defining scope and success metrics - Data collection strategy - Project planning and timeline</p> <p>4.2 Project Execution - Data preprocessing pipeline - Model development and iteration - Evaluation and refinement - Documentation</p> <p>4.3 Deployment and Presentation - Model deployment (cloud or on-premise) - Creating APIs for model serving - Final presentation preparation - Peer review and feedback</p>"},{"location":"lectures/Lecture01_Intro/#learning-outcomes","title":"Learning Outcomes","text":"<p>By completing this course, you will be able to:</p> <p>\u2705 Work with big data technologies (Hadoop, Spark, PySpark) \u2705 Design and implement ETL pipelines \u2705 Perform comprehensive exploratory data analysis \u2705 Apply advanced statistical methods and hypothesis testing \u2705 Build and evaluate machine learning models \u2705 Develop deep learning solutions for various data types \u2705 Deploy data science solutions to production \u2705 Complete an end-to-end project from conception to deployment</p>"},{"location":"lectures/Lecture01_Intro/#8-data-types-and-classification","title":"8. Data Types and Classification","text":"<p>Understanding how data is structured is fundamental to working with it effectively.</p>"},{"location":"lectures/Lecture01_Intro/#structural-classification-of-data","title":"Structural Classification of Data","text":"<p>Data can be categorized into three main types based on its organization:</p>"},{"location":"lectures/Lecture01_Intro/#type-1-structured-data","title":"Type 1: Structured Data","text":"<p>Definition: Data that adheres to a fixed schema, organized in rows and columns with predefined data types.</p> <p>Characteristics: - Fixed schema defined in advance - Relational model: rows represent records, columns represent attributes - Queryable via SQL or DataFrame operations - Easy to search, filter, and aggregate - High consistency and data integrity</p> <p>Storage Systems: - Relational databases (PostgreSQL, MySQL, Oracle) - Data warehouses (Snowflake, BigQuery, Redshift) - CSV and TSV files - Excel spreadsheets</p> <p>Examples: | Type | Example | |------|---------| | Customer data | ID, Name, Email, Phone, Address | | Transaction records | Transaction_ID, Date, Amount, Status | | Sensor readings | Timestamp, Sensor_ID, Value, Unit | | Inventory data | Product_ID, Quantity, Location, Price |</p> <p>Advantages: - Easy to query and analyze - Strong data integrity through constraints - Efficient storage and indexing - Well-understood tools and techniques</p> <p>Disadvantages: - Schema changes require migrations - Cannot easily store nested or variable data - May not fit all use cases</p>"},{"location":"lectures/Lecture01_Intro/#type-2-semi-structured-data","title":"Type 2: Semi-Structured Data","text":"<p>Definition: Data that doesn't conform to a rigid schema but has some organizational properties like tags or markers.</p> <p>Characteristics: - Self-describing (contains metadata) - Hierarchical or nested structure - Variable fields (different records can have different attributes) - Schema can evolve without migration - Queryable with specialized tools (JSONPath, XPath)</p> <p>Storage Systems: - Document databases (MongoDB, CouchDB) - JSON and XML files - Parquet and Avro (with nested types) - Log files with structure - NoSQL databases</p> <p>Examples:</p> <p>JSON Document: <pre><code>{\n  \"user_id\": \"12345\",\n  \"name\": \"Alice Smith\",\n  \"email\": \"alice@example.com\",\n  \"preferences\": {\n    \"theme\": \"dark\",\n    \"notifications\": true\n  },\n  \"purchase_history\": [\n    {\"item\": \"laptop\", \"date\": \"2024-01-15\"},\n    {\"item\": \"mouse\", \"date\": \"2024-02-20\"}\n  ]\n}\n</code></pre></p> <p>XML Data: <pre><code>&lt;user&gt;\n  &lt;id&gt;12345&lt;/id&gt;\n  &lt;name&gt;Alice Smith&lt;/name&gt;\n  &lt;preferences&gt;\n    &lt;theme&gt;dark&lt;/theme&gt;\n  &lt;/preferences&gt;\n&lt;/user&gt;\n</code></pre></p> <p>Advantages: - Flexible schema evolution - Natural representation of hierarchical data - Good for varying attributes across records - Easy to work with in modern programming languages</p> <p>Disadvantages: - Less efficient for complex queries - Can lead to data inconsistency - More storage overhead than structured data - Joins are more difficult</p>"},{"location":"lectures/Lecture01_Intro/#type-3-unstructured-data","title":"Type 3: Unstructured Data","text":"<p>Definition: Data with no predefined format or organization\u2014free-form content that requires special processing to analyze.</p> <p>Characteristics: - No schema or structure - Requires specialized processing (NLP, computer vision) - Often stored as binary or text blobs - Difficult to query directly - Highest volume of enterprise data</p> <p>Storage Systems: - Object storage (Amazon S3, Google Cloud Storage) - Data lakes - File systems - Content management systems - Media servers</p> <p>Examples: | Type | Examples | |------|----------| | Text | Emails, documents, chat logs, social media posts, reviews | | Images | Photos, screenshots, scanned documents, medical images | | Audio | Voice recordings, music, podcasts, call center recordings | | Video | Movies, surveillance footage, user-generated content | | Other | Log files (unstructured), IoT sensor streams |</p> <p>Processing Methods: - Text: Natural Language Processing (NLP), text mining, sentiment analysis - Images: Computer Vision, CNNs, object detection - Audio: Speech recognition, audio classification - Video: Video analysis, object tracking, action recognition</p> <p>Advantages: - Captures rich, nuanced information - Often the most valuable data for advanced AI - Growing availability and importance</p> <p>Disadvantages: - Requires specialized tools and expertise - Computationally expensive to process - Storage intensive - Quality varies widely</p>"},{"location":"lectures/Lecture01_Intro/#data-type-statistics","title":"Data Type Statistics","text":"<p>In a typical enterprise: - 80% of data is unstructured - 15% is semi-structured - 5% is structured</p> <p>However, most traditional analysis happens on structured data because it's easier to work with.</p>"},{"location":"lectures/Lecture01_Intro/#9-data-collection-methods","title":"9. Data Collection Methods","text":"<p>Data scientists need to understand how data is collected because collection methods affect data quality and what analyses are possible.</p>"},{"location":"lectures/Lecture01_Intro/#primary-data-collection","title":"Primary Data Collection","text":"<p>Definition: Data you collect specifically for your analysis\u2014it didn't exist before your collection effort.</p>"},{"location":"lectures/Lecture01_Intro/#a-surveys-and-questionnaires","title":"A. Surveys and Questionnaires","text":"<p>What it is: Directly asking people questions to gather their responses.</p> <p>Examples: - Customer satisfaction surveys after purchase - Employee engagement questionnaires - Market research surveys - User feedback forms</p> <p>Tools: Google Forms, SurveyMonkey, Typeform, Qualtrics</p> <p>Advantages: - Specific to your needs\u2014you control the questions - Can gather data that doesn't exist elsewhere - Can target specific populations - Good for understanding opinions and motivations</p> <p>Disadvantages: - Time-consuming to design and administer - Response bias (people may not answer honestly) - Selection bias (who responds may not be representative) - Low response rates can limit sample size</p> <p>Best Practices: - Keep surveys short and focused - Use clear, unambiguous questions - Include validation questions - Offer incentives for completion - Test before full deployment</p>"},{"location":"lectures/Lecture01_Intro/#b-event-tracking-observation","title":"B. Event Tracking / Observation","text":"<p>What it is: Recording behaviors and actions as they happen.</p> <p>Examples: - Website click tracking - Mobile app usage analytics - Video recording of user sessions - In-store behavior observation - A/B testing interactions</p> <p>Tools: Google Analytics, Mixpanel, Amplitude, Hotjar, FullStory</p> <p>Advantages: - Captures real behavior (not self-reported) - Continuous, automated collection - High volume of data - Objective and consistent</p> <p>Disadvantages: - May not capture the \"why\" behind actions - Privacy concerns - Technical implementation required - Can miss context</p>"},{"location":"lectures/Lecture01_Intro/#c-sensors-and-iot-devices","title":"C. Sensors and IoT Devices","text":"<p>What it is: Automated data capture from physical devices and sensors.</p> <p>Examples: - Fitness trackers (steps, heart rate) - Temperature sensors in warehouses - GPS tracking in vehicles - Manufacturing equipment sensors - Smart home devices</p> <p>Tools: Arduino, Raspberry Pi, commercial IoT platforms (AWS IoT, Azure IoT)</p> <p>Advantages: - Continuous, objective data - High frequency capture - Covers physical world phenomena - Scalable to many devices</p> <p>Disadvantages: - Equipment costs - Maintenance requirements - Data transmission challenges - Sensor calibration issues</p>"},{"location":"lectures/Lecture01_Intro/#secondary-data-collection","title":"Secondary Data Collection","text":"<p>Definition: Using data that already exists\u2014collected by others for various purposes.</p>"},{"location":"lectures/Lecture01_Intro/#a-public-datasets","title":"A. Public Datasets","text":"<p>What it is: Pre-collected data available freely or for a fee.</p> <p>Sources: - Government: census.gov, data.gov, WHO - Research: Kaggle, UCI ML Repository, academic institutions - Organizations: World Bank, UN Data, IMF - Open data initiatives: European Open Data Portal</p> <p>Advantages: - Free or low cost - Often large scale - Pre-cleaned in many cases - Good for learning and benchmarking</p> <p>Disadvantages: - May not fit your exact needs - Data may be outdated - Limited control over quality - Citation and licensing requirements</p>"},{"location":"lectures/Lecture01_Intro/#b-apis-application-programming-interfaces","title":"B. APIs (Application Programming Interfaces)","text":"<p>What it is: Structured interfaces for requesting data from services programmatically.</p> <p>Examples: - Twitter API for tweets - Weather APIs for forecasts - Financial APIs for stock prices - Google Maps API for location data</p> <p>Tools: Python requests library, Postman, API-specific SDKs</p> <p>Advantages: - Clean, structured data - Real-time or frequently updated - Official data from source - Programmatic access enables automation</p> <p>Disadvantages: - Rate limits on requests - May require payment for volume - API changes can break your code - Authentication complexity</p> <p>Example API Call: <pre><code>import requests\n\nresponse = requests.get(\n    \"https://api.weather.com/v1/current\",\n    params={\"location\": \"New York\", \"api_key\": \"YOUR_KEY\"}\n)\ndata = response.json()\n</code></pre></p>"},{"location":"lectures/Lecture01_Intro/#c-web-scraping","title":"C. Web Scraping","text":"<p>What it is: Programmatically extracting data from websites.</p> <p>Examples: - Product prices from e-commerce sites - News articles from media sites - Real estate listings - Social media posts (where allowed)</p> <p>Tools: BeautifulSoup, Scrapy, Selenium (Python)</p> <p>Advantages: - Access to vast amounts of web data - Can get data not available via API - Customizable to your needs</p> <p>Disadvantages: - Legal and ethical concerns (check terms of service) - Website structure changes can break scrapers - May be blocked by sites - Data quality varies</p> <p>Legal Considerations: - Always check the website's robots.txt and terms of service - Don't overload servers with requests - Respect data privacy - Some scraping may violate laws (CFAA in US)</p>"},{"location":"lectures/Lecture01_Intro/#d-internal-databases","title":"D. Internal Databases","text":"<p>What it is: Querying existing databases within your organization.</p> <p>Examples: - CRM records (Salesforce) - Transaction databases - HR systems - Product databases</p> <p>Advantages: - Reliable, historical data - Already collected and maintained - Directly relevant to business questions - Structured and documented</p> <p>Disadvantages: - Need access permissions - May require understanding complex schemas - Data silos between departments - May have quality issues</p>"},{"location":"lectures/Lecture01_Intro/#e-third-party-data-providers","title":"E. Third-Party Data Providers","text":"<p>What it is: Purchasing data from specialized vendors.</p> <p>Examples: - Nielsen ratings (media consumption) - Credit bureau data (Experian, Equifax) - Market research data - Demographic data providers</p> <p>Advantages: - High quality, specialized data - Covers hard-to-collect information - Professional collection methods</p> <p>Disadvantages: - Expensive - Licensing restrictions - May not be exclusive to you - Integration challenges</p>"},{"location":"lectures/Lecture01_Intro/#data-quality-considerations","title":"Data Quality Considerations","text":"<p>When collecting data, always consider:</p> <p>1. Completeness - Are there missing values? - What's the coverage of the data? - Are certain populations underrepresented?</p> <p>2. Accuracy - Is the data correct? - What are the error rates? - How was accuracy validated?</p> <p>3. Consistency - Is the format uniform throughout? - Are definitions consistent? - How are edge cases handled?</p> <p>4. Timeliness - How current is the data? - What's the update frequency? - Is it current enough for your needs?</p> <p>5. Relevance - Does the data actually measure what you need? - Is there a proxy vs direct measurement issue? - Are there confounding factors?</p>"},{"location":"lectures/Lecture01_Intro/#sampling-methods","title":"Sampling Methods","text":"<p>When you can't collect data from everyone, you sample:</p> <p>Random Sampling: Every member of the population has an equal chance of being selected. Gold standard but not always practical.</p> <p>Stratified Sampling: Ensure representation of specific subgroups (e.g., sample equally from each age group).</p> <p>Convenience Sampling: Use whatever data is available. Fast but watch for bias!</p> <p>Systematic Sampling: Select every nth item (e.g., every 10th customer).</p>"},{"location":"lectures/Lecture01_Intro/#legal-and-ethical-considerations","title":"Legal and Ethical Considerations","text":"<p>Privacy: - Do you have consent to collect and use this data? - Are you compliant with GDPR, CCPA, HIPAA, etc.? - Is personal information properly protected?</p> <p>Ownership: - Who owns this data? - What are the licensing terms? - Can you use it for your intended purpose?</p> <p>Bias: - Does your collection method favor certain groups? - Are marginalized populations represented? - Could your data lead to discriminatory outcomes?</p>"},{"location":"lectures/Lecture01_Intro/#10-events-and-data-generation","title":"10. Events and Data Generation","text":"<p>Understanding how data is generated helps you work with it more effectively and understand its limitations.</p>"},{"location":"lectures/Lecture01_Intro/#core-principle","title":"Core Principle","text":"<p>Every piece of data begins as a real-world event that gets captured and stored.</p> <p>When you see a row in a database, that row represents something that happened in the real world\u2014a click, a purchase, a sensor reading, a user signing up. Understanding this connection helps you:</p> <ul> <li>Know what the data actually represents</li> <li>Understand potential quality issues</li> <li>Identify what's missing</li> <li>Make better analytical decisions</li> </ul>"},{"location":"lectures/Lecture01_Intro/#example-the-journey-of-a-purchase-event","title":"Example: The Journey of a Purchase Event","text":"<p>When someone buys something on Amazon, that single action creates data in multiple systems:</p> <pre><code>User clicks \"Buy Now\"\n         \u2502\n         \u251c\u2500\u2192 E-commerce platform: Order details (items, quantity, price)\n         \u2502\n         \u251c\u2500\u2192 Payment processor: Transaction data (amount, method, timestamp)\n         \u2502\n         \u251c\u2500\u2192 Inventory system: Stock level updates\n         \u2502\n         \u251c\u2500\u2192 Shipping provider: Tracking information\n         \u2502\n         \u251c\u2500\u2192 CRM system: Customer interaction log\n         \u2502\n         \u2514\u2500\u2192 Analytics platform: User behavior data (clicks, time on page)\n</code></pre> <p>The Challenge for Data Scientists: - Each system records different aspects - Timestamps might differ by milliseconds - Some data may be missing or delayed - You need to integrate across systems - Must determine the \"source of truth\" for each data point</p>"},{"location":"lectures/Lecture01_Intro/#event-data-structure","title":"Event Data Structure","text":"<p>Events are typically captured as structured records (often JSON):</p> <pre><code>{\n  \"event_type\": \"button_click\",\n  \"event_time\": \"2024-03-15T14:23:45.123Z\",\n  \"user_id\": \"user_12345\",\n  \"session_id\": \"sess_abc123\",\n  \"button_id\": \"checkout_button\",\n  \"page_url\": \"/cart\",\n  \"device_type\": \"mobile\",\n  \"browser\": \"Chrome\",\n  \"ip_address\": \"192.168.1.1\",\n  \"properties\": {\n    \"cart_value\": 149.99,\n    \"item_count\": 3\n  }\n}\n</code></pre> <p>Key Fields Explained:</p> Field Purpose Example event_type What happened button_click, page_view, purchase event_time When it occurred ISO 8601 timestamp user_id Who did it Unique user identifier session_id Groups related events Unique session identifier page_url Where it happened /cart, /checkout device_type Context mobile, desktop, tablet properties Event-specific details Varies by event type"},{"location":"lectures/Lecture01_Intro/#simple-vs-complex-events","title":"Simple vs Complex Events","text":"<p>Simple Event (Button Click): <pre><code>{\n  \"event_type\": \"button_click\",\n  \"event_time\": \"2024-03-15T14:23:45Z\",\n  \"user_id\": \"user_12345\",\n  \"button_id\": \"checkout_button\"\n}\n</code></pre></p> <p>Complex Event (Order Placed): <pre><code>{\n  \"event_type\": \"order_placed\",\n  \"event_time\": \"2024-03-15T14:25:10Z\",\n  \"order_id\": \"ORD_789\",\n  \"user_id\": \"user_12345\",\n  \"items\": [\n    {\"product_id\": \"P001\", \"quantity\": 2, \"price\": 29.99},\n    {\"product_id\": \"P052\", \"quantity\": 1, \"price\": 49.99}\n  ],\n  \"total_amount\": 109.97,\n  \"payment_method\": \"credit_card\",\n  \"shipping_address\": {\n    \"street\": \"123 Main St\",\n    \"city\": \"New York\",\n    \"zip\": \"10001\"\n  },\n  \"discount_code\": \"SPRING2024\"\n}\n</code></pre></p>"},{"location":"lectures/Lecture01_Intro/#11-data-lineage-and-quality","title":"11. Data Lineage and Quality","text":""},{"location":"lectures/Lecture01_Intro/#what-is-data-lineage","title":"What is Data Lineage?","text":"<p>Data lineage tracks the journey of data from its origin through all transformations to its final form. It answers: \"Where did this data come from, and how did it get here?\"</p>"},{"location":"lectures/Lecture01_Intro/#data-lineage-example","title":"Data Lineage Example","text":"<pre><code>Raw Event: User clicked \"Buy Now\" at 2:35:42 PM\n         \u2502\n         \u25bc\nWeb Server Log: Request logged at 2:35:42.123 PM\n         \u2502\n         \u25bc\nAnalytics Platform: Event tracked at 2:35:42.456 PM (300ms delay)\n         \u2502\n         \u25bc\nMessage Queue: Event queued at 2:35:42.500 PM\n         \u2502\n         \u25bc\nData Warehouse: Batch loaded at 3:00:00 PM (24-minute delay)\n         \u2502\n         \u25bc\nYour Analysis: Query run at 4:15:00 PM\n</code></pre> <p>Why This Matters: Understanding this lineage tells you: \"This analysis includes purchases up to 3 PM, not real-time data.\"</p>"},{"location":"lectures/Lecture01_Intro/#how-data-quality-affects-models","title":"How Data Quality Affects Models","text":"<p>Poor data quality leads to poor models. The saying \"garbage in, garbage out\" is absolutely true in data science.</p> <p>Common Data Quality Issues:</p> Issue Description Impact on Models Missing values Gaps in data Biased predictions, reduced accuracy Duplicates Same event recorded multiple times Inflated counts, wrong patterns Incorrect values Typos, wrong entries Wrong relationships learned Inconsistent formats Same thing represented differently Failed joins, missed patterns Outdated data Information no longer current Predictions based on old patterns Selection bias Some populations missing Model doesn't generalize"},{"location":"lectures/Lecture01_Intro/#the-human-element-in-data-quality","title":"The Human Element in Data Quality","text":"<p>Data quality isn't just a technical issue\u2014human decisions during data collection have lasting impacts.</p> <p>Example Scenario:</p> <p>A Product Manager decides to track a new recommendation feature:</p> <p>What they tracked: - Number of recommendations shown - Click-through rate - Items added to cart from recommendations - Final purchases from recommendations</p> <p>What they didn't track: - Time spent viewing recommendations - Recommendations scrolled past but not clicked - Order of recommendations clicked - User's browsing history before seeing recommendations</p> <p>Impact on Your Analysis: You want to know \"Do users prefer recommendations at the top of the page?\"</p> <p>\u2192 Can't answer because position data wasn't tracked \u2192 Must request new tracking implementation \u2192 Can only analyze future data, not historical</p> <p>Lesson: Data scientists should be involved in tracking decisions early to ensure analytical needs are met.</p>"},{"location":"lectures/Lecture01_Intro/#engineering-decisions-affect-data-quality","title":"Engineering Decisions Affect Data Quality","text":"<p>How data is collected technically can change what you see:</p> <p>Client-side tracking: <pre><code>button.addEventListener('click', () =&gt; {\n  sendEventToServer({type: 'button_click'});\n  // Continue with action immediately\n});\n</code></pre> - Risk: If network is slow or user navigates away, event might not be sent - Result: Undercount of clicks</p> <p>Server-side tracking: <pre><code>button.addEventListener('click', async () =&gt; {\n  await sendEventToServer({type: 'button_click'});\n  // Wait for confirmation, then continue\n});\n</code></pre> - Risk: User experiences delay, may click multiple times - Result: Accurate count but worse user experience</p> <p>The Same Feature Can Show Different Numbers Depending on Implementation!</p>"},{"location":"lectures/Lecture01_Intro/#12-event-driven-architecture","title":"12. Event-Driven Architecture","text":""},{"location":"lectures/Lecture01_Intro/#critical-design-decisions","title":"Critical Design Decisions","text":"<p>When designing event tracking systems, several decisions significantly impact what data is available for analysis.</p>"},{"location":"lectures/Lecture01_Intro/#decision-1-granularity","title":"Decision 1: Granularity","text":"<p>High Granularity: Track everything\u2014every mouse movement, scroll, hover, keystroke.</p> <p>Advantages: - Rich behavioral data - Can analyze user hesitation - Enables detailed session replay - Supports advanced analysis</p> <p>Disadvantages: - Massive data volume (storage costs) - Privacy concerns - Processing complexity - Signal-to-noise ratio issues</p> <p>Examples: Hotjar, FullStory, Heap Analytics</p> <p>Low Granularity: Track only major actions\u2014clicks, purchases, sign-ups.</p> <p>Advantages: - Manageable data volume - Clearer signals - Easier to process - Lower storage costs</p> <p>Disadvantages: - Miss subtle behavior patterns - Can't answer detailed questions - Limited to pre-defined events</p> <p>Examples: Google Analytics (standard setup), basic event logging</p>"},{"location":"lectures/Lecture01_Intro/#decision-2-timing-synchronous-vs-asynchronous","title":"Decision 2: Timing (Synchronous vs Asynchronous)","text":"<p>Synchronous Collection: <pre><code>User Action \u2192 Wait for Logging \u2192 Wait for Confirmation \u2192 Continue\n</code></pre> - User sees confirmation only after event is logged - Pro: Guaranteed data capture - Con: Slower user experience</p> <p>Asynchronous Collection: <pre><code>User Action \u2192 Immediately Continue \u2192 Log Event in Background\n</code></pre> - User continues immediately, logging happens separately - Pro: Fast user experience - Con: May lose data if user closes browser or network fails</p> <p>Trade-off: Speed vs reliability. Choose based on how critical the data is.</p>"},{"location":"lectures/Lecture01_Intro/#decision-3-event-sourcing","title":"Decision 3: Event Sourcing","text":"<p>Traditional Approach (State-Based): Store only the current state.</p> <pre><code>User Account Table:\n- user_id: 12345\n- balance: $150\n- status: active\n</code></pre> <p>You only see the current state. How did the balance get to $150? Unknown.</p> <p>Event Sourcing Approach: Store every change as an event.</p> <pre><code>Event Log:\n1. account_created (balance: $0)\n2. deposit ($100)\n3. deposit ($75)\n4. withdrawal ($25)\n\nCurrent State: $150 (calculated from events)\n</code></pre> <p>Advantages of Event Sourcing: - Can replay history - Can analyze \"how did we get here?\" - Can reconstruct state at any past point - Perfect audit trail - Can fix bugs by reprocessing events</p> <p>Disadvantages: - More complex to implement - Storage requirements higher - Query patterns different - Eventual consistency challenges</p>"},{"location":"lectures/Lecture01_Intro/#13-traditional-data-vs-big-data","title":"13. Traditional Data vs Big Data","text":""},{"location":"lectures/Lecture01_Intro/#when-does-data-become-big","title":"When Does Data Become \"Big\"?","text":"<p>Big Data isn't just about size\u2014it's about data that exceeds the capabilities of traditional systems to store, process, or analyze effectively.</p>"},{"location":"lectures/Lecture01_Intro/#comparison-table","title":"Comparison Table","text":"Aspect Traditional Data Big Data Volume GB to low TB TB to EB Processing Single machine Distributed cluster Storage Single database Distributed storage Scaling Vertical (bigger server) Horizontal (more servers) Cost curve Exponential (n\u00b2) Linear (n) Consistency Strong (ACID) Eventual (BASE) Latency Batch (minutes-hours) Batch + Stream (ms-hours) Data types Structured Structured + Semi + Unstructured Tools RDBMS, Excel, Pandas Hadoop, Spark, Kafka"},{"location":"lectures/Lecture01_Intro/#understanding-scale","title":"Understanding Scale","text":"<pre><code>Data Size Reference:\n1 KB  (Kilobyte)  = A short email\n1 MB  (Megabyte)  = A high-resolution photo\n1 GB  (Gigabyte)  = A movie\n1 TB  (Terabyte)  = ~500 movies, or ~1 billion rows of simple data\n1 PB  (Petabyte)  = 1,024 TB = Netflix's entire library\n1 EB  (Exabyte)   = 1,024 PB = All words ever spoken by humans (estimated)\n</code></pre> <p>Traditional systems: Handle up to ~1 TB comfortably on a single machine Big Data systems: Handle TB to EB across distributed clusters</p>"},{"location":"lectures/Lecture01_Intro/#why-size-matters","title":"Why Size Matters","text":"<p>Your computer has limited RAM (working memory): - Typical laptop: 8-32 GB - Typical server: 64-512 GB</p> <p>If your data fits in RAM, you can use simple tools (Pandas, R). If not, you need distributed systems (Spark, Hadoop).</p>"},{"location":"lectures/Lecture01_Intro/#processing-single-vs-distributed","title":"Processing: Single vs Distributed","text":"<p>Single Machine Processing O(n): <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      YOUR COMPUTER          \u2502\n\u2502                             \u2502\n\u2502  CPU processes all          \u2502\n\u2502  1 billion rows             \u2502\n\u2502  one after another          \u2502\n\u2502                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nTime = n (linear with data size)\n</code></pre></p> <p>Distributed Processing O(n/p) + O(log p): <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Machine 1  \u2502  \u2502  Machine 2  \u2502  \u2502  Machine 3  \u2502  \u2502  Machine 4  \u2502\n\u2502 250M rows   \u2502  \u2502 250M rows   \u2502  \u2502 250M rows   \u2502  \u2502 250M rows   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                \u2502                \u2502                \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                  \u2502\n                                  \u25bc\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502    Combine    \u2502\n                         \u2502    Results    \u2502\n                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nTime = n/p (data per machine) + log(p) (combining results)\n</code></pre></p> <p>Where: - n = total data size - p = number of machines</p> <p>With 4 machines processing 1 billion rows: - Each processes 250 million rows (parallel) - Then results are combined (log\u2082(4) = 2 steps)</p>"},{"location":"lectures/Lecture01_Intro/#scaling-vertical-vs-horizontal","title":"Scaling: Vertical vs Horizontal","text":"<p>Vertical Scaling (Scale UP): Make one machine more powerful.</p> <pre><code>Before:                     After:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Small       \u2502            \u2502 BIGGER              \u2502\n\u2502 Server      \u2502     \u2192      \u2502 SERVER              \u2502\n\u2502 16GB RAM    \u2502            \u2502 256GB RAM           \u2502\n\u2502 4 CPU cores \u2502            \u2502 64 CPU cores        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Cost: Grows exponentially (n\u00b2) - 2x RAM costs ~3x price - High-end hardware is disproportionately expensive - Physical limits eventually reached</p> <p>Horizontal Scaling (Scale OUT): Add more machines.</p> <pre><code>Before:                     After:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             \u2502            \u2502 Server  \u2502 \u2502 Server  \u2502 \u2502 Server  \u2502 \u2502 Server  \u2502\n\u2502 1 Server    \u2502     \u2192      \u2502   1     \u2502 \u2502   2     \u2502 \u2502   3     \u2502 \u2502   4     \u2502\n\u2502             \u2502            \u2502         \u2502 \u2502         \u2502 \u2502         \u2502 \u2502         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Cost: Grows linearly (n) - Need 2x capacity? Add 2x machines - Uses commodity hardware (cheaper) - No physical limits (keep adding)</p>"},{"location":"lectures/Lecture01_Intro/#consistency-acid-vs-base","title":"Consistency: ACID vs BASE","text":"<p>ACID (Traditional Databases): - Atomicity: All operations succeed or all fail - Consistency: Database always in valid state - Isolation: Concurrent transactions don't interfere - Durability: Committed changes are permanent</p> <p>Example: Bank transfer must debit one account AND credit another\u2014never just one.</p> <p>BASE (Distributed Systems): - Basically Available: System always responds - Soft state: State may change over time - Eventually consistent: All nodes will converge</p> <p>Example: Instagram likes might show different counts to different users for a few seconds, then converge.</p> <p>Why Big Data Uses Eventual Consistency:</p> <p>Strong consistency requires all nodes to agree before responding: <pre><code>Write \u2192 Node 1 confirms \u2192 Node 2 confirms \u2192 Node 3 confirms \u2192 \"Success!\"\n(Slow but guaranteed consistent)\n</code></pre></p> <p>Eventual consistency responds immediately: <pre><code>Write \u2192 Node 1 confirms \u2192 \"Success!\" \u2192 Nodes 2,3 sync in background\n(Fast but temporarily inconsistent)\n</code></pre></p> <p>For analytical workloads, eventual consistency is usually acceptable.</p>"},{"location":"lectures/Lecture01_Intro/#latency-batch-vs-stream","title":"Latency: Batch vs Stream","text":"<p>Batch Processing: Collect data, then process in bulk.</p> <pre><code>Hour 1:  Collect data \u2500\u2500\u2500\u2500\u2500\u2510\nHour 2:  Collect data \u2500\u2500\u2500\u2500\u2500\u2524\nHour 3:  Collect data \u2500\u2500\u2500\u2500\u2500\u2524\nHour 4:  Collect data \u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\nHour 5:  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 PROCESS ALL DATA AT ONCE    \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\nHour 6:  Results ready!\n</code></pre> <p>Latency: Minutes to hours Use cases: Monthly reports, data warehouse updates, model training</p> <p>Stream Processing: Process data immediately as it arrives.</p> <pre><code>Event 1 \u2500\u2500\u2192 Process \u2500\u2500\u2192 Result (5ms later)\nEvent 2 \u2500\u2500\u2192 Process \u2500\u2500\u2192 Result (5ms later)\nEvent 3 \u2500\u2500\u2192 Process \u2500\u2500\u2192 Result (5ms later)\n</code></pre> <p>Latency: Milliseconds Use cases: Fraud detection, real-time recommendations, live dashboards</p> <p>Real-World Example: Credit Card Fraud</p> <p>Batch (Traditional): 1. Purchase at 2:00 PM 2. Transaction stored 3. Fraud check runs at midnight 4. Fraud detected at 12:05 AM 5. Card blocked 10 hours later 6. Thief already spent $5000!</p> <p>Stream (Big Data): 1. Purchase at 2:00 PM 2. Transaction analyzed in 50ms 3. Fraud detected at 2:00:00.050 PM 4. Card blocked immediately 5. Thief stopped!</p>"},{"location":"lectures/Lecture01_Intro/#storage-row-vs-column-oriented","title":"Storage: Row vs Column Oriented","text":"<p>Row-Oriented (Traditional RDBMS): Data stored row by row.</p> <pre><code>Logical Table:\n| Name  | Age | City | Salary |\n|-------|-----|------|--------|\n| Alice | 30  | NYC  | 70000  |\n| Bob   | 25  | LA   | 60000  |\n\nPhysical Storage:\n[Alice,30,NYC,70000][Bob,25,LA,60000]\n   Row 1               Row 2\n</code></pre> <p>Good for: Updating entire records, OLTP workloads Bad for: Aggregating single columns across many rows</p> <p>Column-Oriented (Big Data, Analytics): Data stored column by column.</p> <pre><code>Same Logical Table...\n\nPhysical Storage:\n[Alice,Bob]           \u2190 All names together\n[30,25]               \u2190 All ages together\n[NYC,LA]              \u2190 All cities together\n[70000,60000]         \u2190 All salaries together\n</code></pre> <p>Good for: Aggregations (AVG, SUM, COUNT), analytics Bad for: Updating single records</p> <p>Why Column Storage is Faster for Analytics:</p> <p>Query: \"What is the average salary?\"</p> <p>Row storage: Must read ALL columns for ALL rows <pre><code>Read: Alice,30,NYC,70000 | Bob,25,LA,60000 | ...\nOnly need: 70000, 60000\nWasted reads!\n</code></pre></p> <p>Column storage: Read only the salary column <pre><code>Read: 70000,60000,80000,...\nMuch less data from disk!\n</code></pre></p>"},{"location":"lectures/Lecture01_Intro/#14-the-3-vs-of-big-data","title":"14. The 3 V's of Big Data","text":"<p>The three V's define what makes data \"big\" and challenging for traditional systems.</p>"},{"location":"lectures/Lecture01_Intro/#volume","title":"Volume","text":"<p>Definition: The sheer amount of data generated and stored.</p> <p>Scale Examples: - Facebook generates 4 petabytes per day - Twitter processes 500 million tweets per day - Netflix stores 15+ petabytes of content - YouTube receives 500 hours of video per minute - Large Hadron Collider generates 1 PB per second (filtered to 25 PB/year)</p> <p>Why It's a Challenge: - Can't fit on single machine - Traditional databases max out - Storage costs become significant - Processing time grows with data</p> <p>Solutions: - Distributed storage (HDFS, S3) - Horizontal scaling - Data compression - Tiered storage (hot/warm/cold)</p>"},{"location":"lectures/Lecture01_Intro/#velocity","title":"Velocity","text":"<p>Definition: The speed at which data is generated and must be processed.</p> <p>Speed Examples: - IoT sensors: millions of readings per second - Stock market: microsecond updates - Social media: thousands of posts per second - Credit card transactions: 65,000 per second globally - Autonomous vehicles: generate 4 TB per day each</p> <p>Why It's a Challenge: - Batch processing too slow - Real-time decisions needed - Data arrives continuously - Must process faster than it arrives</p> <p>Solutions: - Stream processing (Kafka, Flink, Spark Streaming) - In-memory computing - Message queues - Real-time analytics engines</p>"},{"location":"lectures/Lecture01_Intro/#variety","title":"Variety","text":"<p>Definition: The different types, formats, and sources of data.</p> <p>Data Types: - Structured: Database tables, CSV files - Semi-structured: JSON, XML, logs - Unstructured: Text, images, video, audio</p> <p>Source Variety: - Internal databases - External APIs - User-generated content - Sensors and IoT - Social media - Third-party data providers</p> <p>Why It's a Challenge: - Different schemas and formats - Integration complexity - No single query language works for all - Quality varies by source</p> <p>Solutions: - Data lakes (store everything, schema on read) - ETL/ELT pipelines - Data catalogs - Flexible query engines (Spark)</p>"},{"location":"lectures/Lecture01_Intro/#additional-vs","title":"Additional V's","text":"<p>While the original 3 V's are most common, additional V's are often mentioned:</p> <p>Veracity (Quality): - Data may be incomplete, inconsistent, or incorrect - Requires validation and cleaning - Quality affects model reliability</p> <p>Value (Business Impact): - Raw data has little value - Must be processed for insights - ROI on infrastructure must be justified</p>"},{"location":"lectures/Lecture01_Intro/#15-big-data-architecture","title":"15. Big Data Architecture","text":""},{"location":"lectures/Lecture01_Intro/#traditional-architecture-pre-2010","title":"Traditional Architecture (Pre-2010)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Data      \u2502 \u2192  \u2502   ETL   \u2502 \u2192  \u2502    Single     \u2502 \u2192  \u2502  BI Tools    \u2502\n\u2502   Sources    \u2502    \u2502         \u2502    \u2502   Database    \u2502    \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Problems: - Single server bottleneck (one machine does everything) - Vertical scaling hits physical and cost limits - Batch processing only (wait hours for results) - Structured data only (can't handle images, video) - Expensive specialized hardware required</p>"},{"location":"lectures/Lecture01_Intro/#modern-big-data-architecture","title":"Modern Big Data Architecture","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502        DATA SOURCES             \u2502\n                    \u2502  (APIs, DBs, IoT, Logs, etc.)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                                      \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     INGESTION LAYER             \u2502\n                    \u2502  (Kafka, Flume, Kinesis)        \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                                      \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     DISTRIBUTED STORAGE         \u2502\n                    \u2502  (HDFS, S3, Data Lake)          \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                                      \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     PROCESSING LAYER            \u2502\n                    \u2502  (Spark, Hadoop, Flink)         \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n                                      \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502     ANALYTICS &amp; ML              \u2502\n                    \u2502  (Jupyter, TensorFlow, etc.)    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Advantages: - Horizontal scaling (add more commodity servers) - Fault tolerant (server fails, no data loss) - Real-time + batch processing supported - All data types (structured, semi, unstructured) - Cost-effective (uses cheap hardware)</p>"},{"location":"lectures/Lecture01_Intro/#lambda-architecture","title":"Lambda Architecture","text":"<p>Lambda Architecture combines batch and real-time processing to provide both accuracy and speed.</p> <pre><code>                         DATA SOURCES\n                              \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                   \u2502\n                    \u25bc                   \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502  BATCH LAYER  \u2502   \u2502  SPEED LAYER  \u2502\n            \u2502               \u2502   \u2502               \u2502\n            \u2502 Hadoop/Spark  \u2502   \u2502 Storm/Flink   \u2502\n            \u2502 Complete data \u2502   \u2502 Recent data   \u2502\n            \u2502 Accurate      \u2502   \u2502 Fast          \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502                   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  SERVING LAYER  \u2502\n                    \u2502  Merge Results  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>How It Works:</p> <ol> <li>Batch Layer:</li> <li>Processes ALL historical data</li> <li>Runs periodically (hourly, daily)</li> <li>Produces complete, accurate views</li> <li>Tools: Hadoop, Spark batch jobs</li> <li> <p>Example: \"Total sales for all time\"</p> </li> <li> <p>Speed Layer:</p> </li> <li>Processes only recent data</li> <li>Real-time processing</li> <li>Low latency (seconds)</li> <li>Tools: Storm, Flink, Spark Streaming</li> <li> <p>Example: \"Sales in last 5 minutes\"</p> </li> <li> <p>Serving Layer:</p> </li> <li>Merges batch and speed results</li> <li>Provides unified query interface</li> <li>Example: \"Total sales\" = batch_total + last_5_min</li> </ol> <p>Pros: - Best of both worlds (accuracy + speed) - Fault tolerant - Handles all data velocities</p> <p>Cons: - Complex to maintain (two codebases) - Duplicate logic in batch and speed layers - High resource usage</p>"},{"location":"lectures/Lecture01_Intro/#kappa-architecture","title":"Kappa Architecture","text":"<p>Kappa simplifies Lambda by treating everything as a stream.</p> <pre><code>    DATA SOURCES\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  STREAMING PLATFORM \u2502\n\u2502       (Kafka)       \u2502\n\u2502  Stores all events  \u2502\n\u2502  Replayable         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  STREAM PROCESSING  \u2502\n\u2502   (Flink, Spark)    \u2502\n\u2502  Single codebase    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   SERVING LAYER     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Idea: Treat ALL data as streams, even historical data. Need to reprocess? Replay the stream from the beginning.</p> <p>How It Works: 1. All data goes to Kafka (can store days/weeks) 2. Stream processors consume in real-time 3. Need historical analysis? Replay from beginning 4. Single codebase for all processing</p> <p>Pros: - Simpler than Lambda (one codebase) - Real-time by default - Easier to maintain and reason about</p> <p>Cons: - Requires replayable streams (storage in Kafka) - Higher infrastructure cost - Stream processing can be complex</p> <p>When to Use: When real-time is the default and you can afford stream infrastructure.</p>"},{"location":"lectures/Lecture01_Intro/#modern-data-stack","title":"Modern Data Stack","text":"<p>Cloud-native architecture with best-of-breed tools.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     DATA SOURCES                        \u2502\n\u2502            (Databases, APIs, SaaS Apps)                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   DATA INGESTION                        \u2502\n\u2502              (Fivetran, Airbyte, Stitch)               \u2502\n\u2502              Automated data connectors                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               CLOUD DATA WAREHOUSE                      \u2502\n\u2502           (Snowflake, BigQuery, Redshift)              \u2502\n\u2502              Auto-scaling, managed                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  TRANSFORMATION                         \u2502\n\u2502                      (dbt)                             \u2502\n\u2502              SQL-based data modeling                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 BI &amp; ANALYTICS                          \u2502\n\u2502          (Tableau, Looker, Power BI, Jupyter)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Principles: - Cloud-native: No infrastructure management - SQL-based: Transformations in SQL (dbt) - Self-service: Analysts can work independently - Modular: Best tool for each job - Pay-as-you-go: Only pay for what you use</p> <p>Typical Stack: - Ingestion: Fivetran or Airbyte (automated connectors) - Storage: Snowflake or BigQuery (auto-scaling warehouse) - Transformation: dbt (SQL-based modeling) - Orchestration: Airflow or Prefect - BI: Looker, Tableau, or Metabase</p> <p>Why It's Popular: - Fast setup (days, not months) - Managed services (less ops work) - Scales automatically - Modern SQL capabilities - Lower total cost for many use cases</p>"},{"location":"lectures/Lecture01_Intro/#16-mapreduce","title":"16. MapReduce","text":""},{"location":"lectures/Lecture01_Intro/#what-is-mapreduce","title":"What is MapReduce?","text":"<p>MapReduce is a programming model for processing large datasets across distributed clusters. It was popularized by Google and implemented in Hadoop.</p>"},{"location":"lectures/Lecture01_Intro/#the-two-phases","title":"The Two Phases","text":"<p>MAP Phase: Transform each record independently REDUCE Phase: Aggregate/combine the results</p>"},{"location":"lectures/Lecture01_Intro/#word-count-example-the-hello-world-of-mapreduce","title":"Word Count Example (The \"Hello World\" of MapReduce)","text":"<p>Problem: Count how many times each word appears in a large collection of documents.</p> <p>Input: Documents distributed across 3 machines <pre><code>Machine 1: \"hello world\"\nMachine 2: \"hello there\"\nMachine 3: \"world world\"\n</code></pre></p> <p>Step 1: MAP (Parallel) Each machine processes its data independently: <pre><code>Machine 1: \"hello world\" \u2192 (hello, 1), (world, 1)\nMachine 2: \"hello there\" \u2192 (hello, 1), (there, 1)\nMachine 3: \"world world\" \u2192 (world, 1), (world, 1)\n</code></pre></p> <p>Step 2: SHUFFLE (Automatic) Group all values by key: <pre><code>hello \u2192 [(hello, 1), (hello, 1)]\nworld \u2192 [(world, 1), (world, 1), (world, 1)]\nthere \u2192 [(there, 1)]\n</code></pre></p> <p>Step 3: REDUCE (Aggregate) Sum up values for each key: <pre><code>hello: 1 + 1 = 2\nworld: 1 + 1 + 1 = 3\nthere: 1\n</code></pre></p> <p>Output: <pre><code>hello: 2\nworld: 3\nthere: 1\n</code></pre></p>"},{"location":"lectures/Lecture01_Intro/#why-mapreduce-works-for-big-data","title":"Why MapReduce Works for Big Data","text":"<p>Key Properties:</p> <ol> <li>Parallel Processing: MAP phase runs simultaneously on all machines</li> <li>Data Locality: Process data where it's stored (don't move data, move computation)</li> <li>Fault Tolerance: If a task fails, only that task restarts</li> <li>Scalability: Add more machines to process more data</li> <li>Simplicity: Developer only writes MAP and REDUCE functions</li> </ol>"},{"location":"lectures/Lecture01_Intro/#mapreduce-code-example-python-like-pseudocode","title":"MapReduce Code Example (Python-like pseudocode)","text":"<pre><code>def map_function(document):\n    \"\"\"Called once per document\"\"\"\n    for word in document.split():\n        emit(word, 1)  # Output key-value pair\n\ndef reduce_function(word, counts):\n    \"\"\"Called once per unique word\"\"\"\n    total = sum(counts)\n    emit(word, total)\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#mapreduce-limitations","title":"MapReduce Limitations","text":"<p>The Disk Problem: MapReduce writes intermediate results to disk after each step.</p> <pre><code>MAP \u2192 Write to Disk \u2192 Read from Disk \u2192 REDUCE \u2192 Write to Disk\n</code></pre> <p>Why This Matters: - Disk I/O is slow (100x slower than memory) - For iterative algorithms (ML), this happens repeatedly - Training a model might require 100+ iterations - Each iteration reads/writes entire dataset to disk</p> <p>This is why Spark was created.</p>"},{"location":"lectures/Lecture01_Intro/#17-apache-spark","title":"17. Apache Spark","text":""},{"location":"lectures/Lecture01_Intro/#what-is-spark","title":"What is Spark?","text":"<p>Apache Spark is a unified analytics engine for large-scale data processing. It was created at UC Berkeley to address MapReduce's limitations.</p>"},{"location":"lectures/Lecture01_Intro/#why-spark-is-faster","title":"Why Spark is Faster","text":"<p>The Key Difference: In-Memory Processing</p> <pre><code>MapReduce:\nStep 1 \u2192 Disk \u2192 Step 2 \u2192 Disk \u2192 Step 3 \u2192 Disk\n\nSpark:\nStep 1 \u2192 Memory \u2192 Step 2 \u2192 Memory \u2192 Step 3\n</code></pre> <p>By keeping data in memory between steps, Spark is: - 10-100x faster than MapReduce for iterative algorithms - Especially beneficial for machine learning (many iterations)</p>"},{"location":"lectures/Lecture01_Intro/#core-concepts","title":"Core Concepts","text":""},{"location":"lectures/Lecture01_Intro/#rdd-resilient-distributed-dataset","title":"RDD (Resilient Distributed Dataset)","text":"<p>The fundamental data structure in Spark.</p> <p>Properties: - Resilient: Can recover from failures - Distributed: Spread across cluster - Dataset: Collection of elements</p> <p>Operations on RDDs:</p> <p>Transformations (Lazy): - <code>map()</code> - Apply function to each element - <code>filter()</code> - Keep elements matching condition - <code>flatMap()</code> - Map then flatten - <code>groupByKey()</code> - Group by key - <code>reduceByKey()</code> - Group and reduce by key - <code>join()</code> - Join two RDDs</p> <p>Actions (Trigger Execution): - <code>collect()</code> - Return all elements - <code>count()</code> - Count elements - <code>first()</code> - Return first element - <code>take(n)</code> - Return first n elements - <code>reduce()</code> - Aggregate all elements - <code>saveAsTextFile()</code> - Write to storage</p>"},{"location":"lectures/Lecture01_Intro/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Spark doesn't execute transformations immediately. It builds a DAG (Directed Acyclic Graph) of operations and executes only when an action is called.</p> <pre><code># These are transformations (lazy - nothing happens yet)\nrdd1 = rdd.filter(lambda x: x &gt; 0)\nrdd2 = rdd1.map(lambda x: x * 2)\n\n# This is an action (triggers execution of everything)\nresult = rdd2.collect()\n</code></pre> <p>Why Lazy? - Allows Spark to optimize the execution plan - Can combine operations for efficiency - Avoids unnecessary computation</p>"},{"location":"lectures/Lecture01_Intro/#dataframes","title":"DataFrames","text":"<p>Higher-level abstraction than RDDs, similar to Pandas DataFrames or SQL tables.</p> <p>Advantages over RDDs: - Schema aware: Knows column names and types - Catalyst optimizer: Automatically optimizes queries - Easier to use: SQL-like operations - Better performance: Optimized execution</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\n# Read data\ndf = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n\n# SQL-like operations\nresult = df \\\n    .filter(df.amount &gt; 100) \\\n    .groupBy(\"category\") \\\n    .agg({\"amount\": \"sum\", \"quantity\": \"avg\"})\n\n# Or use actual SQL\ndf.createTempView(\"sales\")\nresult = spark.sql(\"\"\"\n    SELECT category, SUM(amount) as total\n    FROM sales\n    WHERE amount &gt; 100\n    GROUP BY category\n\"\"\")\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#spark-ecosystem","title":"Spark Ecosystem","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502           SPARK CORE            \u2502\n                    \u2502    (RDD, Task Scheduling)       \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502               \u2502           \u2502           \u2502               \u2502\n        \u25bc               \u25bc           \u25bc           \u25bc               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Spark SQL   \u2502 \u2502  MLlib    \u2502 \u2502  GraphX   \u2502 \u2502 Streaming \u2502 \u2502 SparkR    \u2502\n\u2502               \u2502 \u2502           \u2502 \u2502           \u2502 \u2502           \u2502 \u2502           \u2502\n\u2502 Structured    \u2502 \u2502 Machine   \u2502 \u2502 Graph     \u2502 \u2502 Real-time \u2502 \u2502 R API     \u2502\n\u2502 Data &amp; SQL    \u2502 \u2502 Learning  \u2502 \u2502 Processing\u2502 \u2502 Streams   \u2502 \u2502           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Spark SQL: Query structured data with SQL or DataFrames MLlib: Scalable machine learning library GraphX: Graph processing and analytics Structured Streaming: Real-time data processing SparkR: R interface to Spark</p>"},{"location":"lectures/Lecture01_Intro/#pyspark-example","title":"PySpark Example","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, avg, sum\n\n# Initialize Spark\nspark = SparkSession.builder \\\n    .appName(\"SalesAnalysis\") \\\n    .getOrCreate()\n\n# Read data from distributed storage\ndf = spark.read.parquet(\"s3://data/sales/\")\n\n# Transformations (lazy)\nresult = df \\\n    .filter(col(\"year\") == 2024) \\\n    .filter(col(\"amount\") &gt; 0) \\\n    .groupBy(\"product_category\", \"region\") \\\n    .agg(\n        sum(\"amount\").alias(\"total_sales\"),\n        avg(\"amount\").alias(\"avg_order\"),\n        count(\"*\").alias(\"num_orders\")\n    ) \\\n    .orderBy(col(\"total_sales\").desc())\n\n# Action (triggers execution)\nresult.show(20)\n\n# Write results\nresult.write.parquet(\"s3://output/sales_summary/\")\n\n# Stop Spark\nspark.stop()\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#performance-tips-for-data-scientists","title":"Performance Tips for Data Scientists","text":"<ol> <li> <p>Cache/Persist: Reuse data across multiple operations    <pre><code>df.cache()  # Keep in memory\n# Now df can be used multiple times without recomputing\n</code></pre></p> </li> <li> <p>Minimize Shuffles: Avoid operations that move data between nodes</p> </li> <li><code>groupByKey()</code> is expensive (shuffles all data)</li> <li> <p><code>reduceByKey()</code> is better (aggregates locally first)</p> </li> <li> <p>Broadcast Small Tables: For joins with small tables    <pre><code>from pyspark.sql.functions import broadcast\nresult = big_df.join(broadcast(small_df), \"key\")\n</code></pre></p> </li> <li> <p>Right-Size Partitions: Aim for 128MB-1GB per partition    <pre><code>df.repartition(100)  # Increase parallelism\ndf.coalesce(10)      # Decrease partitions (no shuffle)\n</code></pre></p> </li> <li> <p>Use Parquet: Columnar format with compression    <pre><code>df.write.parquet(\"output/\")  # Much better than CSV\n</code></pre></p> </li> <li> <p>Filter Early: Reduce data before expensive operations    <pre><code># Good: filter before join\nfiltered = df.filter(col(\"year\") == 2024)\nresult = filtered.join(other_df)\n\n# Bad: join then filter\nresult = df.join(other_df).filter(col(\"year\") == 2024)\n</code></pre></p> </li> </ol>"},{"location":"lectures/Lecture01_Intro/#when-to-use-spark-vs-pandas","title":"When to Use Spark vs Pandas","text":"Criteria Use Pandas Use Spark Data size &lt; 1 GB &gt; 1 GB Single machine fits Yes No Interactive analysis Yes Limited Production pipelines Simple Complex, distributed Learning curve Lower Higher Iteration speed Faster Slower startup <p>Rule of Thumb: Start with Pandas. Switch to Spark when data doesn't fit in memory.</p>"},{"location":"lectures/Lecture01_Intro/#18-database-fundamentals","title":"18. Database Fundamentals","text":""},{"location":"lectures/Lecture01_Intro/#why-databases-matter-for-data-scientists","title":"Why Databases Matter for Data Scientists","text":"<p>In academia, you work with clean CSV files: <pre><code>import pandas as pd\ndf = pd.read_csv('data.csv')  # Magic! Data appears\n</code></pre></p> <p>In industry, data lives in databases: - PostgreSQL: User accounts - MySQL: Products and inventory - MongoDB: User profiles - Redis: Real-time features - Snowflake: Analytics</p> <p>Your job is to connect these pieces to extract insights.</p>"},{"location":"lectures/Lecture01_Intro/#the-database-universe","title":"The Database Universe","text":"<pre><code>ALL DATABASES\n    \u2502\n    \u251c\u2500\u2500 SQL (Relational)\n    \u2502   \u251c\u2500\u2500 OLTP (Transactional)\n    \u2502   \u2502   \u251c\u2500\u2500 PostgreSQL\n    \u2502   \u2502   \u251c\u2500\u2500 MySQL\n    \u2502   \u2502   \u251c\u2500\u2500 Oracle\n    \u2502   \u2502   \u2514\u2500\u2500 SQL Server\n    \u2502   \u2502\n    \u2502   \u2514\u2500\u2500 OLAP (Analytical)\n    \u2502       \u251c\u2500\u2500 Snowflake\n    \u2502       \u251c\u2500\u2500 BigQuery\n    \u2502       \u251c\u2500\u2500 Redshift\n    \u2502       \u2514\u2500\u2500 ClickHouse\n    \u2502\n    \u2514\u2500\u2500 NoSQL (Non-Relational)\n        \u251c\u2500\u2500 Document\n        \u2502   \u251c\u2500\u2500 MongoDB\n        \u2502   \u2514\u2500\u2500 CouchDB\n        \u2502\n        \u251c\u2500\u2500 Key-Value\n        \u2502   \u251c\u2500\u2500 Redis\n        \u2502   \u2514\u2500\u2500 Memcached\n        \u2502\n        \u251c\u2500\u2500 Column-Family\n        \u2502   \u251c\u2500\u2500 Cassandra\n        \u2502   \u2514\u2500\u2500 HBase\n        \u2502\n        \u2514\u2500\u2500 Graph\n            \u251c\u2500\u2500 Neo4j\n            \u2514\u2500\u2500 Neptune\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#essential-skills-for-data-scientists","title":"Essential Skills for Data Scientists","text":"<p>CRITICAL (Can't work without): - SQL queries: SELECT, JOIN, WHERE, GROUP BY, ORDER BY - Extracting data to Python/R (pandas, SQLAlchemy) - Understanding table relationships - Basic performance awareness</p> <p>IMPORTANT (Makes you effective): - Knowing when to use SQL vs NoSQL - Data warehouse concepts - Query optimization basics - Joining data from multiple sources</p> <p>HELPFUL (Makes you valuable): - NoSQL fundamentals - Caching strategies - Database design principles - ETL/ELT concepts</p>"},{"location":"lectures/Lecture01_Intro/#19-oltp-vs-olap","title":"19. OLTP vs OLAP","text":""},{"location":"lectures/Lecture01_Intro/#oltp-online-transaction-processing","title":"OLTP: Online Transaction Processing","text":"<p>Purpose: Handle day-to-day business operations</p> <p>Think of it as: The \"operations\" database</p> <p>Characteristics: - Optimized for WRITES (inserts, updates, deletes) - Handles single records or small batches - Thousands of concurrent users - Real-time transaction processing - Row-oriented storage (easy to update whole records) - Strong consistency (ACID properties)</p> <p>Examples of OLTP Operations: - Adding items to a shopping cart - Updating a user's profile - Processing a payment - Recording a new order - Changing a password</p> <p>Common OLTP Systems: - PostgreSQL - MySQL - Oracle - SQL Server - MariaDB</p> <p>When you see: \"Update user's email\" or \"Add new order\" \u2192 That's OLTP</p>"},{"location":"lectures/Lecture01_Intro/#olap-online-analytical-processing","title":"OLAP: Online Analytical Processing","text":"<p>Purpose: Analyze historical data for insights</p> <p>Think of it as: The \"analytics\" database</p> <p>Characteristics: - Optimized for READS (complex queries across millions of rows) - Handles millions to billions of records - Fewer concurrent users (analysts, data scientists) - Historical and aggregated data - Column-oriented storage (fast aggregations) - Often eventual consistency</p> <p>Examples of OLAP Operations: - Calculate monthly revenue trends - Segment customers by behavior - Analyze product performance - Create forecasting models - Build executive dashboards</p> <p>Common OLAP Systems: - Snowflake - Google BigQuery - Amazon Redshift - ClickHouse - Apache Druid</p> <p>When you see: \"What's our revenue trend?\" or \"Segment customers\" \u2192 That's OLAP</p>"},{"location":"lectures/Lecture01_Intro/#storage-difference-row-vs-column","title":"Storage Difference: Row vs Column","text":"<p>Row Storage (OLTP): <pre><code>[ID:1][Name:Alice][Age:28][City:NYC]\n[ID:2][Name:Bob][Age:35][City:Boston]\n[ID:3][Name:Carol][Age:42][City:Chicago]\n</code></pre> - All columns for one row stored together - Fast to read/write entire records - Good for: \"Get all info about user #1\"</p> <p>Column Storage (OLAP): <pre><code>[ID:1][ID:2][ID:3]\n[Name:Alice][Name:Bob][Name:Carol]\n[Age:28][Age:35][Age:42]\n[City:NYC][City:Boston][City:Chicago]\n</code></pre> - All values for one column stored together - Fast to aggregate single columns - Good for: \"What's the average age?\"</p>"},{"location":"lectures/Lecture01_Intro/#comparison-table_1","title":"Comparison Table","text":"Aspect OLTP OLAP Purpose Transactions Analysis Operations INSERT, UPDATE, DELETE SELECT, aggregate Data Current, detailed Historical, summarized Users Many (thousands) Few (analysts) Queries Simple, fast Complex, slower Response time Milliseconds Seconds to minutes Storage Row-oriented Column-oriented Consistency Strong (ACID) Often eventual Examples PostgreSQL, MySQL Snowflake, BigQuery"},{"location":"lectures/Lecture01_Intro/#how-they-work-together","title":"How They Work Together","text":"<p>In most organizations, OLTP and OLAP systems work together:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  OLTP Systems   \u2502\n\u2502  (PostgreSQL)   \u2502\n\u2502                 \u2502\n\u2502 User actions    \u2502\n\u2502 Transactions    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502 ETL/ELT Pipeline\n         \u2502 (nightly or real-time)\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  OLAP Systems   \u2502\n\u2502  (Snowflake)    \u2502\n\u2502                 \u2502\n\u2502 Historical data \u2502\n\u2502 Analytics       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Data Science  \u2502\n\u2502   Analytics     \u2502\n\u2502   BI Dashboards \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#20-sql-vs-nosql","title":"20. SQL vs NoSQL","text":""},{"location":"lectures/Lecture01_Intro/#sql-relational-databases","title":"SQL (Relational Databases)","text":"<p>The traditional, structured approach</p> <p>Characteristics: - Fixed schema (defined in advance) - Tables with rows and columns - Relationships via foreign keys - SQL query language (standardized) - ACID guarantees - Primarily vertical scaling</p> <p>Strengths: - Complex queries with JOINs - Strong consistency - Data integrity enforcement - Mature, well-understood technology - Standard query language</p> <p>Weaknesses: - Schema changes require migrations - Vertical scaling limits - Not ideal for unstructured data - Can be complex for simple use cases</p> <p>Use SQL When: - Data is structured with clear relationships - You need complex queries with multiple JOINs - Transactions must be ACID compliant - Data integrity is critical (banking, healthcare) - You need ad-hoc querying capability</p>"},{"location":"lectures/Lecture01_Intro/#nosql-non-relational-databases","title":"NoSQL (Non-Relational Databases)","text":"<p>The flexible, modern approach</p> <p>Characteristics: - Flexible schema (can vary by record) - Various data models (document, key-value, etc.) - Query language varies by database - Often eventual consistency - Primarily horizontal scaling</p> <p>Strengths: - Schema flexibility - Horizontal scalability - High throughput for specific patterns - Good for unstructured data - Simple for simple use cases</p> <p>Weaknesses: - Limited/no JOINs - Eventual consistency can complicate logic - No standard query language - Less mature tooling in some cases - Each type requires learning new paradigms</p> <p>Use NoSQL When: - Schema is flexible or evolving - Need massive scale and throughput - Simple lookup patterns (no complex joins) - Specific use case matches NoSQL strength - Development speed is priority</p>"},{"location":"lectures/Lecture01_Intro/#comparison-table_2","title":"Comparison Table","text":"Factor SQL NoSQL Schema Fixed, predefined Flexible, dynamic Relationships Foreign keys, JOINs Embedded, denormalized Query language SQL (standard) Varies by database Consistency ACID (strong) BASE (eventual) Scaling Vertical (scale up) Horizontal (scale out) Transactions Full support Limited or none Use cases Complex queries, transactions Scale, flexibility, speed"},{"location":"lectures/Lecture01_Intro/#real-world-companies-use-both","title":"Real-World: Companies Use Both","text":"<p>Most companies use multiple database types for different needs:</p> <p>E-commerce Company: - PostgreSQL: Orders and payments (ACID needed) - Redis: Shopping cart, sessions (fast access) - MongoDB: Product catalog (flexible attributes) - Elasticsearch: Search functionality - Snowflake: Analytics and reporting</p> <p>Social Media Platform: - PostgreSQL: User accounts (transactions) - Cassandra: Posts and feeds (high write volume) - Redis: Real-time counters, caching - Neo4j: Friend relationships, recommendations - Spark: ML and batch analytics</p> <p>Key Principle: Use the right tool for each job.</p>"},{"location":"lectures/Lecture01_Intro/#21-relational-database-concepts","title":"21. Relational Database Concepts","text":""},{"location":"lectures/Lecture01_Intro/#tables-relations","title":"Tables (Relations)","text":"<p>A table is the fundamental structure in relational databases.</p> <pre><code>Table: USERS\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id  \u2502    name     \u2502 age \u2502   city   \u2502        email         \u2502\n\u2502   (PK)   \u2502             \u2502     \u2502          \u2502                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    1     \u2502 Alice Smith \u2502  28 \u2502 New York \u2502 alice@example.com    \u2502\n\u2502    2     \u2502 Bob Jones   \u2502  35 \u2502 Boston   \u2502 bob@example.com      \u2502\n\u2502    3     \u2502 Carol White \u2502  42 \u2502 Chicago  \u2502 carol@example.com    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Components: - Table: Collection of related data - Column (Attribute): Type of information, same data type for all values - Row (Tuple/Record): One complete entry - Cell: Individual data point - Primary Key (PK): Unique identifier for each row</p> <p>Rules: - Every column has a defined data type - Primary key must be unique and not NULL - Every row has the same columns - No partial rows allowed</p>"},{"location":"lectures/Lecture01_Intro/#keys","title":"Keys","text":""},{"location":"lectures/Lecture01_Intro/#primary-key-pk","title":"Primary Key (PK)","text":"<p>Uniquely identifies each row in a table.</p> <p>Rules: - Cannot be NULL - Must be unique (no duplicates) - Should never change - Usually auto-incrementing integers</p> <p>Example: <pre><code>CREATE TABLE users (\n    user_id INT PRIMARY KEY AUTO_INCREMENT,\n    name VARCHAR(100),\n    email VARCHAR(100)\n);\n</code></pre></p>"},{"location":"lectures/Lecture01_Intro/#foreign-key-fk","title":"Foreign Key (FK)","text":"<p>References the primary key of another table, creating relationships.</p> <p>Rules: - Can have duplicates (many orders for one user) - Can be NULL (optional relationship) - Must match an existing PK (referential integrity)</p> <p>Example: <pre><code>CREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    user_id INT,\n    amount DECIMAL(10,2),\n    FOREIGN KEY (user_id) REFERENCES users(user_id)\n);\n</code></pre></p>"},{"location":"lectures/Lecture01_Intro/#relationships","title":"Relationships","text":""},{"location":"lectures/Lecture01_Intro/#one-to-many-most-common-80-of-relationships","title":"One-to-Many (Most Common - 80% of relationships)","text":"<p>One record in table A relates to many records in table B.</p> <pre><code>USERS                           ORDERS\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id=1 \u2502 \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 order_id=101 \u2502\n\u2502 Alice     \u2502                  \u2502 user_id=1    \u2502\n\u2502           \u2502 \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 order_id=102 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502 user_id=1    \u2502\n                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Examples: - One customer \u2192 Many orders - One author \u2192 Many books - One department \u2192 Many employees</p>"},{"location":"lectures/Lecture01_Intro/#many-to-many-requires-junction-table","title":"Many-to-Many (Requires Junction Table)","text":"<p>Many records in A relate to many records in B.</p> <pre><code>STUDENTS          ENROLLMENTS           COURSES\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Alice   \u2502\u2190\u2500\u2500\u2500\u2500\u2500\u2502 student_id=1    \u2502\u2500\u2500\u2192\u2502 Math    \u2502\n\u2502         \u2502      \u2502 course_id=101   \u2502   \u2502         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502 student_id=1    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502 course_id=102   \u2502\u2500\u2500\u2192\u2502 Physics \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524   \u2502         \u2502\n\u2502 Bob     \u2502\u2190\u2500\u2500\u2500\u2500\u2500\u2502 student_id=2    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502         \u2502      \u2502 course_id=101   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Examples: - Students \u2194 Courses - Products \u2194 Tags - Actors \u2194 Movies</p>"},{"location":"lectures/Lecture01_Intro/#one-to-one-rare-5-of-relationships","title":"One-to-One (Rare - 5% of relationships)","text":"<p>One record in A relates to exactly one record in B.</p> <p>Used for: - Separating frequently vs rarely accessed data - Security (sensitive data in separate table) - Performance optimization</p>"},{"location":"lectures/Lecture01_Intro/#normalization","title":"Normalization","text":"<p>Normalization organizes data to reduce redundancy and improve integrity.</p>"},{"location":"lectures/Lecture01_Intro/#the-problem-denormalized-data","title":"The Problem: Denormalized Data","text":"<pre><code>ORDERS (Bad Design)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502order_id \u2502 user_name   \u2502 user_email    \u2502 product  \u2502 amount  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  101    \u2502 Alice Smith \u2502 alice@ex.com  \u2502 Laptop   \u2502  1200   \u2502\n\u2502  102    \u2502 Alice Smith \u2502 alice@ex.com  \u2502 Mouse    \u2502   25    \u2502\n\u2502  103    \u2502 Bob Jones   \u2502 bob@ex.com    \u2502 Laptop   \u2502  1200   \u2502\n\u2502  104    \u2502 Alice Smith \u2502 alice@ex.com  \u2502 Phone    \u2502  800    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Problems: - Alice's info repeated 3 times (storage waste) - Changing Alice's email requires updating 3 rows (update anomaly) - Typo risk (\"Alise\" instead of \"Alice\") - Delete Bob's order and lose all Bob's info (delete anomaly)</p>"},{"location":"lectures/Lecture01_Intro/#the-solution-normalized-data","title":"The Solution: Normalized Data","text":"<p>USERS Table: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id  \u2502    name     \u2502    email      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    1     \u2502 Alice Smith \u2502 alice@ex.com  \u2502\n\u2502    2     \u2502 Bob Jones   \u2502 bob@ex.com    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>PRODUCTS Table: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 product_id \u2502  name   \u2502 price  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502    501     \u2502 Laptop  \u2502  1200  \u2502\n\u2502    502     \u2502 Mouse   \u2502   25   \u2502\n\u2502    503     \u2502 Phone   \u2502   800  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>ORDERS Table: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 order_id \u2502 user_id  \u2502 product_id \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   101    \u2502    1     \u2502    501     \u2502\n\u2502   102    \u2502    1     \u2502    502     \u2502\n\u2502   103    \u2502    2     \u2502    501     \u2502\n\u2502   104    \u2502    1     \u2502    503     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Benefits: - Each fact stored once - Update email in one place - No inconsistency possible - Can delete orders without losing user - Less storage used</p>"},{"location":"lectures/Lecture01_Intro/#22-essential-sql-for-data-scientists","title":"22. Essential SQL for Data Scientists","text":""},{"location":"lectures/Lecture01_Intro/#the-queries-youll-use-daily","title":"The Queries You'll Use Daily","text":""},{"location":"lectures/Lecture01_Intro/#select-reading-data","title":"SELECT: Reading Data","text":"<pre><code>-- Basic select\nSELECT * FROM users;\n\n-- Select specific columns\nSELECT name, email FROM users;\n\n-- With conditions\nSELECT name, email \nFROM users \nWHERE city = 'New York';\n\n-- Multiple conditions\nSELECT name, email \nFROM users \nWHERE city = 'New York' AND age &gt; 25;\n\n-- Pattern matching\nSELECT * FROM users WHERE email LIKE '%@gmail.com';\n\n-- Sorting\nSELECT * FROM users ORDER BY age DESC;\n\n-- Limiting results\nSELECT * FROM users ORDER BY created_at DESC LIMIT 10;\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#aggregations","title":"Aggregations","text":"<pre><code>-- Count rows\nSELECT COUNT(*) FROM orders;\n\n-- Count with condition\nSELECT COUNT(*) FROM orders WHERE amount &gt; 100;\n\n-- Sum\nSELECT SUM(amount) FROM orders;\n\n-- Average\nSELECT AVG(amount) FROM orders;\n\n-- Min and Max\nSELECT MIN(amount), MAX(amount) FROM orders;\n\n-- Group By\nSELECT \n    city,\n    COUNT(*) as user_count,\n    AVG(age) as avg_age\nFROM users\nGROUP BY city;\n\n-- Having (filter groups)\nSELECT \n    city,\n    COUNT(*) as user_count\nFROM users\nGROUP BY city\nHAVING COUNT(*) &gt; 100;\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#joins","title":"JOINs","text":"<pre><code>-- INNER JOIN (only matching rows)\nSELECT \n    u.name,\n    o.order_id,\n    o.amount\nFROM users u\nINNER JOIN orders o ON u.user_id = o.user_id;\n\n-- LEFT JOIN (all from left + matches from right)\nSELECT \n    u.name,\n    COUNT(o.order_id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.user_id = o.user_id\nGROUP BY u.user_id, u.name;\n\n-- Multiple JOINs\nSELECT \n    u.name,\n    p.product_name,\n    o.amount\nFROM orders o\nJOIN users u ON o.user_id = u.user_id\nJOIN products p ON o.product_id = p.product_id;\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#subqueries","title":"Subqueries","text":"<pre><code>-- Subquery in WHERE\nSELECT * FROM users\nWHERE user_id IN (\n    SELECT user_id FROM orders WHERE amount &gt; 1000\n);\n\n-- Subquery as table\nSELECT \n    category,\n    avg_price\nFROM (\n    SELECT \n        category,\n        AVG(price) as avg_price\n    FROM products\n    GROUP BY category\n) subquery\nWHERE avg_price &gt; 50;\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#window-functions","title":"Window Functions","text":"<pre><code>-- Rank within groups\nSELECT \n    name,\n    department,\n    salary,\n    RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank\nFROM employees;\n\n-- Running total\nSELECT \n    date,\n    amount,\n    SUM(amount) OVER (ORDER BY date) as running_total\nFROM daily_sales;\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#connecting-sql-to-python","title":"Connecting SQL to Python","text":"<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\n\n# Create connection\nengine = create_engine('postgresql://user:password@host:5432/dbname')\n\n# Read SQL query into DataFrame\ndf = pd.read_sql_query(\"\"\"\n    SELECT \n        u.name,\n        COUNT(o.order_id) as orders,\n        SUM(o.amount) as total_spent\n    FROM users u\n    LEFT JOIN orders o ON u.user_id = o.user_id\n    GROUP BY u.user_id, u.name\n    ORDER BY total_spent DESC\n\"\"\", engine)\n\n# Now work with df in Pandas\nprint(df.head())\n</code></pre>"},{"location":"lectures/Lecture01_Intro/#23-nosql-database-types","title":"23. NoSQL Database Types","text":""},{"location":"lectures/Lecture01_Intro/#document-databases","title":"Document Databases","text":"<p>Examples: MongoDB, CouchDB, DynamoDB</p> <p>Concept: Store data as JSON-like documents. Each document can have different fields.</p> <p>Example Document: <pre><code>{\n  \"_id\": \"user123\",\n  \"name\": \"Alice Smith\",\n  \"email\": \"alice@example.com\",\n  \"age\": 28,\n  \"address\": {\n    \"street\": \"123 Main St\",\n    \"city\": \"New York\"\n  },\n  \"orders\": [\n    {\"item\": \"laptop\", \"price\": 1200},\n    {\"item\": \"mouse\", \"price\": 25}\n  ]\n}\n</code></pre></p> <p>Characteristics: - Flexible schema (add fields anytime) - Nested data natural (objects within objects) - Horizontal scaling - Weak joins (query within documents)</p> <p>Use Cases: - User profiles (varying attributes) - Content management - Product catalogs - Mobile app backends</p> <p>MongoDB Example: <pre><code>// Insert\ndb.users.insertOne({name: \"Alice\", email: \"alice@ex.com\"});\n\n// Query\ndb.users.find({age: {$gt: 25}});\n\n// Update\ndb.users.updateOne(\n  {name: \"Alice\"},\n  {$set: {premium: true}}\n);\n</code></pre></p>"},{"location":"lectures/Lecture01_Intro/#key-value-databases","title":"Key-Value Databases","text":"<p>Examples: Redis, Memcached, etcd</p> <p>Concept: Simplest model\u2014just key \u2192 value pairs. Extremely fast.</p> <p>How It Works: <pre><code>KEY              \u2192  VALUE\nuser:123         \u2192  {\"name\": \"Alice\", \"age\": 28}\nsession:abc      \u2192  {\"user_id\": 123, \"expires\": 1640000000}\ncounter:visits   \u2192  42567\n</code></pre></p> <p>Characteristics: - In-memory (microsecond latency) - Simple: just GET and SET - No queries by value (only by key) - High throughput (millions ops/sec)</p> <p>Use Cases: - Session storage - Caching - Real-time counters - Rate limiting - Leaderboards</p> <p>Redis Example: <pre><code>import redis\nr = redis.Redis()\n\n# Set value\nr.set('user:123', '{\"name\":\"Alice\"}')\n\n# Get value\nuser = r.get('user:123')\n\n# Increment counter\nr.incr('visits:today')\n\n# Set with expiration (1 hour)\nr.setex('session:abc', 3600, 'session_data')\n</code></pre></p>"},{"location":"lectures/Lecture01_Intro/#column-family-databases","title":"Column-Family Databases","text":"<p>Examples: Cassandra, HBase, ScyllaDB</p> <p>Concept: Store data in column families instead of rows. Optimized for massive scale and high writes.</p> <p>Structure: <pre><code>Row Key: user123\n\u251c\u2500\u2500 Column Family: profile\n\u2502   \u251c\u2500\u2500 name: \"Alice\"\n\u2502   \u251c\u2500\u2500 email: \"alice@example.com\"\n\u2502   \u2514\u2500\u2500 age: 28\n\u2502\n\u2514\u2500\u2500 Column Family: activity\n    \u251c\u2500\u2500 last_login: \"2024-01-15\"\n    \u2514\u2500\u2500 posts_count: 47\n</code></pre></p> <p>Characteristics: - Petabyte scale (Netflix uses Cassandra) - High write throughput - No single point of failure - Tunable consistency - Complex data modeling required</p> <p>Use Cases: - Time-series data (IoT, metrics) - Event logging - Messaging apps - Recommendation systems</p>"},{"location":"lectures/Lecture01_Intro/#graph-databases","title":"Graph Databases","text":"<p>Examples: Neo4j, Amazon Neptune, ArangoDB</p> <p>Concept: Store data as nodes (entities) and edges (relationships). Perfect for connected data.</p> <p>Visual: <pre><code>    (Person: Alice)\n         \u2502\n         \u2502 KNOWS\n         \u25bc\n    (Person: Bob)\n         \u2502\n         \u2502 LIKES\n         \u25bc\n    (Product: Laptop)\n</code></pre></p> <p>Characteristics: - Natural for relationships - Fast traversals (\"friends of friends\") - Flexible schema - Pattern matching</p> <p>Use Cases: - Social networks - Fraud detection - Recommendation engines - Knowledge graphs</p> <p>Neo4j (Cypher) Example: <pre><code>// Find friends of friends\nMATCH (me:Person {name: 'Alice'})-[:KNOWS*2]-&gt;(friend)\nRETURN friend.name;\n\n// Recommend products friends like\nMATCH (me:Person {name: 'Alice'})-[:KNOWS]-&gt;(friend)-[:LIKES]-&gt;(product)\nRETURN product.name, COUNT(friend) as friend_count\nORDER BY friend_count DESC;\n</code></pre></p>"},{"location":"lectures/Lecture01_Intro/#nosql-selection-guide","title":"NoSQL Selection Guide","text":"Need Database Type Example Flexible documents Document MongoDB Ultra-fast cache Key-Value Redis Massive write scale Column-Family Cassandra Connected data Graph Neo4j Full-text search Search engine Elasticsearch"},{"location":"lectures/Lecture01_Intro/#24-key-takeaways","title":"24. Key Takeaways","text":""},{"location":"lectures/Lecture01_Intro/#for-data-science-career","title":"For Data Science Career","text":"<ol> <li>Data Science = Modeling + Computing + Domain</li> <li>Different emphasis creates different roles</li> <li> <p>Data Scientists need breadth across all three</p> </li> <li> <p>Understand the Data Ecosystem</p> </li> <li>Know where data comes from (collection methods)</li> <li>Understand data lineage and quality impacts</li> <li> <p>Different tools for different purposes</p> </li> <li> <p>Master the Fundamentals</p> </li> <li>SQL is essential (SELECT, JOIN, GROUP BY)</li> <li>Python/R for analysis and modeling</li> <li>Statistics for making valid conclusions</li> </ol>"},{"location":"lectures/Lecture01_Intro/#for-big-data","title":"For Big Data","text":"<ol> <li>Know When to Scale</li> <li>Pandas for &lt; 1 GB</li> <li>Spark for &gt; 1 GB or distributed</li> <li> <p>Don't use big data tools for small data</p> </li> <li> <p>Understand the 3 V's</p> </li> <li>Volume: How much data</li> <li>Velocity: How fast it arrives</li> <li> <p>Variety: What types of data</p> </li> <li> <p>MapReduce and Spark</p> </li> <li>MapReduce: Map \u2192 Shuffle \u2192 Reduce</li> <li>Spark: In-memory, 10-100x faster</li> <li>Use DataFrames over RDDs</li> </ol>"},{"location":"lectures/Lecture01_Intro/#for-databases","title":"For Databases","text":"<ol> <li>SQL vs NoSQL</li> <li>SQL: Structured data, complex queries, transactions</li> <li>NoSQL: Flexibility, scale, specific patterns</li> <li> <p>Most companies use BOTH</p> </li> <li> <p>OLTP vs OLAP</p> </li> <li>OLTP: Transactions (write-heavy)</li> <li>OLAP: Analytics (read-heavy)</li> <li> <p>Data flows from OLTP \u2192 OLAP</p> </li> <li> <p>Choose the Right Tool</p> </li> <li>Don't try to make one database do everything</li> <li>Match the tool to the use case</li> <li>Understand trade-offs</li> </ol>"},{"location":"lectures/Lecture01_Intro/#for-success","title":"For Success","text":"<ol> <li> <p>Start Simple, Scale When Needed</p> <ul> <li>Begin with the simplest solution that works</li> <li>Add complexity only when required</li> <li>Premature optimization wastes time</li> </ul> </li> <li> <p>Communication is Key</p> <ul> <li>Technical skills get you in the door</li> <li>Communication creates impact</li> <li>Learn to tell stories with data</li> </ul> </li> <li> <p>Keep Learning</p> <ul> <li>The field evolves rapidly</li> <li>Stay curious and adaptable</li> <li>Build projects to apply knowledge</li> </ul> </li> </ol>"},{"location":"lectures/Lecture01_Intro/#conclusion","title":"Conclusion","text":"<p>Data Science is a vast field that combines statistics, programming, and domain expertise to extract value from data. This guide has covered:</p> <ul> <li>Roles and Skills: Understanding the data ecosystem and where you fit</li> <li>Data Fundamentals: Types, collection, events, and quality</li> <li>Big Data: When and how to scale beyond single machines</li> <li>Processing: MapReduce, Spark, and modern architectures</li> <li>Databases: SQL, NoSQL, and choosing the right tool</li> </ul> <p>The journey from data to insight involves many steps and many tools. The best data scientists aren't just technically skilled\u2014they understand the full picture and can navigate the entire landscape.</p> <p>Remember: Start with the fundamentals, build projects, and never stop learning. The field will continue to evolve, but strong foundations will always matter.</p>"},{"location":"lectures/Lecture01_Intro/#resources-for-further-learning","title":"Resources for Further Learning","text":""},{"location":"lectures/Lecture01_Intro/#online-courses","title":"Online Courses","text":"<ul> <li>Coursera: Data Science Specialization (Johns Hopkins)</li> <li>fast.ai: Practical Deep Learning</li> <li>DataCamp: Interactive Python/SQL courses</li> </ul>"},{"location":"lectures/Lecture01_Intro/#books","title":"Books","text":"<ul> <li>\"Python for Data Analysis\" by Wes McKinney</li> <li>\"Designing Data-Intensive Applications\" by Martin Kleppmann</li> <li>\"The Elements of Statistical Learning\" by Hastie, Tibshirani, Friedman</li> </ul>"},{"location":"lectures/Lecture01_Intro/#practice-platforms","title":"Practice Platforms","text":"<ul> <li>Kaggle: Competitions and datasets</li> <li>LeetCode: SQL practice</li> <li>HackerRank: Coding challenges</li> </ul>"},{"location":"lectures/Lecture01_Intro/#documentation","title":"Documentation","text":"<ul> <li>Pandas: pandas.pydata.org</li> <li>Spark: spark.apache.org</li> <li>PostgreSQL: postgresql.org/docs</li> </ul> <p>This guide was created as a comprehensive reference for the Introduction to Data Science course. It covers topics from basic concepts to advanced technologies, providing both theoretical understanding and practical guidance.</p>"},{"location":"lectures/course_outline/","title":"Data Science Applications Course","text":"<p>Duration: 4 months</p>"},{"location":"lectures/course_outline/#module-1-data-science-fundamentals-and-exploratory-data-analysis-eda","title":"Module 1: Data Science Fundamentals and Exploratory Data Analysis (EDA)","text":""},{"location":"lectures/course_outline/#11-introduction-to-data-science-tools-and-environments","title":"1.1 Introduction to Data Science Tools and Environments","text":"<p>Key Topics: - Overview of Data Science Roles and Tools</p>"},{"location":"lectures/course_outline/#12-big-data-technologies-and-data-engineering","title":"1.2 Big Data Technologies and Data Engineering","text":"<p>Key Topics: - Introduction to Big Data Ecosystem (Hadoop, Spark) - Intro to Databases - Introduction to SQL for Data Retrieval and Manipulation - Introduction to NoSQL Databases (MongoDB) - Data Processing with Apache Spark (PySpark) - Building ETL Pipelines for Big Data - Practical Sessions: Working with Big Data Tools and Techniques</p>"},{"location":"lectures/course_outline/#13-full-etl-and-eda-on-real-world-datasets","title":"1.3 Full ETL and EDA on Real-World Datasets","text":""},{"location":"lectures/course_outline/#module-2-advanced-statistical-methods-and-machine-learning-for-data-science","title":"Module 2: Advanced Statistical Methods and Machine Learning for Data Science","text":""},{"location":"lectures/course_outline/#21-advanced-statistical-methods","title":"2.1 Advanced Statistical Methods","text":"<p>Key Topics: - Hypothesis Testing (T-tests, Chi-Square Tests) - Analysis of Variance (ANOVA) - Regression Analysis (Linear, Logistic, and Polynomial Regression) - Time Series Analysis and Forecasting - Practice: Applying Advanced Statistical Methods</p>"},{"location":"lectures/course_outline/#22-machine-learning-algorithms-for-data-science","title":"2.2 Machine Learning Algorithms for Data Science","text":"<p>Key Topics: - Supervised Learning Algorithms (Decision Trees, Random Forests, Gradient Boosting Machines) - Unsupervised Learning Algorithms (K-Means, Hierarchical Clustering, DBSCAN) - Introduction to Ensemble Methods - Model Evaluation and Validation Techniques (Cross-Validation, ROC-AUC, Precision-Recall) - Practice: Implementing ML Algorithms on Real Datasets</p>"},{"location":"lectures/course_outline/#module-3-deep-learning-for-data-science","title":"Module 3: Deep Learning for Data Science","text":""},{"location":"lectures/course_outline/#31-introduction-to-deep-learning-and-specialized-applications","title":"3.1 Introduction to Deep Learning and Specialized Applications","text":"<p>Key Topics: - Fundamentals of Neural Networks and Deep Learning - Building and Training Deep Learning Models (Using TensorFlow/Keras, PyTorch) - Convolutional Neural Networks (CNNs) for Image Data - Recurrent Neural Networks (RNNs) for Time-Series Data - Natural Language Processing (NLP) for Text Data - Advanced Time Series Analysis and Forecasting Techniques - Practical Sessions: Developing and Tuning Deep Learning Models</p>"},{"location":"lectures/course_outline/#module-4-capstone-project","title":"Module 4: Capstone Project","text":""},{"location":"lectures/course_outline/#41-capstone-project-preparation-execution-and-implementation","title":"4.1 Capstone Project Preparation, Execution, and Implementation","text":"<p>Key Topics: 1. Project Planning and Design 2. Data Collection and Preprocessing Strategies 3. Model Selection and Evaluation Plan 4. Proposal Presentation and Feedback 5. Building and Refining the Data Science Solution 6. Model Training, Tuning, and Evaluation 7. Deployment of the Model (on Cloud or On-Premise) 8. Preparing Final Presentation and Documentation</p>"},{"location":"lectures/course_outline/#42-capstone-project-completion-presentation-and-review","title":"4.2 Capstone Project Completion, Presentation, and Review","text":"<p>Key Topics: 1. Project Presentation to Peers and Instructors 2. Feedback and Q&amp;A Sessions 3. Final Grading and Evaluation 4. Course Wrap-Up and Future Directions in Data Science</p>"},{"location":"lectures/course_outline/#course-overview-summary","title":"Course Overview Summary","text":"Module Focus Area Duration Module 1 Data Science Fundamentals &amp; EDA ~4-6 weeks Module 2 Advanced Statistics &amp; Machine Learning ~6-8 weeks Module 3 Deep Learning ~4-6 weeks Module 4 Capstone Project 4 weeks Total Complete Course 4 months"},{"location":"lectures/course_outline/#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this course, students will be able to:</p> <p>\u2705 Work with big data technologies (Hadoop, Spark, PySpark) \u2705 Design and implement ETL pipelines \u2705 Perform comprehensive exploratory data analysis \u2705 Apply advanced statistical methods and hypothesis testing \u2705 Build and evaluate machine learning models \u2705 Develop deep learning solutions for various data types \u2705 Deploy data science solutions to production \u2705 Complete an end-to-end data science project from conception to deployment</p>"},{"location":"lectures/course_outline/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic Python programming knowledge</li> <li>Fundamental statistics and mathematics</li> <li>Understanding of basic data structures and algorithms</li> <li>Familiarity with Jupyter notebooks (recommended)</li> </ul>"},{"location":"lectures/course_outline/#tools-and-technologies-covered","title":"Tools and Technologies Covered","text":""},{"location":"lectures/course_outline/#programming-languages","title":"Programming Languages","text":"<ul> <li>Python (primary)</li> <li>SQL</li> </ul>"},{"location":"lectures/course_outline/#big-data-technologies","title":"Big Data Technologies","text":"<ul> <li>Apache Hadoop</li> <li>Apache Spark</li> <li>PySpark</li> </ul>"},{"location":"lectures/course_outline/#databases","title":"Databases","text":"<ul> <li>SQL databases</li> <li>MongoDB (NoSQL)</li> </ul>"},{"location":"lectures/course_outline/#machine-learning-frameworks","title":"Machine Learning Frameworks","text":"<ul> <li>Scikit-learn</li> <li>TensorFlow</li> <li>Keras</li> <li>PyTorch</li> </ul>"},{"location":"lectures/course_outline/#development-tools","title":"Development Tools","text":"<ul> <li>Jupyter Notebooks</li> <li>Git/GitHub</li> <li>Cloud platforms (AWS/Azure/GCP)</li> </ul>"},{"location":"lectures/course_outline/#assessment-methods","title":"Assessment Methods","text":"<ol> <li>Hands-on Assignments - Practical exercises throughout each module</li> <li>Project Work - Real-world dataset analysis and model building</li> <li>Capstone Project - Comprehensive end-to-end data science project</li> <li>Presentations - Project proposals and final presentations</li> <li>Peer Review - Collaborative learning and feedback sessions</li> </ol>"}]}