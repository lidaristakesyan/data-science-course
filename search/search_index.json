{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Data Science Course","text":"<p>Welcome to the Data Science Course \ud83c\udf31</p> <p>This site contains:</p> <ul> <li>\ud83d\udcda Lecture notes  </li> <li>\ud83d\udcbb Code &amp; notebooks  </li> <li>\ud83d\udcdd Homework &amp; exercises  </li> <li>\ud83d\udcca Slides &amp; visualizations  </li> </ul>"},{"location":"#structure","title":"Structure","text":"<ul> <li>Lectures \u2013 theory and examples  </li> <li>Notebooks \u2013 hands-on code in Jupyter  </li> <li>Homework \u2013 assignments and solutions  </li> <li>Slides \u2013 PDF / HTML slides for each week</li> </ul>"},{"location":"lectures/","title":"Lectures","text":"<p>Complete lecture notes for the Data Science course.</p>"},{"location":"lectures/#available-lectures","title":"\ud83d\udcc5 Available Lectures","text":"<ul> <li>Lecture 01: Environment Setup</li> </ul> <p>More lectures will be added throughout the semester.</p>"},{"location":"lectures/Lecture01_Environment_Setup/","title":"Lecture01 Environment Setup","text":""},{"location":"lectures/Lecture01_Environment_Setup/#setting-up-python-projects-with-venv-jupyter-vs-code-pycharm-conda-and-git","title":"\ud83d\udc0d Setting Up Python Projects with venv, Jupyter, VS Code, PyCharm, Conda, and Git","text":""},{"location":"lectures/Lecture01_Environment_Setup/#1-what-is-venv","title":"\ud83d\udcda 1. What is <code>venv</code>?","text":"<p><code>venv</code> stands for Virtual Environment.</p> <ul> <li>It creates an isolated Python environment.</li> <li>Each project gets its own libraries without affecting system-wide Python.</li> <li>Prevents version conflicts between different projects.</li> </ul> <p>Think of <code>venv</code> like a special box where only the things your project needs live.</p>"},{"location":"lectures/Lecture01_Environment_Setup/#2-creating-and-using-a-venv","title":"\u2699\ufe0f 2. Creating and Using a <code>venv</code>","text":"<p>Step 1: Create a Virtual Environment</p> <pre><code>python -m venv venv\n</code></pre> <ul> <li>This creates a folder <code>venv/</code> containing a copy of Python and installed libraries.</li> </ul> <p>Step 2: Activate the Environment</p> <ul> <li>On Mac/Linux: <pre><code>source venv/bin/activate\n</code></pre></li> <li>On Windows (CMD): <pre><code>venv\\Scripts\\activate\n</code></pre></li> <li>On Windows (Powershell): <pre><code>.\\venv\\Scripts\\Activate.ps1\n</code></pre></li> </ul> <p>You will see the terminal prompt change to something like: <pre><code>(venv) your-folder $\n</code></pre></p> <p>Now, you're working inside your project's environment!</p>"},{"location":"lectures/Lecture01_Environment_Setup/#3-installing-packages-and-requirementstxt","title":"\ud83d\udce6 3. Installing Packages and <code>requirements.txt</code>","text":"<p>Inside the active environment:</p> <p>Install libraries:</p> <pre><code>pip install numpy matplotlib pandas\n</code></pre> <p>Freeze current environment into <code>requirements.txt</code>:</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>This file lists all installed libraries and versions.</p> <p>Later, to recreate the environment elsewhere:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"lectures/Lecture01_Environment_Setup/#4-working-with-jupyter-notebook","title":"\ud83d\udcd3 4. Working with Jupyter Notebook","text":"<p>Step 1: Install Jupyter inside your <code>venv</code></p> <pre><code>pip install notebook ipykernel\n</code></pre> <p>Step 2: Add your <code>venv</code> as a Jupyter kernel</p> <pre><code>python -m ipykernel install --user --name=venv --display-name \"Python (venv)\"\n</code></pre> <ul> <li><code>--name</code> is the internal ID.</li> <li><code>--display-name</code> is what you will see in the Notebook.</li> </ul> <p>Step 3: Launch Jupyter</p> <pre><code>jupyter notebook\n</code></pre> <p>Then inside Jupyter, choose Kernel &gt; Change Kernel &gt; Python (venv).</p> <p>\u2705 Now Jupyter runs inside your virtual environment!</p>"},{"location":"lectures/Lecture01_Environment_Setup/#5-using-vs-code","title":"\ud83d\udda5 5. Using VS Code","text":"<p>Step 1: Install the Python extension for VS Code.</p> <p>Step 2: Open your project folder.</p> <p>Step 3: Select the interpreter: - Press <code>Ctrl+Shift+P</code> \u2192 search <code>Python: Select Interpreter</code> - Choose your <code>venv</code> from the list.</p> <p>VS Code will automatically use the selected environment for running and debugging.</p> <p>\u2705 Jupyter Notebooks in VS Code will also use your environment now!</p>"},{"location":"lectures/Lecture01_Environment_Setup/#6-using-pycharm","title":"\ud83e\udde0 6. Using PyCharm","text":"<p>Step 1: Open PyCharm \u2192 Open your project folder.</p> <p>Step 2: Go to: <pre><code>File \u2192 Settings \u2192 Project: your_project \u2192 Python Interpreter\n</code></pre></p> <p>Step 3: Click the gear icon \u2192 Add \u2192 Existing Environment</p> <ul> <li>Select <code>venv/bin/python</code> (Mac/Linux) or <code>venv\\Scripts\\python.exe</code> (Windows).</li> </ul> <p>\u2705 PyCharm now uses your project-specific environment.</p>"},{"location":"lectures/Lecture01_Environment_Setup/#7-using-conda-optional","title":"\ud83e\uddea 7. Using Conda (optional)","text":"<p>If you prefer <code>conda</code>, you can create a Conda environment instead of <code>venv</code>:</p> <pre><code>conda create -n myenv python=3.10\nconda activate myenv\n</code></pre> <p>Install packages as usual:</p> <pre><code>pip install numpy pandas matplotlib\n</code></pre> <p>You can export the environment to a file:</p> <pre><code>conda env export &gt; environment.yml\n</code></pre> <p>And recreate it elsewhere:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>\u26a1 Conda is more powerful when you need heavy scientific libraries (like TensorFlow, PyTorch) easily installed.</p>"},{"location":"lectures/Lecture01_Environment_Setup/#8-using-git-with-your-project","title":"\ud83d\udee0 8. Using Git with Your Project","text":"<p>Best practices:</p> <ul> <li>Don't push your venv folder to GitHub (it\u2019s huge and unnecessary).</li> <li>Create a <code>.gitignore</code> file and add:</li> </ul> <pre><code>venv/\n__pycache__/\n*.pyc\n.ipynb_checkpoints/\n</code></pre> <ul> <li>Always track your <code>requirements.txt</code> or <code>environment.yml</code>.</li> </ul> <p>Basic Git workflow:</p> <pre><code>git init\ngit add .\ngit commit -m \"Initial project setup\"\ngit remote add origin https://github.com/yourusername/yourrepo.git\ngit push -u origin main\n</code></pre> <p>\u2705 This way, others can clone your repo and install everything easily using your requirements!</p>"},{"location":"lectures/Lecture01_Environment_Setup/#quick-summary","title":"\ud83d\ude80 Quick Summary","text":"Tool Use Case <code>venv</code> Isolate Python packages <code>requirements.txt</code> Record/install packages easily Jupyter Notebook Run interactive Python notebooks VS Code Lightweight code editing &amp; notebooks PyCharm Full professional Python IDE Conda Manage heavy data science libraries Git Version control for your project"},{"location":"lectures/Lecture01_Environment_Setup/#final-tip","title":"\ud83d\udce2 Final Tip:","text":"<p>ALWAYS work inside a virtual environment (venv or conda). It keeps your machine clean and your projects professional!</p>"},{"location":"lectures/Lecture2/","title":"Lecture2","text":"<p>Data Science Tools and Environments</p> <p>Advanced Course for Data Science Students</p> <p>What is Data and How to Use It Optimally in Analysis, ML/DL Pipelines</p> <p>Duration: 7 Lectures</p> <p>Part I: Core Tools &amp; Data Processing (3 Lectures) | Part II: Big Data Ecosystem (4 Lectures)</p> <p>Course Overview</p> <p>This advanced course assumes prior familiarity with Python programming and basic data science concepts. The curriculum bridges foundational knowledge with expert-level application, focusing on production-ready techniques, performance optimization, and industry best practices. Students gain hands-on experience with professional workflows that scale from exploratory analysis to enterprise-grade data pipelines.</p> <p>Prerequisites</p> <ul> <li>Proficiency in Python programming (functions, classes, decorators, context managers)</li> <li>Basic understanding of NumPy arrays and Pandas DataFrames</li> <li>Familiarity with command-line interfaces and Git version control</li> <li>Foundational statistics knowledge (distributions, hypothesis testing, correlation)</li> </ul> <p>PART I: Core Tools, Environments &amp; Data Processing</p> <p>Lecture 1: Professional Development Environments &amp; Advanced Tool Configuration</p> <p>Learning Objectives</p> <ul> <li>Design and implement reproducible virtual environments using venv, conda, and containerization</li> <li>Configure JupyterLab for production-grade data science workflows with custom kernels</li> <li>Implement environment management best practices for ML/DL pipeline reproducibility</li> <li>Master dependency management, version pinning, and conflict resolution strategies</li> </ul> <p>1.1 Virtual Environment Architecture</p> <p>Virtual environments provide isolated Python installations that prevent dependency conflicts between projects. Understanding the underlying architecture is essential for troubleshooting and optimization. A virtual environment creates a self-contained directory structure containing a Python interpreter symlink, a site-packages directory for installed libraries, and activation scripts that modify PATH and PYTHONPATH environment variables.</p> <p>The venv Module: Native Python Isolation</p> <p>Python's built-in venv module creates lightweight virtual environments. The key insight is that venv does not copy the Python interpreter; instead, it creates symlinks (on Unix) or copies (on Windows) to the system Python, significantly reducing disk usage while maintaining isolation.</p>"},{"location":"lectures/Lecture2/#create-virtual-environment-with-system-site-packages-access","title":"Create virtual environment with system site-packages access","text":"<p>python -m venv --system-site-packages ./venv_with_system</p>"},{"location":"lectures/Lecture2/#create-completely-isolated-environment","title":"Create completely isolated environment","text":"<p>python -m venv --clear ./venv_isolated</p>"},{"location":"lectures/Lecture2/#upgrade-pip-immediately-after-creation","title":"Upgrade pip immediately after creation","text":"<p>source ./venv_isolated/bin/activate</p> <p>python -m pip install --upgrade pip setuptools wheel</p> <p>Conda Environments: Beyond Python</p> <p>Conda environments differ fundamentally from venv in that they can manage non-Python dependencies (C libraries, compilers, CUDA toolkits). This is crucial for data science where packages like NumPy, SciPy, and TensorFlow depend on optimized BLAS/LAPACK implementations.</p>"},{"location":"lectures/Lecture2/#create-environment-with-specific-python-and-dependencies","title":"Create environment with specific Python and dependencies","text":"<p>conda create -n ds_env python=3.11 numpy scipy pandas scikit-learn -c conda-forge</p>"},{"location":"lectures/Lecture2/#export-environment-for-reproducibility","title":"Export environment for reproducibility","text":"<p>conda env export --from-history &gt; environment.yml</p>"},{"location":"lectures/Lecture2/#create-exact-reproduction-from-lock-file","title":"Create exact reproduction from lock file","text":"<p>conda-lock install -n ds_env_locked conda-lock.yml</p> <p>1.2 Advanced JupyterLab Configuration</p> <p>JupyterLab serves as the primary interactive development environment for data scientists. Advanced configuration transforms it from a notebook viewer into a comprehensive IDE with debugging, profiling, and collaboration capabilities.</p> <p>Custom Kernel Management</p>"},{"location":"lectures/Lecture2/#register-virtual-environment-as-jupyter-kernel","title":"Register virtual environment as Jupyter kernel","text":"<p>python -m ipykernel install --user --name=ds_project --display-name=\"DS Project (Python 3.11)\"</p>"},{"location":"lectures/Lecture2/#list-available-kernels","title":"List available kernels","text":"<p>jupyter kernelspec list</p>"},{"location":"lectures/Lecture2/#remove-obsolete-kernel","title":"Remove obsolete kernel","text":"<p>jupyter kernelspec uninstall ds_old_project</p> <p>1.3 Data Types: Memory Representation and Optimization</p> <p>Understanding how Python and NumPy represent data in memory is fundamental to writing efficient data science code. Memory layout directly impacts cache utilization, vectorization possibilities, and overall computational performance.</p> <p>NumPy dtype System</p> <p>import numpy as np</p>"},{"location":"lectures/Lecture2/#examine-dtype-properties","title":"Examine dtype properties","text":"<p>dt = np.dtype('float32')</p> <p>print(f\"Itemsize: {dt.itemsize} bytes\")</p> <p>print(f\"Byte order: {dt.byteorder}\")</p>"},{"location":"lectures/Lecture2/#memory-efficient-integer-selection","title":"Memory-efficient integer selection","text":"<p>data = np.array([1, 2, 3, 100, 200], dtype=np.uint8)\u00a0 # 1 byte per element</p> <p>data_large = np.array([1, 2, 3, 100, 200], dtype=np.int64)\u00a0 # 8 bytes</p> <p>print(f\"uint8 memory: {data.nbytes} bytes\")</p> <p>print(f\"int64 memory: {data_large.nbytes} bytes\")</p> <p>Pandas Memory Optimization</p> <p>import pandas as pd</p> <p>def optimize_dataframe(df):</p> <p>\"\"\"Reduce DataFrame memory footprint.\"\"\"</p> <p>for col in df.columns:</p> <p>col_type = df[col].dtype</p> <p>if col_type == 'object':</p> <p>num_unique = df[col].nunique()</p> <p>if num_unique / len(df) &lt; 0.5:\u00a0 # Cardinality &lt; 50%</p> <p>df[col] = df[col].astype('category')</p> <p>elif col_type == 'float64':</p> <p>df[col] = pd.to_numeric(df[col], downcast='float')</p> <p>elif col_type == 'int64':</p> <p>df[col] = pd.to_numeric(df[col], downcast='integer')</p> <p>return df</p> <p>Recommended Resources for Lecture 1</p> Resource Type Coverage Python Data Science Handbook (VanderPlas) Book, Ch. 1-2 IPython, Jupyter, NumPy fundamentals Effective Python, 3rd Ed. (Slatkin) Book, Items 73-77 Virtual environments, dependencies conda.io Documentation Official Docs Environment management, conda-forge JupyterLab Documentation Official Docs Extension system, configuration NumPy User Guide: Data Types Official Docs dtype system, structured arrays <p>Lecture 2: Advanced NumPy, Pandas, and SciPy for High-Performance Computing</p> <p>Learning Objectives</p> <ul> <li>Master NumPy broadcasting, advanced indexing, and memory-efficient operations</li> <li>Implement high-performance Pandas operations using vectorization and method chaining</li> <li>Apply SciPy's sparse matrices and optimization routines for scientific computing</li> <li>Profile and optimize code bottlenecks in data processing pipelines</li> </ul> <p>2.1 NumPy: Beyond the Basics</p> <p>Broadcasting: The Key to Vectorized Operations</p> <p>Broadcasting is NumPy's mechanism for performing operations on arrays of different shapes. Understanding broadcasting rules eliminates the need for explicit loops and enables highly optimized SIMD operations. The rules are: (1) dimensions are compared from right to left, (2) dimensions are compatible if equal or one is 1, (3) missing dimensions are treated as 1.</p> <p>import numpy as np</p>"},{"location":"lectures/Lecture2/#broadcasting-example-normalize-features","title":"Broadcasting example: normalize features","text":"<p>data = np.random.randn(1000, 50)\u00a0 # 1000 samples, 50 features</p> <p>mean = data.mean(axis=0)\u00a0 # Shape: (50,)</p> <p>std = data.std(axis=0)\u00a0 \u00a0 # Shape: (50,)</p>"},{"location":"lectures/Lecture2/#broadcasting-1000-50-50-1000-50","title":"Broadcasting (1000, 50) - (50,) -&gt; (1000, 50)","text":"<p>normalized = (data - mean) / std</p>"},{"location":"lectures/Lecture2/#outer-product-via-broadcasting","title":"Outer product via broadcasting","text":"<p>a = np.arange(5)[:, np.newaxis]\u00a0 # Shape: (5, 1)</p> <p>b = np.arange(3) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Shape: (3,)</p> <p>outer = a * b\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Shape: (5, 3)</p> <p>Advanced Indexing Patterns</p>"},{"location":"lectures/Lecture2/#boolean-indexing-for-conditional-selection","title":"Boolean indexing for conditional selection","text":"<p>data = np.random.randn(1000, 10)</p> <p>mask = (data[:, 0] &gt; 0) &amp; (data[:, 1] &lt; 0)</p> <p>filtered = data[mask]\u00a0 # Returns copy</p>"},{"location":"lectures/Lecture2/#fancy-indexing-for-reordering","title":"Fancy indexing for reordering","text":"<p>indices = np.argsort(data[:, 0])[::-1]\u00a0 # Descending order</p> <p>sorted_data = data[indices]\u00a0 # Returns copy</p>"},{"location":"lectures/Lecture2/#npwhere-for-conditional-element-selection","title":"np.where for conditional element selection","text":"<p>result = np.where(data &gt; 0, data, 0)\u00a0 # ReLU operation</p> <p>2.2 Pandas: Production-Grade Data Manipulation</p> <p>Method Chaining for Readable Pipelines</p> <p>import pandas as pd</p> <p>import numpy as np</p> <p>def calculate_metrics(df):</p> <p>return df.assign(</p> <p>total=lambda x: x['quantity'] * x['price'],</p> <p>log_total=lambda x: np.log1p(x['total'])</p> <p>)</p> <p>result = (</p> <p>pd.read_csv('sales.csv')</p> <p>.query('date &gt;= \"2024-01-01\"')</p> <p>.pipe(calculate_metrics)</p> <p>.groupby('category', as_index=False)</p> <p>.agg({'total': ['sum', 'mean'], 'quantity': 'count'})</p> <p>.sort_values('sum', ascending=False)</p> <p>)</p> <p>GroupBy: Split-Apply-Combine Pattern</p>"},{"location":"lectures/Lecture2/#transform-broadcast-aggregation-back-to-original-index","title":"transform: broadcast aggregation back to original index","text":"<p>df['pct_of_category'] = (</p> <p>df.groupby('category')['sales']</p> <p>.transform(lambda x: x / x.sum() * 100)</p> <p>)</p>"},{"location":"lectures/Lecture2/#multiple-aggregations-with-named-columns","title":"Multiple aggregations with named columns","text":"<p>agg_result = df.groupby('category').agg(</p> <p>total_sales=('sales', 'sum'),</p> <p>avg_sales=('sales', 'mean'),</p> <p>num_products=('product_id', 'nunique')</p> <p>)</p> <p>2.3 SciPy: Scientific Computing Toolkit</p> <p>Sparse Matrices for Memory Efficiency</p> <p>from scipy import sparse</p> <p>import numpy as np</p>"},{"location":"lectures/Lecture2/#construct-sparse-matrix-incrementally","title":"Construct sparse matrix incrementally","text":"<p>row = np.array([0, 0, 1, 2, 2])</p> <p>col = np.array([0, 2, 1, 0, 2])</p> <p>data = np.array([1, 2, 3, 4, 5])</p> <p>coo = sparse.coo_matrix((data, (row, col)), shape=(3, 3))</p>"},{"location":"lectures/Lecture2/#convert-to-csr-for-efficient-arithmetic","title":"Convert to CSR for efficient arithmetic","text":"<p>csr = coo.tocsr()</p> <p>print(f\"Sparse memory: {csr.data.nbytes + csr.indices.nbytes + csr.indptr.nbytes} bytes\")</p> <p>Recommended Resources for Lecture 2</p> Resource Type Coverage Python Data Science Handbook (VanderPlas) Book, Ch. 2-3 NumPy, Pandas comprehensive NumPy Documentation: Broadcasting Official Guide Broadcasting rules, examples Pandas User Guide: GroupBy Official Docs Split-apply-combine patterns \"High Performance Python\" (Gorelick) Book, Ch. 4-6 NumPy internals, profiling Wes McKinney's Python for Data Analysis Book, 3rd Ed. Pandas creator's authoritative guide <p>Lecture 3: Data Processing, Visualization, and Feature Engineering</p> <p>Learning Objectives</p> <ul> <li>Implement robust data import/export pipelines supporting multiple formats</li> <li>Apply advanced missing data imputation techniques appropriate to data characteristics</li> <li>Design feature engineering pipelines with proper train/test separation</li> <li>Create publication-quality visualizations using Matplotlib and Seaborn</li> </ul> <p>3.1 Multi-Format Data Import/Export</p> <p>Efficient CSV Processing</p> <p>import pandas as pd</p>"},{"location":"lectures/Lecture2/#memory-efficient-csv-reading","title":"Memory-efficient CSV reading","text":"<p>chunk_iter = pd.read_csv(</p> <p>'large_file.csv',</p> <p>chunksize=100_000,</p> <p>dtype={'id': 'int32', 'category': 'category', 'value': 'float32'},</p> <p>usecols=['id', 'category', 'value', 'date'],</p> <p>parse_dates=['date'],</p> <p>na_values=['', 'NA', 'NULL', '-999']</p> <p>)</p>"},{"location":"lectures/Lecture2/#process-chunks-and-aggregate","title":"Process chunks and aggregate","text":"<p>results = []</p> <p>for chunk in chunk_iter:</p> <p>processed = chunk.groupby('category')['value'].sum()</p> <p>results.append(processed)</p> <p>final = pd.concat(results).groupby(level=0).sum()</p> <p>Parquet: The Columnar Format for Analytics</p> <p>import pandas as pd</p>"},{"location":"lectures/Lecture2/#write-with-compression-and-partitioning","title":"Write with compression and partitioning","text":"<p>df.to_parquet(</p> <p>'data/',</p> <p>engine='pyarrow',</p> <p>compression='snappy',</p> <p>partition_cols=['year', 'month'],</p> <p>index=False</p> <p>)</p>"},{"location":"lectures/Lecture2/#read-with-column-selection-and-filtering","title":"Read with column selection and filtering","text":"<p>df = pd.read_parquet(</p> <p>'data/',</p> <p>columns=['id', 'value', 'category'],</p> <p>filters=[('year', '&gt;=', 2023), ('category', 'in', ['A', 'B'])]</p> <p>)</p> <p>3.2 Missing Data: Theory and Practice</p> <p>Advanced Imputation Strategies</p> <p>from sklearn.impute import KNNImputer, IterativeImputer</p> <p>from sklearn.ensemble import RandomForestRegressor</p>"},{"location":"lectures/Lecture2/#knn-imputation-preserves-multivariate-relationships","title":"KNN Imputation - preserves multivariate relationships","text":"<p>knn_imputer = KNNImputer(n_neighbors=5, weights='distance')</p> <p>df_imputed = pd.DataFrame(</p> <p>knn_imputer.fit_transform(df),</p> <p>columns=df.columns,</p> <p>index=df.index</p> <p>)</p>"},{"location":"lectures/Lecture2/#iterative-imputation-with-random-forest","title":"Iterative imputation with Random Forest","text":"<p>iter_imputer = IterativeImputer(</p> <p>estimator=RandomForestRegressor(n_estimators=100, random_state=42),</p> <p>max_iter=10,</p> <p>random_state=42</p> <p>)</p> <p>3.3 Data Transformation Pipeline</p> <p>Scaling and Normalization</p> <p>from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer</p> <p>from sklearn.compose import ColumnTransformer</p>"},{"location":"lectures/Lecture2/#column-specific-transformations","title":"Column-specific transformations","text":"<p>preprocessor = ColumnTransformer([</p> <p>('num_standard', StandardScaler(), ['age', 'income']),</p> <p>('num_robust', RobustScaler(), ['transaction_amount']),</p> <p>('num_power', PowerTransformer(method='yeo-johnson'), ['skewed_feature']),</p> <p>], remainder='passthrough')</p>"},{"location":"lectures/Lecture2/#critical-fit-on-training-data-only-transform-both","title":"CRITICAL: fit on training data only, transform both","text":"<p>X_train_scaled = preprocessor.fit_transform(X_train)</p> <p>X_test_scaled = preprocessor.transform(X_test)\u00a0 # No fit!</p> <p>3.4 Outlier Detection and Handling</p> <p>from sklearn.ensemble import IsolationForest</p> <p>from sklearn.neighbors import LocalOutlierFactor</p>"},{"location":"lectures/Lecture2/#isolation-forest-efficient-for-high-dimensions","title":"Isolation Forest - efficient for high dimensions","text":"<p>iso_forest = IsolationForest(</p> <p>contamination=0.05,</p> <p>random_state=42,</p> <p>n_jobs=-1</p> <p>)</p> <p>outlier_labels = iso_forest.fit_predict(X)\u00a0 # -1 = outlier</p>"},{"location":"lectures/Lecture2/#local-outlier-factor-density-based","title":"Local Outlier Factor - density-based","text":"<p>lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)</p> <p>lof_labels = lof.fit_predict(X)</p> <p>3.5 Visualization for Analysis</p> <p>Seaborn for Statistical Visualization</p> <p>import seaborn as sns</p> <p>import matplotlib.pyplot as plt</p> <p>sns.set_theme(style='whitegrid', palette='deep', font_scale=1.1)</p>"},{"location":"lectures/Lecture2/#heatmap-for-correlation-matrix","title":"Heatmap for correlation matrix","text":"<p>fig, ax = plt.subplots(figsize=(10, 8))</p> <p>corr = df.corr()</p> <p>mask = np.triu(np.ones_like(corr, dtype=bool))</p> <p>sns.heatmap(</p> <p>corr, mask=mask, annot=True, fmt='.2f',</p> <p>cmap='RdBu_r', center=0, square=True, ax=ax</p> <p>)</p> <p>3.6 Feature Engineering</p> <p>from sklearn.feature_selection import SelectKBest, mutual_info_classif, SelectFromModel</p> <p>from sklearn.ensemble import RandomForestClassifier</p>"},{"location":"lectures/Lecture2/#mutual-information-captures-non-linear-relationships","title":"Mutual information (captures non-linear relationships)","text":"<p>mi_selector = SelectKBest(score_func=mutual_info_classif, k=20)</p> <p>X_mi = mi_selector.fit_transform(X, y)</p>"},{"location":"lectures/Lecture2/#tree-based-importance-with-threshold","title":"Tree-based importance with threshold","text":"<p>rf = RandomForestClassifier(n_estimators=100, random_state=42)</p> <p>model_selector = SelectFromModel(rf, threshold='median')</p> <p>X_model = model_selector.fit_transform(X, y)</p> <p>Recommended Resources for Lecture 3</p> Resource Type Coverage \"Feature Engineering for ML\" (Zheng) Book Comprehensive feature engineering Pandas I/O Documentation Official Docs All file format support Scikit-learn User Guide: Preprocessing Official Docs Transformers, pipelines \"Fundamentals of Data Visualization\" (Wilke) Book Principles of visualization Seaborn Tutorial Gallery Official Docs Statistical visualization patterns <p>PART II: Big Data Ecosystem &amp; Distributed Computing</p> <p>Lecture 4: Big Data Foundations - Hadoop Ecosystem and Distributed Computing</p> <p>Learning Objectives</p> <ul> <li>Understand theoretical foundations of distributed computing and MapReduce</li> <li>Explain HDFS architecture, data locality, and fault tolerance mechanisms</li> <li>Compare batch vs. stream processing paradigms and their use cases</li> <li>Evaluate when big data tools are necessary vs. single-machine optimization</li> </ul> <p>4.1 The Big Data Problem</p> <p>Big data is characterized by the \"Three Vs\": Volume (terabytes to petabytes), Velocity (real-time streaming), and Variety (structured, semi-structured, unstructured). Distributed computing addresses this by partitioning data and computation across clusters of commodity machines.</p> <p>When Do You Actually Need Big Data Tools?</p> <p>Modern single machines with 64-128GB RAM and NVMe storage can process datasets of 50-100GB efficiently using optimized libraries (Polars, DuckDB). The overhead of distributed systems only pays off beyond certain thresholds.</p>"},{"location":"lectures/Lecture2/#before-reaching-for-spark-try","title":"Before reaching for Spark, try:","text":"<p>import polars as pl</p> <p>df = pl.scan_csv('large_file.csv')\u00a0 # Lazy - doesn't load yet</p> <p>result = (</p> <p>df.filter(pl.col('date') &gt; '2024-01-01')</p> <p>.group_by('category')</p> <p>.agg(pl.col('value').sum())</p> <p>.collect()\u00a0 # Execute optimized query plan</p> <p>)</p>"},{"location":"lectures/Lecture2/#or-duckdb-in-process-analytical-database","title":"Or DuckDB - In-process analytical database","text":"<p>import duckdb</p> <p>result = duckdb.execute(\"\"\"</p> <p>SELECT category, SUM(value) as total</p> <p>FROM read_csv_auto('large_file.csv')</p> <p>WHERE date &gt; '2024-01-01'</p> <p>GROUP BY category</p> <p>\"\"\").df()</p> <p>4.2 Hadoop Distributed File System (HDFS)</p> <p>HDFS is designed for storing very large files with streaming access patterns on commodity hardware. Files are split into large blocks (default 128MB) distributed across DataNodes. The NameNode maintains the filesystem namespace and block locations. This architecture optimizes for high throughput rather than low latency.</p> <p>4.3 MapReduce: The Foundation</p>"},{"location":"lectures/Lecture2/#conceptual-mapreduce-for-word-count","title":"Conceptual MapReduce for word count","text":""},{"location":"lectures/Lecture2/#map-phase-line-word-1-word-1","title":"Map phase: (line) -&gt; [(word, 1), (word, 1), ...]","text":"<p>def mapper(line):</p> <p>for word in line.split():</p> <p>yield (word.lower(), 1)</p>"},{"location":"lectures/Lecture2/#reduce-phase-word-counts-word-total","title":"Reduce phase: (word, [counts]) -&gt; (word, total)","text":"<p>def reducer(word, counts):</p> <p>return (word, sum(counts))</p> <p>Recommended Resources for Lecture 4</p> Resource Type Coverage \"Hadoop: The Definitive Guide\" (White) Book, Ch. 1-4 HDFS, MapReduce fundamentals \"Designing Data-Intensive Apps\" (Kleppmann) Book, Ch. 10 Batch processing theory Google MapReduce Paper (2004) Research Paper Original MapReduce design Google GFS Paper (2003) Research Paper Distributed filesystem design <p>Lecture 5: Apache Spark and PySpark for Data Processing</p> <p>Learning Objectives</p> <ul> <li>Understand Spark's architecture: driver, executors, and the DAG execution model</li> <li>Implement data transformations using DataFrame and SQL APIs</li> <li>Optimize Spark jobs through partitioning, caching, and broadcast variables</li> <li>Debug and profile Spark applications using the Spark UI</li> </ul> <p>5.1 Spark Architecture</p> <p>The Driver-Executor Model</p> <p>Spark applications consist of a driver program that coordinates execution and executors that perform computation. The driver maintains the SparkContext, converts user code into a DAG of stages, and schedules tasks on executors. This architecture enables in-memory computation across multiple operations.</p> <p>from pyspark.sql import SparkSession</p>"},{"location":"lectures/Lecture2/#create-sparksession","title":"Create SparkSession","text":"<p>spark = SparkSession.builder \\</p> <p>.appName('DataProcessingApp') \\</p> <p>.config('spark.executor.memory', '4g') \\</p> <p>.config('spark.executor.cores', '2') \\</p> <p>.config('spark.sql.shuffle.partitions', '200') \\</p> <p>.getOrCreate()</p> <p>Lazy Evaluation and the DAG</p>"},{"location":"lectures/Lecture2/#transformations-lazy-build-dag","title":"Transformations (lazy - build DAG)","text":"<p>df = spark.read.parquet('s3://bucket/data/')</p> <p>filtered = df.filter(df['date'] &gt; '2024-01-01')\u00a0 # Not executed</p> <p>aggregated = filtered.groupBy('category').sum('value')\u00a0 # Not executed</p>"},{"location":"lectures/Lecture2/#action-triggers-execution","title":"Action (triggers execution)","text":"<p>result = aggregated.collect()\u00a0 # NOW the entire pipeline executes</p>"},{"location":"lectures/Lecture2/#explain-the-execution-plan","title":"Explain the execution plan","text":"<p>aggregated.explain(extended=True)</p> <p>5.2 DataFrame API</p> <p>Window Functions for Advanced Analytics</p> <p>from pyspark.sql.window import Window</p> <p>from pyspark.sql import functions as F</p> <p>window_category = Window.partitionBy('category').orderBy(F.desc('sales'))</p> <p>window_rolling = Window.partitionBy('store').orderBy('date').rowsBetween(-6, 0)</p> <p>df_windowed = df.withColumn(</p> <p>'rank_in_category', F.rank().over(window_category)</p> <p>).withColumn(</p> <p>'rolling_7day_avg', F.avg('sales').over(window_rolling)</p> <p>).withColumn(</p> <p>'prev_day_sales', F.lag('sales', 1).over(</p> <p>Window.partitionBy('store').orderBy('date'))</p> <p>)</p> <p>5.3 Performance Optimization</p> <p>Partitioning Strategies</p>"},{"location":"lectures/Lecture2/#check-current-partitioning","title":"Check current partitioning","text":"<p>print(f\"Number of partitions: {df.rdd.getNumPartitions()}\")</p>"},{"location":"lectures/Lecture2/#repartition-by-column-hash-partitioning","title":"Repartition by column (hash partitioning)","text":"<p>df_repartitioned = df.repartition(100, 'customer_id')</p>"},{"location":"lectures/Lecture2/#coalesce-to-reduce-partitions-no-shuffle","title":"Coalesce to reduce partitions (no shuffle)","text":"<p>df_coalesced = df.coalesce(10)</p>"},{"location":"lectures/Lecture2/#check-for-data-skew","title":"Check for data skew","text":"<p>df.groupBy(F.spark_partition_id()).count().show()</p> <p>Broadcast Joins</p> <p>from pyspark.sql.functions import broadcast</p>"},{"location":"lectures/Lecture2/#small-dimension-table","title":"Small dimension table","text":"<p>dim_table = spark.read.parquet('dimensions.parquet')\u00a0 # 10MB</p> <p>fact_table = spark.read.parquet('facts.parquet')\u00a0 # 100GB</p>"},{"location":"lectures/Lecture2/#broadcast-the-small-table","title":"Broadcast the small table","text":"<p>result = fact_table.join(</p> <p>broadcast(dim_table),</p> <p>on='dimension_id',</p> <p>how='left'</p> <p>)</p> <p>Recommended Resources for Lecture 5</p> Resource Type Coverage \"Learning Spark\" (Damji et al., 2nd Ed.) Book Comprehensive PySpark guide \"Spark: The Definitive Guide\" (Chambers) Book Deep Spark internals Apache Spark Documentation Official Docs API reference, tuning guide \"High Performance Spark\" (Karau) Book Performance optimization <p>Lecture 6: SQL Mastery and NoSQL Databases</p> <p>Learning Objectives</p> <ul> <li>Write advanced SQL queries including CTEs, window functions, and recursive queries</li> <li>Understand query execution plans and optimize SQL performance</li> <li>Design and query document databases (MongoDB) and wide-column stores (Cassandra)</li> <li>Select appropriate database technologies based on access patterns</li> </ul> <p>6.1 Advanced SQL Techniques</p> <p>Common Table Expressions (CTEs)</p> <p>-- Multi-level CTE for funnel analysis</p> <p>WITH user_sessions AS (</p> <p>SELECT user_id, session_id, MIN(event_time) as session_start</p> <p>FROM events</p> <p>WHERE event_date &gt;= CURRENT_DATE - INTERVAL '30 days'</p> <p>GROUP BY user_id, session_id</p> <p>),</p> <p>funnel_events AS (</p> <p>SELECT\u00a0</p> <p>e.user_id, e.session_id,</p> <p>MAX(CASE WHEN event_type = 'page_view' THEN 1 ELSE 0 END) as viewed,</p> <p>MAX(CASE WHEN event_type = 'add_to_cart' THEN 1 ELSE 0 END) as added,</p> <p>MAX(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchased</p> <p>FROM events e JOIN user_sessions s ON e.user_id = s.user_id</p> <p>GROUP BY e.user_id, e.session_id</p> <p>)</p> <p>SELECT SUM(viewed) as views, SUM(added) as adds, SUM(purchased) as purchases</p> <p>FROM funnel_events;</p> <p>6.2 MongoDB: Document Database</p> <p>Aggregation Pipeline</p> <p>pipeline = [</p> <p>{'\\(match': {'created_at': {'\\)gte': datetime(2024, 1, 1)}}},</p> <p>{'\\(unwind': '\\)items'},</p> <p>{'$group': {</p> <p>'_id': '$items.product_id',</p> <p>'total_quantity': {'\\(sum': '\\)items.quantity'},</p> <p>'total_revenue': {'\\(sum': {'\\)multiply': ['\\(items.price', '\\)items.quantity']}}</p> <p>}},</p> <p>{'$sort': {'total_revenue': -1}},</p> <p>{'$limit': 10}</p> <p>]</p> <p>results = list(db.orders.aggregate(pipeline))</p> <p>6.3 Apache Cassandra</p> <p>Data Model and Partition Strategy</p> <p>-- Partition by device and day for time-bounded queries</p> <p>CREATE TABLE sensor_readings (</p> <p>device_id UUID,</p> <p>date DATE,</p> <p>timestamp TIMESTAMP,</p> <p>temperature DOUBLE,</p> <p>humidity DOUBLE,</p> <p>PRIMARY KEY ((device_id, date), timestamp)</p> <p>) WITH CLUSTERING ORDER BY (timestamp DESC);</p> <p>-- Efficient query - hits single partition</p> <p>SELECT * FROM sensor_readings</p> <p>WHERE device_id = ? AND date = '2024-06-15'</p> <p>LIMIT 1000;</p> <p>Recommended Resources for Lecture 6</p> Resource Type Coverage \"SQL Performance Explained\" (Winand) Book Indexes, execution plans \"MongoDB: The Definitive Guide\" (Bradshaw) Book, 3rd Ed. Document modeling, aggregation \"Cassandra: The Definitive Guide\" (Carpenter) Book, 3rd Ed. Data modeling, operations Use The Index, Luke (website) Online Tutorial SQL indexing deep dive <p>Lecture 7: Building Production ETL Pipelines</p> <p>Learning Objectives</p> <ul> <li>Design robust ETL/ELT pipelines following software engineering best practices</li> <li>Implement workflow orchestration using Apache Airflow</li> <li>Build data quality checks and monitoring into pipelines</li> <li>Apply incremental processing patterns for efficiency</li> </ul> <p>7.1 ETL vs. ELT Architectures</p> <p>Extract-Transform-Load (ETL) processes data before loading. Extract-Load-Transform (ELT) leverages the processing power of modern data warehouses (Snowflake, BigQuery, Redshift) where raw data is loaded first, then transformed using SQL within the warehouse. The transformation layer is typically managed by tools like dbt.</p> <p>-- dbt model example (SQL-based transformation)</p> <p>{{ config(materialized='incremental', unique_key='order_id') }}</p> <p>WITH source_orders AS (</p> <p>SELECT * FROM {{ ref('stg_orders') }}</p> <p>{% if is_incremental() %}</p> <p>WHERE updated_at &gt; (SELECT MAX(updated_at) FROM {{ this }})</p> <p>{% endif %}</p> <p>)</p> <p>SELECT o.order_id, o.customer_id, c.customer_segment</p> <p>FROM source_orders o</p> <p>LEFT JOIN {{ ref('dim_customers') }} c ON o.customer_id = c.customer_id</p> <p>7.2 Apache Airflow: Workflow Orchestration</p> <p>DAG Definition</p> <p>from airflow import DAG</p> <p>from airflow.operators.python import PythonOperator</p> <p>from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator</p> <p>from datetime import timedelta</p> <p>default_args = {</p> <p>'owner': 'data_engineering',</p> <p>'retries': 3,</p> <p>'retry_delay': timedelta(minutes=5),</p> <p>}</p> <p>with DAG(</p> <p>dag_id='daily_sales_etl',</p> <p>default_args=default_args,</p> <p>schedule_interval='0 6 * * *',</p> <p>catchup=False,</p> <p>) as dag:</p> <p>validate = PythonOperator(task_id='validate', python_callable=validate_data)</p> <p>transform = SparkSubmitOperator(task_id='transform', application='/spark/transform.py')</p> <p>load = PythonOperator(task_id='load', python_callable=load_to_warehouse)</p> <p>validate &gt;&gt; transform &gt;&gt; load</p> <p>7.3 Data Quality and Validation</p> <p>Great Expectations Framework</p> <p>import great_expectations as gx</p> <p>from great_expectations.dataset import PandasDataset</p> <p>def create_sales_expectations(df):</p> <p>ge_df = PandasDataset(df)</p> <p>ge_df.expect_column_values_to_not_be_null('order_id')</p> <p>ge_df.expect_column_values_to_be_unique('order_id')</p> <p>ge_df.expect_column_values_to_be_between('quantity', min_value=1, max_value=1000)</p> <p>ge_df.expect_column_values_to_be_between('price', min_value=0.01, max_value=100000)</p> <p>return ge_df.validate()</p> <p>7.4 Incremental Processing Patterns</p> <p>Change Data Capture (CDC)</p>"},{"location":"lectures/Lecture2/#timestamp-based-incremental-extraction","title":"Timestamp-based incremental extraction","text":"<p>def extract_incremental(conn, table, last_run_time):</p> <p>query = f\"\"\"</p> <p>SELECT * FROM {table}</p> <p>WHERE updated_at &gt; %(last_run)s</p> <p>AND updated_at &lt;= %(current_run)s</p> <p>\"\"\"</p> <p>current_run = datetime.utcnow()</p> <p>df = pd.read_sql(query, conn, params={'last_run': last_run_time, 'current_run': current_run})</p> <p>save_watermark(table, current_run)</p> <p>return df</p> <p>Recommended Resources for Lecture 7</p> Resource Type Coverage \"Fundamentals of Data Engineering\" (Reis) Book Modern data engineering patterns \"Data Pipelines Pocket Reference\" (Densmore) Book Practical pipeline patterns Apache Airflow Documentation Official Docs DAG authoring, operators dbt Documentation &amp; Courses Official Resources Modern ELT transformation Great Expectations Documentation Official Docs Data quality testing <p>Comprehensive Reading List</p> <p>Core Textbooks</p> <ul> <li>VanderPlas, J. (2016). Python Data Science Handbook. O'Reilly Media. [Chapters 1-4]</li> <li>McKinney, W. (2022). Python for Data Analysis, 3rd Edition. O'Reilly Media.</li> <li>Kleppmann, M. (2017). Designing Data-Intensive Applications. O'Reilly Media. [Ch. 3, 10-12]</li> <li>Damji, J. et al. (2020). Learning Spark, 2nd Edition. O'Reilly Media.</li> <li>Reis, J. &amp; Housley, M. (2022). Fundamentals of Data Engineering. O'Reilly Media.</li> </ul> <p>Supplementary Books</p> <ul> <li>Gorelick, M. &amp; Ozsvald, I. (2020). High Performance Python, 2nd Ed. O'Reilly.</li> <li>Zheng, A. &amp; Casari, A. (2018). Feature Engineering for Machine Learning. O'Reilly.</li> <li>White, T. (2015). Hadoop: The Definitive Guide, 4th Ed. O'Reilly.</li> <li>Chambers, B. &amp; Zaharia, M. (2018). Spark: The Definitive Guide. O'Reilly.</li> <li>Winand, M. (2012). SQL Performance Explained. Self-published.</li> <li>Bradshaw, S. et al. (2019). MongoDB: The Definitive Guide, 3rd Ed. O'Reilly.</li> </ul> <p>Research Papers</p> <ul> <li>Dean, J. &amp; Ghemawat, S. (2004). MapReduce: Simplified Data Processing on Large Clusters. OSDI.</li> <li>Ghemawat, S. et al. (2003). The Google File System. SOSP.</li> <li>Zaharia, M. et al. (2012). Resilient Distributed Datasets. NSDI.</li> </ul> <p>Online Resources</p> <ul> <li>Official Documentation: NumPy, Pandas, Scikit-learn, PySpark, Airflow</li> <li>MongoDB University (university.mongodb.com) - Free certification courses</li> <li>DataStax Academy - Cassandra training</li> <li>dbt Learn (courses.getdbt.com) - Modern data transformation</li> <li>Use The Index, Luke (use-the-index-luke.com) - SQL indexing</li> </ul> <p>Appendix: Environment Setup Guide</p>"},{"location":"lectures/Lecture2/#create-conda-environment-for-course","title":"Create conda environment for course","text":"<p>conda create -n ds_tools python=3.11 -y</p> <p>conda activate ds_tools</p>"},{"location":"lectures/Lecture2/#core-data-science-libraries","title":"Core data science libraries","text":"<p>pip install numpy pandas scipy scikit-learn matplotlib seaborn</p> <p>pip install jupyterlab ipykernel ipywidgets</p>"},{"location":"lectures/Lecture2/#additional-libraries","title":"Additional libraries","text":"<p>pip install polars duckdb pyarrow fastparquet</p> <p>pip install sqlalchemy psycopg2-binary pymongo cassandra-driver</p> <p>pip install great-expectations missingno category_encoders</p>"},{"location":"lectures/Lecture2/#pyspark-requires-java-811","title":"PySpark (requires Java 8/11)","text":"<p>pip install pyspark==3.5.0</p>"},{"location":"lectures/Lecture2/#register-jupyter-kernel","title":"Register Jupyter kernel","text":"<p>python -m ipykernel install --user --name=ds_tools</p> <p>\u2014 End of Course Document \u2014</p>"}]}